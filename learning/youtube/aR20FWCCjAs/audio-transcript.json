{
  "audio_file": "youtube/aR20FWCCjAs/audio.mp3",
  "total_duration": 5762.06,
  "segments": [
    {
      "text": "You know, it's crazy that all of this is real. Yeah. Don't you think so like all this AI stuff and all this Bay Area? Yeah that it's happened. Like",
      "start": 0,
      "end": 10.93,
      "confidence": 0.90956575
    },
    {
      "text": " Isn't it straight out of Science Fiction? Yeah, another thing that's crazy is like how normal this low tape off feels",
      "start": 10.93,
      "end": 18.7,
      "confidence": 0.85283023
    },
    {
      "text": " The idea that would be investing one person of GDP in AI like if you like felt like a bigger deal, you know we're right now just feels like we get used to things pretty fast turns out. Yeah.",
      "start": 18.7,
      "end": 29.64,
      "confidence": 0.9091969
    },
    {
      "text": " But it's kind of like it's abstract like, what does it mean?",
      "start": 29.64,
      "end": 33.37,
      "confidence": 0.92716527
    },
    {
      "text": " What it means that you see in the news. Yeah, that Sergeant such company announced such and such dollar amount. Right?",
      "start": 33.37,
      "end": 39.69,
      "confidence": 0.82096463
    },
    {
      "text": " That's, that's all you see, right?",
      "start": 39.69,
      "end": 42.42,
      "confidence": 0.9327703
    },
    {
      "text": " It's not really felt in any other way so far. Yeah. Should we actually begin here? I think this is interesting discussion. Sure.",
      "start": 42.42,
      "end": 48.5,
      "confidence": 0.8960936
    },
    {
      "text": " I think your point about well from the average person's point of view.",
      "start": 48.5,
      "end": 52.74,
      "confidence": 0.9310102
    },
    {
      "text": " Nothing. Is that different will continue being true even into the singularity? No, I don't think so. Okay, interesting. So the thing, which I was referring to not feeling different.",
      "start": 52.74,
      "end": 64.26,
      "confidence": 0.9433084
    },
    {
      "text": " Is okay. So such and such company announced some difficult to comprehend dollar amount of investment. Right? I don't think anyone knows what to do with that. Yeah, but I think that the impact over AI is going to be felt",
      "start": 64.26,
      "end": 81.21,
      "confidence": 0.9030402
    },
    {
      "text": "The eye is going to be diffused through the economy. The very strong economic forces for this.",
      "start": 81.21,
      "end": 86.82,
      "confidence": 0.91923666
    },
    {
      "text": " And I think the impact is going to be felt very strongly.",
      "start": 86.82,
      "end": 90.21,
      "confidence": 0.979142
    },
    {
      "text": " When do you expect that impact? I think the models seem smarter than their economic impact would imply,",
      "start": 90.21,
      "end": 97.63,
      "confidence": 0.97631794
    },
    {
      "text": " yeah, this is",
      "start": 97.63,
      "end": 100.29,
      "confidence": 0.9620243
    },
    {
      "text": " one of the very confusing things about the models right now.",
      "start": 100.29,
      "end": 104.33,
      "confidence": 0.9957723
    },
    {
      "text": " How to reconcile?",
      "start": 104.33,
      "end": 106.71,
      "confidence": 0.9878046
    },
    {
      "text": " The fact.",
      "start": 106.71,
      "end": 108.81,
      "confidence": 0.9857657
    },
    {
      "text": " That they are doing so well on evals. And you look at the evils and you go, those are pretty hard evals, right? They're doing so well.",
      "start": 108.81,
      "end": 118.65,
      "confidence": 0.91819596
    },
    {
      "text": " but the economic impact seems to be dramatically behind and it's almost like",
      "start": 118.65,
      "end": 126.57,
      "confidence": 0.95751345
    },
    {
      "text": " it's it's very difficult to make sense of how can the model on the one hand? Do these amazing things? And I know the other hand like repeated self twice, in some situation, in a kind of an example would be, let's say you use vybe coding to do something.",
      "start": 126.57,
      "end": 143.9,
      "confidence": 0.9274317
    },
    {
      "text": " And you go to some place and then you get a bug.",
      "start": 143.9,
      "end": 146.89,
      "confidence": 0.89685243
    },
    {
      "text": " And then you tell them the model. Can you please fix the bug? Yeah and the model says oh my God. You're so right. I have a bug. Let me go fix that and it uses the second bug. Yeah.",
      "start": 146.89,
      "end": 156.33,
      "confidence": 0.91017836
    },
    {
      "text": "And then you tell, if you have a, you have this new second bug, right? And it tells you oh my God, how could have done it. You're so right again.",
      "start": 156.33,
      "end": 162.58,
      "confidence": 0.887859
    },
    {
      "text": " And brings back the first bug. Yeah. And you can alternate between those. Yeah. And it's like, how is that possible? Yeah, it's like",
      "start": 162.58,
      "end": 170.03,
      "confidence": 0.9625189
    },
    {
      "text": " I'm not sure, but it does suggest that the",
      "start": 170.03,
      "end": 173.83,
      "confidence": 0.97574425
    },
    {
      "text": " something strange is going on.",
      "start": 173.83,
      "end": 176.64,
      "confidence": 0.97382784
    },
    {
      "text": " I have to possible explanations. So here, this is the more kind of",
      "start": 176.64,
      "end": 180.84,
      "confidence": 0.9749264
    },
    {
      "text": " Whimsical explanation is it? Maybe a real training makes the models a little bit too single-minded and narrowly focused a little bit too.",
      "start": 180.84,
      "end": 189.98,
      "confidence": 0.88702047
    },
    {
      "text": " I don't know, I don't know where",
      "start": 189.98,
      "end": 193.15,
      "confidence": 0.8218482
    },
    {
      "text": " even though it also makes them aware in some other ways.",
      "start": 193.15,
      "end": 196.46,
      "confidence": 0.83623797
    },
    {
      "text": " And because of this, they can't do basic things. But there is another explanation which is",
      "start": 196.46,
      "end": 203.53,
      "confidence": 0.9623234
    },
    {
      "text": " back when people were doing pre-training,",
      "start": 203.53,
      "end": 206.92,
      "confidence": 0.89435786
    },
    {
      "text": " the question of what data to train on was answered.",
      "start": 206.92,
      "end": 212.4,
      "confidence": 0.93895066
    },
    {
      "text": " Because they that answer was everything. Yeah.",
      "start": 212.4,
      "end": 215.77,
      "confidence": 0.81565744
    },
    {
      "text": " When you do pre-training, you need all the data.",
      "start": 215.77,
      "end": 219.58,
      "confidence": 0.85044014
    },
    {
      "text": " So, you don't have to think it's going to be this date or that data. Yeah. But when people do are real training, they do need to think they say, okay? We want to have this kind of our old training for this thing, and that kind of a real training for that thing. And from what I hear,",
      "start": 219.58,
      "end": 235.83,
      "confidence": 0.89776057
    },
    {
      "text": "All the companies have teams that just produce new URL environments and just started to the training. Mix.",
      "start": 235.83,
      "end": 242.38,
      "confidence": 0.96004176
    },
    {
      "text": " And the question is, what are those? There are so many degrees of freedom? There is such a huge variety of irrelevance you could produce.",
      "start": 242.38,
      "end": 249.65,
      "confidence": 0.9281675
    },
    {
      "text": " and one of the",
      "start": 249.65,
      "end": 251.69,
      "confidence": 0.88354146
    },
    {
      "text": " one thing you could do and I think that's something that is done inadvertently.",
      "start": 251.69,
      "end": 256.94,
      "confidence": 0.9835667
    },
    {
      "text": " Is that people take inspiration from The evals you say, hey, I would love our model to do really well when we release it on the evos look great.",
      "start": 256.94,
      "end": 267.87,
      "confidence": 0.8801142
    },
    {
      "text": " What would be RL training that could help on this task, right?",
      "start": 267.87,
      "end": 273.37,
      "confidence": 0.94966084
    },
    {
      "text": " I think that is something that happens and I think it could explain a lot of what's going on.",
      "start": 273.37,
      "end": 278.83,
      "confidence": 0.9783193
    },
    {
      "text": " if you combine this with generalization of the models, actually being inadequate,",
      "start": 278.83,
      "end": 284.14,
      "confidence": 0.92311215
    },
    {
      "text": " That has the potential to explain a lot of what we are seeing this disconnect between evil performance.",
      "start": 284.14,
      "end": 291.35,
      "confidence": 0.8323839
    },
    {
      "text": " and actual real world performance, which is something that we don't",
      "start": 291.35,
      "end": 295.58,
      "confidence": 0.8569345
    },
    {
      "text": " Today exactly even understand what what we mean by that. I like this idea that the real reward hacking is a human researchers who are too focused on the evolves.",
      "start": 295.58,
      "end": 308.86,
      "confidence": 0.8914043
    },
    {
      "text": "I think there's two ways to understand.",
      "start": 308.86,
      "end": 311.48,
      "confidence": 0.9854554
    },
    {
      "text": " or to try to think about what you have, just",
      "start": 311.48,
      "end": 314.32,
      "confidence": 0.9560668
    },
    {
      "text": " Pointed out. One is look if it's the case that simply by becoming superhuman at a coding competition. A model will not automatically become more tasteful.",
      "start": 314.32,
      "end": 325.47,
      "confidence": 0.97816604
    },
    {
      "text": " And exercise better judgment about how to improve your code base. Well, then you should expand the suite of environments such that you're not just testing it on having the best performance in a coding competition. It should also be able to make the best kind of application for X thing. Or why thing or is anything in another, maybe this is what you're thinking at is to say, why should it be the case in the first place that",
      "start": 325.47,
      "end": 348.97,
      "confidence": 0.91651165
    },
    {
      "text": " Becoming superhuman at coding competitions, doesn't make you a more tasteful, programmer, more generally. Maybe the thing to do is not to",
      "start": 348.97,
      "end": 356.52,
      "confidence": 0.93458134
    },
    {
      "text": " Keep stacking up the amount of environments and the diversity of environments to figure out a poach, with let you learn from one and improve your performance on something else.",
      "start": 356.52,
      "end": 367.58,
      "confidence": 0.939685
    },
    {
      "text": " So I have an analog a human analogy, which might be helpful. So even the case, let's take the case of competitive programming. Since you mentioned that and suppose you have two students,",
      "start": 367.58,
      "end": 379.07,
      "confidence": 0.95665765
    },
    {
      "text": "One of them work decided they want to be the best competitive programmers. So they will practice.",
      "start": 379.07,
      "end": 385.7,
      "confidence": 0.9459335
    },
    {
      "text": " 10,000 hours for that domain.",
      "start": 385.7,
      "end": 388.95,
      "confidence": 0.9501486
    },
    {
      "text": " They will solve all the problems, memorize all the proof techniques and be very very you know.",
      "start": 388.95,
      "end": 394.45,
      "confidence": 0.9319454
    },
    {
      "text": " Be very skilled at quickly and correctly, implementing, all the algorithms and but notice by doing. So, they became the best one of the best.",
      "start": 394.45,
      "end": 403.83,
      "confidence": 0.93962526
    },
    {
      "text": " Student number two thought. Oh, competitive programming school. Maybe the practice for a 100 hours much, much less. And they also did really well, which one do you think is going to do better in their career later on the second? Right and I think that's basically what's going on. The models are much more likely the first student but even more because then we say okay,",
      "start": 403.83,
      "end": 423.45,
      "confidence": 0.9477495
    },
    {
      "text": " So, the model should be good with competitive programming, so let's get every single competitive. Programming problem ever.",
      "start": 423.45,
      "end": 429.72,
      "confidence": 0.93270546
    },
    {
      "text": " And then, let's do some data augmentation. So we have even more competitive, programming problems. Yes. And we train on that. And so now I got this great competitive programmer and with this analogy, I think it's more intuitive.",
      "start": 429.72,
      "end": 440.85,
      "confidence": 0.9289008
    },
    {
      "text": " I think it's more intuitive with this analogy that? Yeah. Okay. So if it's so well trained. Okay. It's like all the different algorithms and all the different proof techniques like, right? It's at its fingertips.",
      "start": 440.85,
      "end": 451.87,
      "confidence": 0.83125305
    },
    {
      "text": "And it's more intuitive that with this level of preparation. It not necessarily generalize to other things.",
      "start": 451.87,
      "end": 458.35,
      "confidence": 0.9563699
    },
    {
      "text": " Then what is the analogy for what the second student is doing, before they do the 100 hours of fine-tuning. I think it's like,",
      "start": 458.35,
      "end": 470.13,
      "confidence": 0.949003
    },
    {
      "text": " They have it. I think it's the eat Factor. Yeah. Right. And like I know like, when I was an undergrad, I remember there was, there was a student like this that studied with me. So, I know it exists. I think it's interesting to distinguish it from whatever pre-training does. So when we would understand what you just said about, we don't have to choose the data and pre-training is to say,",
      "start": 470.13,
      "end": 491.81,
      "confidence": 0.9415488
    },
    {
      "text": " Actually, it's not dissimilar to the 10,000 hours of practice. It just that you get that 10,000 hours of practice for free because it's already somewhere in the pre-training distribution. But it's like maybe you're suggesting actually, there's actually not that much generalization for free training, they're just so much data and free training but it's like it's not really generalizing better than RL like the main strength of pre-training is that there is a so much agree. Yeah.",
      "start": 491.81,
      "end": 516.07,
      "confidence": 0.9208489
    },
    {
      "text": " And B.",
      "start": 516.07,
      "end": 517.6,
      "confidence": 0.7975051
    },
    {
      "text": " You don't have to think hard about what data to put into free training.",
      "start": 517.6,
      "end": 521.89,
      "confidence": 0.85384065
    },
    {
      "text": "And it's a very kind of natural data and it does include in it, a lot of what people do. Yeah.",
      "start": 521.89,
      "end": 529.45,
      "confidence": 0.9747932
    },
    {
      "text": " People's thoughts. And a lot of the features that you know, it's like the whole world as projected by people onto text. Yeah.",
      "start": 529.45,
      "end": 539.72,
      "confidence": 0.96082795
    },
    {
      "text": " And pre-training tries to capture that using a huge amount of data.",
      "start": 539.72,
      "end": 543.64,
      "confidence": 0.9254511
    },
    {
      "text": " It's",
      "start": 543.64,
      "end": 545.29,
      "confidence": 0.94328153
    },
    {
      "text": " It's very the pre-training is very difficult to reason about because it's so hard to understand the manner in which the model, relies on pre-training data.",
      "start": 545.29,
      "end": 557.08,
      "confidence": 0.94475746
    },
    {
      "text": " And whenever the model makes a mistake, could it be? Because something by chance is not as supported by the pre-training data, you know, and preach support by pre-training is maybe a loose term",
      "start": 557.08,
      "end": 569.04,
      "confidence": 0.8841812
    },
    {
      "text": " I don't know if I can have anything more useful on this, but",
      "start": 569.04,
      "end": 574.36,
      "confidence": 0.96087205
    },
    {
      "text": " I don't think there is a human animal to preaching. Hmm.",
      "start": 574.36,
      "end": 578.98,
      "confidence": 0.86634463
    },
    {
      "text": " Here's analogies that people have proposed for what the human analogy to returning his and I'm curious to get your thoughts on why their potential wrong.",
      "start": 578.98,
      "end": 587.41,
      "confidence": 0.77445865
    },
    {
      "text": " One is to think about the first 18 or 15 or 13 years of a person's life when they aren't necessarily economically productive. But they are doing something that is making them understand the world better and so forth. And the other is to think about Evolution as doing some kind of search for three billion years, which then results in",
      "start": 587.41,
      "end": 610.62,
      "confidence": 0.9615438
    },
    {
      "text": "A human lifetime instance.",
      "start": 610.62,
      "end": 613.63,
      "confidence": 0.95361626
    },
    {
      "text": " And then I'm curious if you think either of these are actually analogous appreciating or how would you think about it? At least what lifetime human learning is like if not pretending?",
      "start": 613.63,
      "end": 622.48,
      "confidence": 0.95217615
    },
    {
      "text": " I think there are some similarities between both of these two pre-training and portraying to play the role of both of these. But I think there are some big differences as well.",
      "start": 622.48,
      "end": 633.64,
      "confidence": 0.96752614
    },
    {
      "text": " the amount of pre-training data is,",
      "start": 633.64,
      "end": 637.18,
      "confidence": 0.87257004
    },
    {
      "text": " Very, very staggering, yes.",
      "start": 637.18,
      "end": 640.36,
      "confidence": 0.9440373
    },
    {
      "text": " And somehow a human being after even 15 years with the tiny fraction of the free training data, they know much less, but whatever. They do know, they know much more deeply somehow and the mistakes like",
      "start": 640.36,
      "end": 654.96,
      "confidence": 0.90436226
    },
    {
      "text": " Like already at that age, you would not make mistakes that Arias make. Yeah. There is another thing you might say, could it be something like Evolution and then answer is maybe. But in this case, I think Evolution might actually have an edge like there is this. I remember",
      "start": 654.96,
      "end": 670.06,
      "confidence": 0.9141143
    },
    {
      "text": " Reading about this case, where some, you know, that one thing that neuroscientists do.",
      "start": 670.06,
      "end": 676.69,
      "confidence": 0.94333786
    },
    {
      "text": " Or rather one way in which neuroscientists can learn about. The brain is by studying people with brain damage to different parts of the brain.",
      "start": 676.69,
      "end": 685.51,
      "confidence": 0.8485505
    },
    {
      "text": "And so, and some people have the most strange symptoms you could imagine. It's actually really, really interesting.",
      "start": 685.51,
      "end": 691.88,
      "confidence": 0.96278507
    },
    {
      "text": " And there was one case that comes to mind, it's relevant.",
      "start": 691.88,
      "end": 694.92,
      "confidence": 0.9506521
    },
    {
      "text": " I read about this person.",
      "start": 694.92,
      "end": 697.95,
      "confidence": 0.9918879
    },
    {
      "text": " Who had some kind of brain damage that took out, I think a stroke or an accident that took out his emotional?",
      "start": 697.95,
      "end": 707.26,
      "confidence": 0.95765567
    },
    {
      "text": " Processing. So he stopped feeling any emotion.",
      "start": 707.26,
      "end": 710.38,
      "confidence": 0.93645984
    },
    {
      "text": " And as a result of that, you know, he still remained very articulate and he could solve little puzzles and on test, he seemed to be just fine.",
      "start": 710.38,
      "end": 721.05,
      "confidence": 0.92645967
    },
    {
      "text": " But he thought no emotion, he didn't feel sad, he didn't feel angry, he didn't feel animated and he became somehow extremely bad. At making any decisions at all.",
      "start": 721.05,
      "end": 731.18,
      "confidence": 0.9519369
    },
    {
      "text": " It would take him hours to decide on which socks to wear, and he would make very bad financial decisions.",
      "start": 731.18,
      "end": 737.78,
      "confidence": 0.9424364
    },
    {
      "text": " and that's very",
      "start": 737.78,
      "end": 740.75,
      "confidence": 0.92696744
    },
    {
      "text": " Does, what does it say about?",
      "start": 740.75,
      "end": 745.52,
      "confidence": 0.8758767
    },
    {
      "text": " The role of our building emotions.",
      "start": 745.52,
      "end": 749.39,
      "confidence": 0.92787766
    },
    {
      "text": " In making us like a viable agent, essentially, and I guess to connect to your question about pre-training.",
      "start": 749.39,
      "end": 756.49,
      "confidence": 0.877269
    },
    {
      "text": "it's like,",
      "start": 756.49,
      "end": 757.75,
      "confidence": 0.91261435
    },
    {
      "text": " Maybe preach, like maybe if you are good enough, that like getting everything out of pre-training, you couldn't get, you could get that as well. But that's the kind of thing which seems",
      "start": 757.75,
      "end": 767.14,
      "confidence": 0.94620043
    },
    {
      "text": " Well, it may or may not be possible to get that from pre-train.",
      "start": 767.14,
      "end": 775.75,
      "confidence": 0.92152756
    },
    {
      "text": " What is?",
      "start": 775.75,
      "end": 777.35,
      "confidence": 0.97003293
    },
    {
      "text": " That clearly not just directly emotion. It seems like some",
      "start": 777.35,
      "end": 782.94,
      "confidence": 0.9733367
    },
    {
      "text": " Almost value function like thing, which is giving telling you which decision to be like what the end reward for any decision should be.",
      "start": 782.94,
      "end": 791.71,
      "confidence": 0.898788
    },
    {
      "text": " And you think that doesn't sort of implicitly come from?",
      "start": 791.71,
      "end": 795.02,
      "confidence": 0.9475379
    },
    {
      "text": " I think it's cool. I'm just saying, it's not, it's not 100% obvious. Yeah.",
      "start": 795.02,
      "end": 800.06,
      "confidence": 0.90587777
    },
    {
      "text": " But what is that? Like, what how do you think about emotions in? What is the ml analogy for emotions?",
      "start": 800.06,
      "end": 806.02,
      "confidence": 0.890209
    },
    {
      "text": " It should be some kind of a value function thing. Yeah. But I don't think there is a great amount of energy because right now, value functions. Don't play very prominent control and the things people do. Maybe we're worth defining for the audience, what a valley function as if you want to do that. I mean certainly, I will be very happy to do that, right? So,",
      "start": 806.02,
      "end": 825.22,
      "confidence": 0.91336465
    },
    {
      "text": " So, when people do reinforcement learning the very enforcement learning is done right now.",
      "start": 825.22,
      "end": 833.33,
      "confidence": 0.87315094
    },
    {
      "text": "How does it, how do people train those agents?",
      "start": 833.33,
      "end": 836.07,
      "confidence": 0.89388084
    },
    {
      "text": " So, you have a neural net.",
      "start": 836.07,
      "end": 837.96,
      "confidence": 0.9657852
    },
    {
      "text": " And give it a problem.",
      "start": 837.96,
      "end": 839.56,
      "confidence": 0.9600624
    },
    {
      "text": " and then you tell the model, go solve it and the model takes maybe thousands hundreds of thousands of actions",
      "start": 839.56,
      "end": 845.47,
      "confidence": 0.94559795
    },
    {
      "text": " Or thoughts or something and then it produces a solution. The solution is created. And then the score",
      "start": 845.47,
      "end": 851.63,
      "confidence": 0.94815516
    },
    {
      "text": " Is used to provide a training signal for every single action.",
      "start": 851.63,
      "end": 857.22,
      "confidence": 0.9872837
    },
    {
      "text": " In your trajectory.",
      "start": 857.22,
      "end": 859.84,
      "confidence": 0.9329555
    },
    {
      "text": " so, that means that if you are doing",
      "start": 859.84,
      "end": 862.38,
      "confidence": 0.9538913
    },
    {
      "text": " Something that goes for a long time. If you are training, a task that takes a long time to solve, you will do no learning at all until you solve the until you come up with the proposal of Solution. That's how reinforcement learning is done natively. That's how, oh, one are. One ostensibly are done.",
      "start": 862.38,
      "end": 878.98,
      "confidence": 0.86090654
    },
    {
      "text": " The value function, says something like. Okay look.",
      "start": 878.98,
      "end": 884.1,
      "confidence": 0.8512303
    },
    {
      "text": " maybe I could sometimes not always",
      "start": 884.1,
      "end": 887.41,
      "confidence": 0.9746569
    },
    {
      "text": " Could tell you if you're doing well or badly, the notion of a value function, is more useful in some domains than others. So, for example, when you play chess,",
      "start": 887.41,
      "end": 895.78,
      "confidence": 0.9199988
    },
    {
      "text": " And you lose a piece, you know. I messed up.",
      "start": 895.78,
      "end": 899.41,
      "confidence": 0.8805462
    },
    {
      "text": " You don't need to play the whole game to know that what I just did was bad and therefore, whatever, whatever proceeded it was also bad.",
      "start": 899.41,
      "end": 908.44,
      "confidence": 0.96735704
    },
    {
      "text": "So, the value function. Lets you",
      "start": 908.44,
      "end": 910.93,
      "confidence": 0.935452
    },
    {
      "text": " Short-circuit.",
      "start": 910.93,
      "end": 912.32,
      "confidence": 0.482877
    },
    {
      "text": " The weight until the very end. Like let's suppose that you started to pursue some kind of okay, let's suppose that you are doing some kind of a math thing, or a programming thing.",
      "start": 912.32,
      "end": 922.9,
      "confidence": 0.96244556
    },
    {
      "text": " And you're trying to explore a particular solution Direction.",
      "start": 922.9,
      "end": 925.71,
      "confidence": 0.956813
    },
    {
      "text": " and after let's say after 1000 steps of thinking, you concluded that this direction is unpromising,",
      "start": 925.71,
      "end": 933.53,
      "confidence": 0.9278597
    },
    {
      "text": " As soon as you conclude this, you could already get a reward signal.",
      "start": 933.53,
      "end": 939.34,
      "confidence": 0.9887308
    },
    {
      "text": " 1000 times steps previously. When you decided to pursue down the path, you say, oh next time I shouldn't pursue this path in a seamless situation. Long before you actually came up with the proposed solution. This is in the Deep cigar one paper, is that",
      "start": 939.34,
      "end": 954.37,
      "confidence": 0.9121389
    },
    {
      "text": " The.",
      "start": 954.37,
      "end": 955.72,
      "confidence": 0.64019084
    },
    {
      "text": " Space of trajectories is so wide that maybe it's hard to learn a mapping from an intermediate trajectory.",
      "start": 955.72,
      "end": 965.39,
      "confidence": 0.93581253
    },
    {
      "text": " And value and also give that, you know. Encoding for example you will have the wrong idea, then it will go back. Then you'll change something this sounds like such lack of face and deep learning like I mean sure it's might be difficult, but",
      "start": 965.39,
      "end": 979.22,
      "confidence": 0.88179904
    },
    {
      "text": "Nothing deep learning can do. Yeah. So",
      "start": 979.22,
      "end": 983.15,
      "confidence": 0.86485827
    },
    {
      "text": " my expectation, is that",
      "start": 983.15,
      "end": 985.67,
      "confidence": 0.9953039
    },
    {
      "text": " like value function should be useful and",
      "start": 985.67,
      "end": 989.72,
      "confidence": 0.9202296
    },
    {
      "text": " then I fully I fully expect that they will be using the future if not already. What was I alluding to with the person whose emotional Center got and damaged is more that",
      "start": 989.72,
      "end": 1003.68,
      "confidence": 0.9452396
    },
    {
      "text": " maybe what it suggests is that the value function of humans is modulated by emotions in some important way that's hardcoded by evolution.",
      "start": 1003.68,
      "end": 1014.2,
      "confidence": 0.97575235
    },
    {
      "text": " and maybe that is important for people to",
      "start": 1014.2,
      "end": 1018.16,
      "confidence": 0.9728538
    },
    {
      "text": " be affected in the world. That's the thing I was actually gonna ask you. There's nothing really interesting about emotions of Valley function, which is that",
      "start": 1018.16,
      "end": 1025.69,
      "confidence": 0.83807087
    },
    {
      "text": " it's impressive that they have this much utility while still being rather.",
      "start": 1025.69,
      "end": 1032.22,
      "confidence": 0.9201731
    },
    {
      "text": " Simple to understand.",
      "start": 1032.22,
      "end": 1035.51,
      "confidence": 0.94252205
    },
    {
      "text": " So after responses, I do agree that compared to",
      "start": 1035.51,
      "end": 1041.38,
      "confidence": 0.8588672
    },
    {
      "text": " The kind of things that we learn in the things that we are talking about, the kind of, as we are talking about. Emotions are relatively simple.",
      "start": 1041.38,
      "end": 1049.71,
      "confidence": 0.89595443
    },
    {
      "text": "It might even be so simple that maybe you could map them out in a human understandable way. I think it would be cool to do.",
      "start": 1049.71,
      "end": 1057.25,
      "confidence": 0.96086305
    },
    {
      "text": " in terms of utility though, I think there is a thing where",
      "start": 1057.25,
      "end": 1062.77,
      "confidence": 0.9880986
    },
    {
      "text": " You know, there is this complexity robust and straight off.",
      "start": 1062.77,
      "end": 1066.96,
      "confidence": 0.90528756
    },
    {
      "text": " Where complex things can be very useful.",
      "start": 1066.96,
      "end": 1070.95,
      "confidence": 0.94951904
    },
    {
      "text": " But simple things.",
      "start": 1070.95,
      "end": 1074.48,
      "confidence": 0.98090917
    },
    {
      "text": " Are very useful in very broad range of situations.",
      "start": 1074.48,
      "end": 1078.77,
      "confidence": 0.95851296
    },
    {
      "text": " And so I think what one way to interpret, what we are seeing is that we've got these emotions that essentially evolved the mostly mostly from our mammal ancestors.",
      "start": 1078.77,
      "end": 1088.84,
      "confidence": 0.93427694
    },
    {
      "text": " And then fine-tuned a little bit while we were hominins just a bit. We do have like a decent amount of social emotions though.",
      "start": 1088.84,
      "end": 1096.06,
      "confidence": 0.88223314
    },
    {
      "text": " Which mammals May lack.",
      "start": 1096.06,
      "end": 1098.41,
      "confidence": 0.75456977
    },
    {
      "text": " But they're not very sophisticated.",
      "start": 1098.41,
      "end": 1101.17,
      "confidence": 0.84480935
    },
    {
      "text": " And because they're not sophisticated the service so. Well in the very different world compared to the one that we've been living in actually they are they also make mistakes for example our emotions well I don't know the hunger count is an emotion.",
      "start": 1101.17,
      "end": 1114.75,
      "confidence": 0.8636452
    },
    {
      "text": " Bits the beta. But I think, for example, our intuitive feeling of hunger,",
      "start": 1114.75,
      "end": 1120.72,
      "confidence": 0.7856723
    },
    {
      "text": "Is not succeeding. In guiding us correctly in this world with an abundance of food. Yeah, people haven't talked about scaling data,",
      "start": 1120.72,
      "end": 1132.56,
      "confidence": 0.91952616
    },
    {
      "text": " Scaling parameter or scaling compute?",
      "start": 1132.56,
      "end": 1135.43,
      "confidence": 0.9210594
    },
    {
      "text": " Is there a more General way to think about scaling? What are the other scaling axes?",
      "start": 1135.43,
      "end": 1140.15,
      "confidence": 0.9786396
    },
    {
      "text": " so,",
      "start": 1140.15,
      "end": 1141.66,
      "confidence": 0.9126184
    },
    {
      "text": " The thing. So here is a perspective, here's a perspective. I think might be might be true.",
      "start": 1141.66,
      "end": 1148.99,
      "confidence": 0.9080653
    },
    {
      "text": " so,",
      "start": 1148.99,
      "end": 1151.28,
      "confidence": 0.81918275
    },
    {
      "text": " The way a male used to work is that people would just think of it with stuff and try to.",
      "start": 1151.28,
      "end": 1158.3,
      "confidence": 0.9024294
    },
    {
      "text": " And try to get interesting results, that's what's been going on in the past.",
      "start": 1158.3,
      "end": 1164.07,
      "confidence": 0.88763565
    },
    {
      "text": " Then.",
      "start": 1164.07,
      "end": 1167.09,
      "confidence": 0.8540999
    },
    {
      "text": " The scale in Insight arrived, right? Scaling laws.",
      "start": 1167.09,
      "end": 1172.35,
      "confidence": 0.7456975
    },
    {
      "text": " Gpt-3.",
      "start": 1172.35,
      "end": 1173.99,
      "confidence": 0.4656155
    },
    {
      "text": " And suddenly everyone realized we should scale.",
      "start": 1173.99,
      "end": 1177.74,
      "confidence": 0.91512
    },
    {
      "text": " And it's just this, this is an example of how language affects thought.",
      "start": 1177.74,
      "end": 1185.09,
      "confidence": 0.93536514
    },
    {
      "text": " Scaling is what, just one word, but it's such a powerful word because it informs people what to do this. Okay, let's let's try to scale things and so you say, okay, so what are we scaling and pre-training was a thing to scale, it was a particular scale in recipe. Yes. The big breaks through of pre-training.",
      "start": 1185.09,
      "end": 1204.18,
      "confidence": 0.8729267
    },
    {
      "text": "Is the realization that this recipe is good.",
      "start": 1204.18,
      "end": 1207.58,
      "confidence": 0.8877242
    },
    {
      "text": " So you say, hey, if you",
      "start": 1207.58,
      "end": 1210.07,
      "confidence": 0.9622484
    },
    {
      "text": " Mix some compute.",
      "start": 1210.07,
      "end": 1211.79,
      "confidence": 0.97322625
    },
    {
      "text": " With some data into a neural net of a certain size.",
      "start": 1211.79,
      "end": 1215.62,
      "confidence": 0.9833117
    },
    {
      "text": " You will get results and you will know that will be better if you just scale the recipe up. And this is also great companies love this because it gives you a very",
      "start": 1215.62,
      "end": 1226.03,
      "confidence": 0.984341
    },
    {
      "text": " Low risk way of investing your resources, right? It's much harder to invest your resources in research.",
      "start": 1226.03,
      "end": 1236.03,
      "confidence": 0.97154605
    },
    {
      "text": " compare that, you know, if you research you need to have like go for researchers and research and come up with something versus",
      "start": 1236.03,
      "end": 1243.29,
      "confidence": 0.9451421
    },
    {
      "text": " Get more data, get more compute, you know, it will get something from pre-training.",
      "start": 1243.29,
      "end": 1247.76,
      "confidence": 0.86213744
    },
    {
      "text": " and indeed, you know, it looks like I based on various",
      "start": 1247.76,
      "end": 1253.21,
      "confidence": 0.9592897
    },
    {
      "text": " Things. People say onto some people say on Twitter, maybe it appears the Gemini have found a way to get more out of retraining.",
      "start": 1253.21,
      "end": 1261.64,
      "confidence": 0.8834521
    },
    {
      "text": " At some point, the pre-training will run out of data. The data is very clearly finite and so then, okay, what do you do next? Are you do some kind of souped up, retraining different recipe from the one that we've done before? Or are you doing our Rel, or maybe something else. But now that compute is Big, computer's not very big in some sense. We are back to the age of research.",
      "start": 1261.64,
      "end": 1282.29,
      "confidence": 0.8564022
    },
    {
      "text": "So maybe here's another way to put it up until 2020 from 25 from 2020 to 12 to 2020, it was the age of research.",
      "start": 1282.29,
      "end": 1290.67,
      "confidence": 0.9317987
    },
    {
      "text": " Now, from 2020 to 2025, it was the age of scaling or maybe plus minus, let's add Airbus to these years. Because people say this is amazing. You got a scale more, keep scaling, the one word scaling.",
      "start": 1290.67,
      "end": 1302.95,
      "confidence": 0.8684452
    },
    {
      "text": " But now the scale is so big like is it is the belief really that. Oh, it's so big but if you had 100x, more everything would be so different like it would be different for sure. But like is the belief that if you just hunted X the scale,",
      "start": 1302.95,
      "end": 1319.5,
      "confidence": 0.95620316
    },
    {
      "text": " Everything would be transformed.",
      "start": 1319.5,
      "end": 1321.68,
      "confidence": 0.94515276
    },
    {
      "text": " I don't think that's true.",
      "start": 1321.68,
      "end": 1323.35,
      "confidence": 0.96616566
    },
    {
      "text": " So it's back to the age of research. Again just to be computers. That's very interesting to put a",
      "start": 1323.35,
      "end": 1328.44,
      "confidence": 0.77274436
    },
    {
      "text": " But let me ask you the question you just posted and what are we scaling? And what is, what would it mean to have a recipe? Because",
      "start": 1328.44,
      "end": 1337.34,
      "confidence": 0.89215523
    },
    {
      "text": " I guess I'm not aware of a very clean relationship that almost looks like a lot of physics which existed in pre-training, was a power law between data or computer parameters and loss. What is the kind of relationship? We should be seeking? And how should we think about what this new recipe might look like",
      "start": 1337.34,
      "end": 1358.41,
      "confidence": 0.912358
    },
    {
      "text": "so we've already witnessed a transition from",
      "start": 1358.41,
      "end": 1364.57,
      "confidence": 0.95391047
    },
    {
      "text": " one type of scale into a different type of scaling from pre-training to RL.",
      "start": 1364.57,
      "end": 1370.31,
      "confidence": 0.9093647
    },
    {
      "text": " Now, people are scaling URL. Now, based on what people say, on Twitter, they spend more computer on our relative on pre-training at this point.",
      "start": 1370.31,
      "end": 1380.34,
      "confidence": 0.9116281
    },
    {
      "text": " Because our Rel can actually consume quite a bit of compute, you know, you do very, very long rollouts. Yes. So it takes a lot of compute to produce the rollouts.",
      "start": 1380.34,
      "end": 1389.53,
      "confidence": 0.90457046
    },
    {
      "text": " And then you get relatively small amount of learning polar out. So you really can spend, you really can spend a lot of compute.",
      "start": 1389.53,
      "end": 1395.32,
      "confidence": 0.94287854
    },
    {
      "text": " And I could imagine.",
      "start": 1395.32,
      "end": 1397.79,
      "confidence": 0.9407318
    },
    {
      "text": " Like I wouldn't at this at this it's more like I wouldn't even call it a scale scaling. I would say hey like what are you doing? And is the thing you are doing the the most productive thing you could be doing? Yeah. Can you find the most more productive way of using your computer? We've discussed the value function business earlier.",
      "start": 1397.79,
      "end": 1418.8,
      "confidence": 0.9110942
    },
    {
      "text": " And maybe once people get good at value functions, they will be using their resources more productively.",
      "start": 1418.8,
      "end": 1426.37,
      "confidence": 0.9075431
    },
    {
      "text": " And if you find a whole other way of training models, you could say, is this scaling? Or is it just using your resources? I think it becomes a little bit ambiguous. In a sense that when people were in the age of research back, then, it was like, people say, hey, let's try this in this and this. Let's try that. Oh, look something interesting is happening.",
      "start": 1426.37,
      "end": 1446.86,
      "confidence": 0.9488155
    },
    {
      "text": "And I think there will be a return to that. So if we're back in the era of research, stepping back, what is the part of the recipe that we need to think? Most about, when you say value function, people are already trying the current recipe but then having a limited judge and so forth. You could say that's a value function, but it sounds like you have something much more. Fundamental in mind. Do we need? Do we need to go back to? Should we even rethink?",
      "start": 1446.86,
      "end": 1471.54,
      "confidence": 0.9562943
    },
    {
      "text": " Free training at all and not just add more steps to the end of that process. Yeah, so the the discussion about value function. I think it was interesting. I want to like emphasize that I think the value function is something like",
      "start": 1471.54,
      "end": 1486.26,
      "confidence": 0.96640307
    },
    {
      "text": " It's going to make a real more efficient.",
      "start": 1486.26,
      "end": 1489.32,
      "confidence": 0.8724345
    },
    {
      "text": " And I think that makes a difference.",
      "start": 1489.32,
      "end": 1492.27,
      "confidence": 0.9086242
    },
    {
      "text": " But I think that anything you can do with the value function, you can do without just more slowly.",
      "start": 1492.27,
      "end": 1498.14,
      "confidence": 0.9246459
    },
    {
      "text": " The thing, which I think is the most fundamental, is that these models somehow just generalize dramatically worse than people. Yes.",
      "start": 1498.14,
      "end": 1507.16,
      "confidence": 0.92363995
    },
    {
      "text": " And it's super obvious.",
      "start": 1507.16,
      "end": 1509.47,
      "confidence": 0.93616986
    },
    {
      "text": " That's, that's seems like a very fundamental thing, okay? So this is the Crocs generalization and there's two.",
      "start": 1509.47,
      "end": 1517.2,
      "confidence": 0.91336906
    },
    {
      "text": "Sub-questions.",
      "start": 1517.2,
      "end": 1519.61,
      "confidence": 0.1859753
    },
    {
      "text": " There's one, which is about sample efficiency, which is why should it take so much more data for these models to learn than humans. There's a second about",
      "start": 1519.61,
      "end": 1527.28,
      "confidence": 0.97702986
    },
    {
      "text": " Even separate from the amount of data. It takes there's a question of",
      "start": 1527.28,
      "end": 1531.28,
      "confidence": 0.9734926
    },
    {
      "text": " why is it so hard to teach the thing? We want to a model then to a human which is to say,",
      "start": 1531.28,
      "end": 1536.99,
      "confidence": 0.9862568
    },
    {
      "text": " for to a human that we don't necessarily need a verifiable reward to be able to",
      "start": 1536.99,
      "end": 1542.08,
      "confidence": 0.93378484
    },
    {
      "text": " You're probably mentoring a bunch of researchers right now and you're, you know, talking with them, you're showing them your code and you're showing them how you think and from that they're picking up your way of thinking and how you should do research. You don't have to set like a verifiable reward for them that's like okay, this is the next part of the curriculum. And now this is the next part of your curriculum and oh it was this training was on stable and we got there's not this schleppy bespoke process.",
      "start": 1542.08,
      "end": 1565.76,
      "confidence": 0.9366072
    },
    {
      "text": " so, perhaps these two issues are actually related in some way, but I'd be curious to",
      "start": 1565.76,
      "end": 1571.05,
      "confidence": 0.978088
    },
    {
      "text": " Explore this. This second thing which was more like continue to learning and this first thing which feels just like",
      "start": 1571.05,
      "end": 1576.77,
      "confidence": 0.8708665
    },
    {
      "text": " Sample efficiency.",
      "start": 1576.77,
      "end": 1579.02,
      "confidence": 0.77247953
    },
    {
      "text": " yeah, so, you know you could actually Wonder one one possible explanation for",
      "start": 1579.02,
      "end": 1584.74,
      "confidence": 0.9135126
    },
    {
      "text": " The human sample efficiency that needs to be considered easily evolution.",
      "start": 1584.74,
      "end": 1589.61,
      "confidence": 0.8271274
    },
    {
      "text": "And evolution has given us a small amount of the more the most useful information possible.",
      "start": 1589.61,
      "end": 1598.14,
      "confidence": 0.9308856
    },
    {
      "text": " And for things like vision.",
      "start": 1598.14,
      "end": 1600.88,
      "confidence": 0.9900342
    },
    {
      "text": " Hearing and locomotion.",
      "start": 1600.88,
      "end": 1603.66,
      "confidence": 0.922468
    },
    {
      "text": " I think this is a pretty strong case that Evolution actually has given us a lot. So for example, human dexterity far exceeds, I mean,",
      "start": 1603.66,
      "end": 1614.33,
      "confidence": 0.9459571
    },
    {
      "text": " Robots can become dexterous too. If you subject them to like a huge amount of training and simulation,",
      "start": 1614.33,
      "end": 1620.02,
      "confidence": 0.8894307
    },
    {
      "text": " But to train a robot in the real world to quickly like pick up a new skill. Like a person does seems very out of reach and here you could say oh yeah like Locomotion all our ancestors needed. Great Locomotion squirrels like",
      "start": 1620.02,
      "end": 1635.5,
      "confidence": 0.9373209
    },
    {
      "text": " So Locomotion, maybe like we've got like some unbelievable prior, you could make the same case for vision, you know, I believe in the point, oh, like children, learn to drive off the 16 hour after 10 hours of practice, which is true.",
      "start": 1635.5,
      "end": 1650.7,
      "confidence": 0.87583107
    },
    {
      "text": " But our vision is so good.",
      "start": 1650.7,
      "end": 1653.47,
      "confidence": 0.96588033
    },
    {
      "text": " At least for me when I remember myself being five years old, my I was I was very excited about cars back then.",
      "start": 1653.47,
      "end": 1660.38,
      "confidence": 0.91816556
    },
    {
      "text": "And I'm pretty sure my car recognition was more than adequate for self-driving already as a five-year-old.",
      "start": 1660.38,
      "end": 1666.7,
      "confidence": 0.87426394
    },
    {
      "text": " You don't get to see that much data as a five-year-old. You spend most of your time in your parents house.",
      "start": 1666.7,
      "end": 1670.95,
      "confidence": 0.92876834
    },
    {
      "text": " so you have very low data diversity, but you could say maybe that's Evolution to",
      "start": 1670.95,
      "end": 1675.11,
      "confidence": 0.94456995
    },
    {
      "text": " but in language and math and coding.",
      "start": 1675.11,
      "end": 1678.47,
      "confidence": 0.85598975
    },
    {
      "text": " Probably not.",
      "start": 1678.47,
      "end": 1679.68,
      "confidence": 0.96040475
    },
    {
      "text": " It's just seems better than models. I mean obviously models are better than the average human at playing language in math and coding. But are they better at the average human at learning? Oh yeah, oh yeah, absolutely. What I meant to say is that language math and coding and coding suggest that whatever it is that makes people",
      "start": 1679.68,
      "end": 1700.07,
      "confidence": 0.9351608
    },
    {
      "text": " good at Learning.",
      "start": 1700.07,
      "end": 1702.28,
      "confidence": 0.75220776
    },
    {
      "text": " Is probably not so much, a complicated prior but something more, some fundamental thing.",
      "start": 1702.28,
      "end": 1709.02,
      "confidence": 0.93091923
    },
    {
      "text": " Wait, I'm not sure. I understood what should that be? The case.",
      "start": 1709.02,
      "end": 1712.05,
      "confidence": 0.7118539
    },
    {
      "text": " So consider a skill.",
      "start": 1712.05,
      "end": 1713.83,
      "confidence": 0.76108515
    },
    {
      "text": " That people exhibit some kind of great reliability or you know. Yeah.",
      "start": 1713.83,
      "end": 1720.55,
      "confidence": 0.93044096
    },
    {
      "text": " If the scale is one, there was very useful to our ancestors for many millions of years. Hundreds of millions of years. You could say, you could argue that maybe humans are good at it.",
      "start": 1720.55,
      "end": 1733.88,
      "confidence": 0.959163
    },
    {
      "text": "Because of evolution because we have a prior and evolutionary prior that's encoded in some very non-obvious way. Yeah, that somehow makes us so good at it but if people exhibit, great ability reliability robustness ability to learn in a domain that really did not exist until recently.",
      "start": 1733.88,
      "end": 1759.02,
      "confidence": 0.9623787
    },
    {
      "text": " Then this is more an indication.",
      "start": 1759.02,
      "end": 1762.84,
      "confidence": 0.9803495
    },
    {
      "text": " That people might have.",
      "start": 1762.84,
      "end": 1764.83,
      "confidence": 0.9888247
    },
    {
      "text": " just better machine learning period but then how should we think about what that is is it a matter of",
      "start": 1764.83,
      "end": 1773.23,
      "confidence": 0.9660252
    },
    {
      "text": " Yeah, what is the ml analogy for why?",
      "start": 1773.23,
      "end": 1777.82,
      "confidence": 0.8655954
    },
    {
      "text": " There's a couple interesting things about it takes for your samples, it's more unsupervised. You don't have to set a very like, a child learning to drive a car show? No, no, no. I teenager learning how to drive a car is like not exactly. Getting",
      "start": 1777.82,
      "end": 1792.48,
      "confidence": 0.8172739
    },
    {
      "text": " Some pre-built verifiable reward. Their it comes from their interaction with the machine and the with the environment.",
      "start": 1792.48,
      "end": 1801.35,
      "confidence": 0.9011619
    },
    {
      "text": " And yeah, it takes much of her samples, that seems more unsupervised, it seems more robust much more robust, the robustness of people.",
      "start": 1801.35,
      "end": 1810.36,
      "confidence": 0.90803903
    },
    {
      "text": "It's really staggering. Yeah, so like, okay and do you have a unified way of thinking about? Why are all these things happening at once? What is the ml knowledge that would, that could be, it could realize something like this. So, so this is where, you know, one of the things that you've been asking about is, how can, you know, the teenage driver kind of self-correct and learn from their experience without an external teacher. And the answer is, well, they have their value function, right? They have a general sense, which is also, by the way, extremely robust in people like",
      "start": 1810.36,
      "end": 1847.53,
      "confidence": 0.86461794
    },
    {
      "text": " Whatever it is. The human value function, whatever the human value function, is with a few exceptions around addiction. It's actually very, very robust.",
      "start": 1847.53,
      "end": 1857.93,
      "confidence": 0.92999333
    },
    {
      "text": " And so for something like a teenager, that's learning to drive.",
      "start": 1857.93,
      "end": 1862.31,
      "confidence": 0.9358414
    },
    {
      "text": " They start to drive.",
      "start": 1862.31,
      "end": 1864.3,
      "confidence": 0.914772
    },
    {
      "text": " And they already have a sense of how they drive.",
      "start": 1864.3,
      "end": 1867.08,
      "confidence": 0.9492773
    },
    {
      "text": " Immediately.",
      "start": 1867.08,
      "end": 1868.56,
      "confidence": 0.97965634
    },
    {
      "text": " How badly their unconfident, and then they see. Okay. And they, and then, of course, the learning speed of any teenager is so fast, stuff. That 10 hours. You're good to go. Yeah, it seems like humans, have some solution. I'm curious about, like,",
      "start": 1868.56,
      "end": 1880.35,
      "confidence": 0.8520227
    },
    {
      "text": " Well, how are they doing it? And like why is it so hard to like what how do we need to reconceptualize the way we're training models, to make something like this possible, you know?",
      "start": 1880.35,
      "end": 1888.34,
      "confidence": 0.9181113
    },
    {
      "text": "That is a great question to ask.",
      "start": 1888.34,
      "end": 1891.02,
      "confidence": 0.9980955
    },
    {
      "text": " And it's a question.",
      "start": 1891.02,
      "end": 1893.77,
      "confidence": 0.98143667
    },
    {
      "text": " I have a lot of opinions about",
      "start": 1893.77,
      "end": 1896,
      "confidence": 0.89073116
    },
    {
      "text": " But unfortunately.",
      "start": 1896,
      "end": 1899,
      "confidence": 0.9626106
    },
    {
      "text": " We live in a world where not not all machine learning ideas are discussed freely and this is this is one of them. So",
      "start": 1899,
      "end": 1905.51,
      "confidence": 0.94633836
    },
    {
      "text": " this probably a way to do it.",
      "start": 1905.51,
      "end": 1908.14,
      "confidence": 0.93222225
    },
    {
      "text": " I think it can be done. The fact that people",
      "start": 1908.14,
      "end": 1912.73,
      "confidence": 0.9908997
    },
    {
      "text": " are like that. I think it's a proof that it can be done. There may be another blocker though, which is",
      "start": 1912.73,
      "end": 1918.91,
      "confidence": 0.9054496
    },
    {
      "text": " There is a possibility.",
      "start": 1918.91,
      "end": 1920.85,
      "confidence": 0.92739785
    },
    {
      "text": " That the human neurons actually do more compute than we think.",
      "start": 1920.85,
      "end": 1926.96,
      "confidence": 0.94438416
    },
    {
      "text": " and if that is true, and if that plays an important role and things might be more difficult",
      "start": 1926.96,
      "end": 1933.39,
      "confidence": 0.97247916
    },
    {
      "text": " but regardless, I do think it points to the existence of some",
      "start": 1933.39,
      "end": 1937.78,
      "confidence": 0.96317345
    },
    {
      "text": " machine learning principle.",
      "start": 1937.78,
      "end": 1941.08,
      "confidence": 0.8569152
    },
    {
      "text": "The diabetic opinions on. But unfortunately, circumstances make it hard to to discuss in detail even though nobody listens to this podcast. Yeah. Yeah. So I have to say that prepping for Ilya was pretty tough because neither I nor anybody else, had any idea what he's working on and what SSI is trying to do. I had no basis to come up with my questions and the only thing I could go off on a was trying to think from first principles about, what are the bottle next to AGI, because clearly Ilya is working on them in some way part of this question involved thinking about RL scaling because everybody's asking how well our generalized and how we can make a generalized better as part of this. I was reading this paper that came out recently on RL scaling and it showed that actually the learning Corona. RL looks like a sigmoid. I found this very curious. Why should be a sigmoid where? It learns very little for a long time. And then it quickly learns a lot. And then asked him totes. This is very different from the power law. You see in pre-training where the model learns",
      "start": 1941.08,
      "end": 2001.44,
      "confidence": 0.5668365
    },
    {
      "text": "A bunch of the very beginning and then less and less over time. And it actually reminded me of a note that I had written down. After I had a conversation, the researcher friend, where he pointed out that the number of samples that you need to take in order to find a correct answer, scales exponentially with how different your current probability distribution is from the target properly distribution. And I was thinking about how these two ideas are related. I had this vague idea that they should be connected, but I really didn't know how. I don't have a math background, so I couldn't really formalize it. But I wondered if Gemini through could help me out here. And so, it took a picture of my notebook. And I took the paper and I put them both in the context of Gemini 3, and I asked it to find the connection and thought a bunch. And then it realized that the correct way to model the information you gain from a single. Yes, or no outcome in RL is as the entropy of a random binary variable. It made a graph, which showed how the bits you gain first sample in our all versus supervised learning skill. As a pass rating, increase is. And as soon as I saw the",
      "start": 2001.44,
      "end": 2061.38,
      "confidence": 0.612239
    },
    {
      "text": "Graph, the Gemini 3 made immediately a ton of things started making sense to me. Then I wanted to see if there's any empirical basis to this Theory. So I asked Gemini to code an experiment to show whether the Improvement in Los scales in this way with pass rate. I just took the code that Gemini output. I copy pasted it into a Google collab notebook, and I was able to run this toy ml experiment and visualize its results without a single bug. It's interesting because the results look similar, but not identical to what we should have expected. And so, I downloaded this chart, and I put it into Gemini and asked it, what is going on here and came up with the hypothesis that I think is actually correct? Which is that we're capping, how much supervised learning can improve in the beginning, by having a fixed learning rate. And in fact, we should decrease the learning rate over time, it actually gives us an intuitive understanding, for why in practice, we have learning race schedule orders that decrease the learning rate over time. I did this entire flow from coming up with this vague initial question to building a theoretical understanding to running",
      "start": 2061.38,
      "end": 2121.32,
      "confidence": 0.5197639
    },
    {
      "text": " Some toy ml experiments all with Gemini 3. This feels like the first model where it can actually come up with new connections that I wouldn't have anticipated. It's actually now become the default place. I go to, when I want to brainstorm new ways to think about a problem. If you want to read more about RL scaling, you can check out the blog posts that I wrote With a Little Help from Gemini 3 and if you want to check out Gemini 3 yourself, go to Gemini dot Google.",
      "start": 2121.32,
      "end": 2145.57,
      "confidence": 0.8783436
    },
    {
      "text": "I'm curious. If you say, we are back in the era of research.",
      "start": 2145.57,
      "end": 2148.9,
      "confidence": 0.88249683
    },
    {
      "text": " You were there from 2012 to 2020?",
      "start": 2148.9,
      "end": 2152.85,
      "confidence": 0.948209
    },
    {
      "text": " And do you? Yeah what is now the vibe going to be if we go back to the area of research for example,",
      "start": 2152.85,
      "end": 2161.8,
      "confidence": 0.85032314
    },
    {
      "text": " Even after Alex net, the amount of compute that was used to run experiments kept increasing and the size of Frontier systems kept increasing.",
      "start": 2161.8,
      "end": 2171.52,
      "confidence": 0.9539869
    },
    {
      "text": " And do you think now that this era of research will still require tremendous amounts of compute? Do you think it will require going back into the archives and reading old papers? What is you, maybe what was the vibe of like you were Google and open AI in Sanford? And these places when there was like a more of a Vibe of research, what kind of things should we be expecting in the community?",
      "start": 2171.52,
      "end": 2198.14,
      "confidence": 0.9276624
    },
    {
      "text": " So, one consequence of the age of scaling.",
      "start": 2198.14,
      "end": 2203.92,
      "confidence": 0.84987944
    },
    {
      "text": " Is that there was this?",
      "start": 2203.92,
      "end": 2206.09,
      "confidence": 0.88589895
    },
    {
      "text": " Scaling sucked out all the air in the room. Yeah.",
      "start": 2206.09,
      "end": 2210.28,
      "confidence": 0.85948694
    },
    {
      "text": " and so,",
      "start": 2210.28,
      "end": 2212.58,
      "confidence": 0.95089513
    },
    {
      "text": " because scaling sucked out all the air in the room.",
      "start": 2212.58,
      "end": 2215.91,
      "confidence": 0.7038153
    },
    {
      "text": "Everyone starts to do the same thing.",
      "start": 2215.91,
      "end": 2218.93,
      "confidence": 0.84930664
    },
    {
      "text": " We got to the point where",
      "start": 2218.93,
      "end": 2221.9,
      "confidence": 0.99516994
    },
    {
      "text": " we are in a world where there are more companies than ideas but quite a bit actually on that, you know, there is the Silicon Valley saying that says that ideas are cheap execution is everything.",
      "start": 2221.9,
      "end": 2237.6,
      "confidence": 0.97030956
    },
    {
      "text": " And people say that a lot. Yeah, and there is truth to that",
      "start": 2237.6,
      "end": 2241.36,
      "confidence": 0.9267856
    },
    {
      "text": " But then I saw, I saw someone say on Twitter.",
      "start": 2241.36,
      "end": 2244.54,
      "confidence": 0.9350289
    },
    {
      "text": " something like,",
      "start": 2244.54,
      "end": 2246.6,
      "confidence": 0.9383317
    },
    {
      "text": " If ideas are so cheap, how come no one's having any ideas.",
      "start": 2246.6,
      "end": 2250.77,
      "confidence": 0.88682085
    },
    {
      "text": " And I think it's true too. I think.",
      "start": 2250.77,
      "end": 2253.2,
      "confidence": 0.9402901
    },
    {
      "text": " like, if you think about a research progress in terms of bottlenecks,",
      "start": 2253.2,
      "end": 2259.15,
      "confidence": 0.92242986
    },
    {
      "text": " There are several bottlenecks.",
      "start": 2259.15,
      "end": 2262.05,
      "confidence": 0.773318
    },
    {
      "text": " If you go back to the if and one of them is ideas and one of them is your ability to bring them to life. Yeah. Which might be compute but also engineering.",
      "start": 2262.05,
      "end": 2271.39,
      "confidence": 0.9555266
    },
    {
      "text": " so if you go back to the 90s, let's say you had people who have had pretty good ideas,",
      "start": 2271.39,
      "end": 2276.79,
      "confidence": 0.93729895
    },
    {
      "text": " and if they had much larger computers, maybe they could demonstrate that their ideas for viable, but they could not",
      "start": 2276.79,
      "end": 2282.44,
      "confidence": 0.8860615
    },
    {
      "text": " So they could only have very very small demonstration and not convinced anyone? Yeah.",
      "start": 2282.44,
      "end": 2287.04,
      "confidence": 0.8839822
    },
    {
      "text": "So, the bottleneck was compute.",
      "start": 2287.04,
      "end": 2289.22,
      "confidence": 0.87671727
    },
    {
      "text": " Then in the age of scaling.",
      "start": 2289.22,
      "end": 2291.75,
      "confidence": 0.9780061
    },
    {
      "text": " Computers increased a lot.",
      "start": 2291.75,
      "end": 2293.78,
      "confidence": 0.8929651
    },
    {
      "text": " and of course, there is a question of how much computers needed, but computers large, so,",
      "start": 2293.78,
      "end": 2301.85,
      "confidence": 0.9389441
    },
    {
      "text": " Computers large enough.",
      "start": 2301.85,
      "end": 2303.78,
      "confidence": 0.80432636
    },
    {
      "text": " Such that.",
      "start": 2303.78,
      "end": 2305.2,
      "confidence": 0.96576977
    },
    {
      "text": " It's like not obvious that you need that much more compute to prove.",
      "start": 2305.2,
      "end": 2310.84,
      "confidence": 0.98525804
    },
    {
      "text": " Some idea.",
      "start": 2310.84,
      "end": 2312.35,
      "confidence": 0.9286434
    },
    {
      "text": " Like, I'll give you an analogy alexnet was on 2gp.",
      "start": 2312.35,
      "end": 2317.68,
      "confidence": 0.9128895
    },
    {
      "text": " That was the total amount of computers for it.",
      "start": 2317.68,
      "end": 2320.35,
      "confidence": 0.93907714
    },
    {
      "text": " The Transformer.",
      "start": 2320.35,
      "end": 2322.01,
      "confidence": 0.87227726
    },
    {
      "text": " Was built on 8 to 64 gpus. No single Transformer. Paper experiment used more than 64 gpus of 2017 which would be like what two gpus of today?",
      "start": 2322.01,
      "end": 2334.05,
      "confidence": 0.87151295
    },
    {
      "text": " So, the resnet.",
      "start": 2334.05,
      "end": 2336.68,
      "confidence": 0.7900431
    },
    {
      "text": " Right.",
      "start": 2336.68,
      "end": 2337.93,
      "confidence": 0.80548644
    },
    {
      "text": " many, like, even the the You could argue that the black hole one,",
      "start": 2337.93,
      "end": 2343.58,
      "confidence": 0.7441369
    },
    {
      "text": " Reasoning.",
      "start": 2343.58,
      "end": 2345,
      "confidence": 0.87467235
    },
    {
      "text": " was not the most compute heavy thing in the world, so there definitely",
      "start": 2345,
      "end": 2349.58,
      "confidence": 0.89047813
    },
    {
      "text": " For, for research.",
      "start": 2349.58,
      "end": 2352.35,
      "confidence": 0.8302777
    },
    {
      "text": " You need definitely some of computer.",
      "start": 2352.35,
      "end": 2356.39,
      "confidence": 0.63758826
    },
    {
      "text": " But it's far from obvious that you need the absolute largest amount of compute ever for research.",
      "start": 2356.39,
      "end": 2361.8,
      "confidence": 0.9199825
    },
    {
      "text": "You might argue and I think it is true that if you want to build the absolutely best system.",
      "start": 2361.8,
      "end": 2368.38,
      "confidence": 0.9923967
    },
    {
      "text": " if you want to build the absolutely best system,",
      "start": 2368.38,
      "end": 2371.16,
      "confidence": 0.9417304
    },
    {
      "text": " then it helps so much more compute and especially if everyone is within the same paradigm,",
      "start": 2371.16,
      "end": 2376.45,
      "confidence": 0.94590074
    },
    {
      "text": " then compute becomes one of the big differentiators.",
      "start": 2376.45,
      "end": 2381.22,
      "confidence": 0.98627096
    },
    {
      "text": " Yeah, I guess.",
      "start": 2381.22,
      "end": 2383.29,
      "confidence": 0.94015026
    },
    {
      "text": " What was possible to develop these ideas?",
      "start": 2383.29,
      "end": 2386.31,
      "confidence": 0.9724
    },
    {
      "text": " I'm asking you for the history because you're actually there. I'm not sure what actually happened, but it sounds like it was possible to develop these ideas using minimal amounts of compute. But it wasn't the Transformer didn't immediately become famous. It became the thing everybody started doing and then started experimenting on top of and building on top of",
      "start": 2386.31,
      "end": 2402.83,
      "confidence": 0.9589678
    },
    {
      "text": " Because it was validated at higher and higher levels in compute. Correct. And if you at SSI have 50 different ideas, how will, you know, which one is the next Transformer? And which one is, you know, brittle without having the kinds of compute that other Frontier lapse have? So I can I can comment on that which is",
      "start": 2402.83,
      "end": 2425.58,
      "confidence": 0.93143374
    },
    {
      "text": " the short comment is that, you know, you mentioned a society specifically for us",
      "start": 2425.58,
      "end": 2432.73,
      "confidence": 0.8666381
    },
    {
      "text": "The amount of compute that SSI has for research.",
      "start": 2432.73,
      "end": 2437.91,
      "confidence": 0.87172294
    },
    {
      "text": " Is really not that small and I want to explain why like, a simple math can explain why the amount of compute that we have is actually a lot more comparable for research than one might think.",
      "start": 2437.91,
      "end": 2450.53,
      "confidence": 0.98911154
    },
    {
      "text": " now, explain",
      "start": 2450.53,
      "end": 2452.26,
      "confidence": 0.8942071
    },
    {
      "text": " so,",
      "start": 2452.26,
      "end": 2453.77,
      "confidence": 0.8392545
    },
    {
      "text": " SSI has raised 3 billion dollars, which is like.",
      "start": 2453.77,
      "end": 2460.09,
      "confidence": 0.88991874
    },
    {
      "text": " Not small by its like, a lot by any absolute sense, but you could say, but look at the other company is Raising much more.",
      "start": 2460.09,
      "end": 2467.95,
      "confidence": 0.9235292
    },
    {
      "text": " But a lot of what their, a lot of their compute goes for inference.",
      "start": 2467.95,
      "end": 2472.33,
      "confidence": 0.9340086
    },
    {
      "text": " Like these big numbers. This big loans, it's earmarked for inference, that's number one.",
      "start": 2472.33,
      "end": 2479.61,
      "confidence": 0.80107766
    },
    {
      "text": " now, number two,",
      "start": 2479.61,
      "end": 2481.22,
      "confidence": 0.76155657
    },
    {
      "text": " you need, if you want to have a product on which you do inference, you need to have a big staff of Engineers of salespeople. A lot of the research needs to be dedicated for producing all kinds of product related features.",
      "start": 2481.22,
      "end": 2495.18,
      "confidence": 0.9463153
    },
    {
      "text": " So then, when you look at what's actually left for research, the difference becomes a lot smaller.",
      "start": 2495.18,
      "end": 2500.39,
      "confidence": 0.9652073
    },
    {
      "text": " Now, the other thing is, is that if you are doing something different,",
      "start": 2500.39,
      "end": 2506.75,
      "confidence": 0.89216924
    },
    {
      "text": "Do you really need the absolute maximum scale?",
      "start": 2506.75,
      "end": 2509.73,
      "confidence": 0.9335481
    },
    {
      "text": " To prove it. I don't think it's true at all. I think that in our case, we have",
      "start": 2509.73,
      "end": 2516.56,
      "confidence": 0.9569615
    },
    {
      "text": " Sufficient to prove to convince ourselves and anyone else that what we're doing is correct.",
      "start": 2516.56,
      "end": 2522.38,
      "confidence": 0.9140595
    },
    {
      "text": " there's been public SMS that, you know, companies like open AI, spend on the order of",
      "start": 2522.38,
      "end": 2527.01,
      "confidence": 0.81906
    },
    {
      "text": " Five six billion dollars a year, even just so far on experiments. This is separate from the amount of money they're sending on in France and so forth. So it seems like they're spending more a year or running experience like research experiments, the new guys have in total funding. I think it's a question of what you do with it.",
      "start": 2527.01,
      "end": 2545.11,
      "confidence": 0.9332648
    },
    {
      "text": " It's a question of what you do with it. Like they have like the more I think in, in their case, in the case of others. I think there's a lot more Demand on the training compute. There's a lot more different work streams.",
      "start": 2545.11,
      "end": 2557.26,
      "confidence": 0.9532585
    },
    {
      "text": " There is, there are different modalities. There is just more stuff.",
      "start": 2557.26,
      "end": 2561.85,
      "confidence": 0.8720892
    },
    {
      "text": " And so it becomes fragmented.",
      "start": 2561.85,
      "end": 2563.85,
      "confidence": 0.83400947
    },
    {
      "text": " How will SSI make money, you know?",
      "start": 2563.85,
      "end": 2567.31,
      "confidence": 0.94365543
    },
    {
      "text": " my answer to this question is something like",
      "start": 2567.31,
      "end": 2571.07,
      "confidence": 0.9440774
    },
    {
      "text": " We just focus right now. We just focus on the research and then settle. This question will reveal itself.",
      "start": 2571.07,
      "end": 2577.97,
      "confidence": 0.8835697
    },
    {
      "text": "I think there will be lots of possible. Answers is exercise plans to illustrate shots. Super intelligence.",
      "start": 2577.97,
      "end": 2584.41,
      "confidence": 0.7809798
    },
    {
      "text": " Maybe.",
      "start": 2584.41,
      "end": 2585.85,
      "confidence": 0.9074401
    },
    {
      "text": " I think that there is married to it. I think there's a lot of Merit because I think that it's very nice to not be affected by the day-to-day Market competition.",
      "start": 2585.85,
      "end": 2596.86,
      "confidence": 0.9731571
    },
    {
      "text": " but,",
      "start": 2596.86,
      "end": 2598.72,
      "confidence": 0.85735935
    },
    {
      "text": " I think there are two reasons that may cause us to change the plan.",
      "start": 2598.72,
      "end": 2604.73,
      "confidence": 0.97826254
    },
    {
      "text": " One is pragmatic, if timelines turned out to be long,",
      "start": 2604.73,
      "end": 2608.66,
      "confidence": 0.9692321
    },
    {
      "text": " Which they might.",
      "start": 2608.66,
      "end": 2610.78,
      "confidence": 0.955939
    },
    {
      "text": " and second, I think the reason a lot of value,",
      "start": 2610.78,
      "end": 2614.54,
      "confidence": 0.90698546
    },
    {
      "text": " Inn.",
      "start": 2614.54,
      "end": 2615.76,
      "confidence": 0.78780603
    },
    {
      "text": " The best.",
      "start": 2615.76,
      "end": 2617.47,
      "confidence": 0.94328445
    },
    {
      "text": " And most powerful AI being out there impacting the world. Yeah.",
      "start": 2617.47,
      "end": 2623.38,
      "confidence": 0.871378
    },
    {
      "text": " I think this is a meaningful thing, but then so why is your default plan too? Straight out super intelligence because it sounds like you know,",
      "start": 2623.38,
      "end": 2631.73,
      "confidence": 0.89436656
    },
    {
      "text": " Open Ai and the robotic, all these other companies they're explicit thinking is look, we have weaker and weaker intelligence. Is that the public can get used to and prepare for?",
      "start": 2631.73,
      "end": 2640.67,
      "confidence": 0.8823452
    },
    {
      "text": " And why is it potentially better to build a super intelligence directly? So I'll make the case for and again. Yeah, the case for is that you are so one of the challenges,",
      "start": 2640.67,
      "end": 2653.48,
      "confidence": 0.94814676
    },
    {
      "text": "That people face when they're in the market is that they have to participate in the rat race.",
      "start": 2653.48,
      "end": 2659.61,
      "confidence": 0.9020818
    },
    {
      "text": " And the rat race is quite difficult in that it exposes you to to difficult trade-offs, which you need to make.",
      "start": 2659.61,
      "end": 2666.44,
      "confidence": 0.9095727
    },
    {
      "text": " And there is it is it is nice to say, will insulate ourselves from all this and just focus on the research and come out only when we are ready and not before.",
      "start": 2666.44,
      "end": 2677.53,
      "confidence": 0.9718485
    },
    {
      "text": " But the Counterpoint is valid too.",
      "start": 2677.53,
      "end": 2680.5,
      "confidence": 0.9165084
    },
    {
      "text": " And those are, those are opposing forces. The Counterpoint is. Hey, it is useful for the world.",
      "start": 2680.5,
      "end": 2688.68,
      "confidence": 0.9515799
    },
    {
      "text": " To see powerful AI.",
      "start": 2688.68,
      "end": 2690.47,
      "confidence": 0.8088646
    },
    {
      "text": " It is useful for the world to see powerful AI because that's the only way you can communicate it. Well, I guess not even just that. You can communicate the idea but communicate the AI",
      "start": 2690.47,
      "end": 2699.54,
      "confidence": 0.9697425
    },
    {
      "text": " Not the idea.",
      "start": 2699.54,
      "end": 2700.94,
      "confidence": 0.82671213
    },
    {
      "text": " Communicate the AI. What do you mean communicate there? So okay so let's suppose you write an essay about Ai and the essay says AI is going to be this and as going to be that and it's going to be this and you ready can you say? Okay this is an interesting essay right now, suppose you see and AI doing this and they are doing that.",
      "start": 2700.94,
      "end": 2719.68,
      "confidence": 0.84996533
    },
    {
      "text": " It is in comparable.",
      "start": 2719.68,
      "end": 2721.54,
      "confidence": 0.90090245
    },
    {
      "text": " like basically, I think, I think that there is a big benefit",
      "start": 2721.54,
      "end": 2727.08,
      "confidence": 0.9796166
    },
    {
      "text": "From AI, being in the public. And",
      "start": 2727.08,
      "end": 2730.48,
      "confidence": 0.8906144
    },
    {
      "text": " That would be a reason for us to not be quite straight shot. Yeah. Well, I guess it's not even that which I, but I do think that is an important part of it. The other big thing is, I can't think of another discipline and human engineering and research where",
      "start": 2730.48,
      "end": 2746.51,
      "confidence": 0.97293717
    },
    {
      "text": " The end artifact was made safer. Mostly through just thinking about how to make it safe as a post to why are airplane crashes per mile, so much lower today than they were decades ago. Why is it so much harder to find a bug in Linux than it would have been decades ago? And I think it's mostly because these systems were deployed to the world, you noticed failures, those failures were corrected, and assistance became more robust, and I'm not sure why.",
      "start": 2746.51,
      "end": 2775.23,
      "confidence": 0.88429916
    },
    {
      "text": " AGI and superhuman intelligence would be any different especially given and I hope we can talk, we're going to get to this.",
      "start": 2775.23,
      "end": 2782.04,
      "confidence": 0.8677441
    },
    {
      "text": " It seems like the harms of superintelligence are not just about like having some Loveland paperclip out there, but it just like this is a really powerful thing and we don't even know how to conceptualize how people interact with what people will do with that and having gradual access to. It seems like a",
      "start": 2782.04,
      "end": 2799.28,
      "confidence": 0.9303038
    },
    {
      "text": "Um, better way to maybe spread out the impact of it and to help people prepare for it. Well, I think, I think, on this point, even in the straight shot scenario, you would still do a gradual release of it.",
      "start": 2799.28,
      "end": 2813.88,
      "confidence": 0.94636863
    },
    {
      "text": " Is how I would imagine it.",
      "start": 2813.88,
      "end": 2816.36,
      "confidence": 0.86020374
    },
    {
      "text": " The gradualism would be and inherent inherent component of any plan. It just a question of what is the first thing that you get out of the door? That's number one, number two. I also think, you know, I believe you have advocated for continued learning more than other people. And I actually think that this is any important and correct thing and here is why",
      "start": 2816.36,
      "end": 2840.44,
      "confidence": 0.96284634
    },
    {
      "text": " So one of the things so I'll give you another example.",
      "start": 2840.44,
      "end": 2845.43,
      "confidence": 0.9760143
    },
    {
      "text": " Of how thinking how language affects thinking.",
      "start": 2845.43,
      "end": 2848.84,
      "confidence": 0.9396611
    },
    {
      "text": " And in this case, it is to be two words, two words that have shaped everyone's thinking I maintain",
      "start": 2848.84,
      "end": 2856.43,
      "confidence": 0.847326
    },
    {
      "text": " First Ward, a GI",
      "start": 2856.43,
      "end": 2859.87,
      "confidence": 0.67457646
    },
    {
      "text": " Second word, pre-training.",
      "start": 2859.87,
      "end": 2862.19,
      "confidence": 0.73830205
    },
    {
      "text": " Let me explain.",
      "start": 2862.19,
      "end": 2863.65,
      "confidence": 0.9301317
    },
    {
      "text": " So, the word, the term AGI.",
      "start": 2863.65,
      "end": 2867.16,
      "confidence": 0.8645642
    },
    {
      "text": " Why does this term exists?",
      "start": 2867.16,
      "end": 2869.53,
      "confidence": 0.93352526
    },
    {
      "text": "It's a very particular term. Why does it exist?",
      "start": 2869.53,
      "end": 2872.55,
      "confidence": 0.98568755
    },
    {
      "text": " There's a reason.",
      "start": 2872.55,
      "end": 2873.82,
      "confidence": 0.8590296
    },
    {
      "text": " the reason that the term AGI exists,",
      "start": 2873.82,
      "end": 2877.28,
      "confidence": 0.9423072
    },
    {
      "text": " Isn't in my opinion, not so much because it's like a very important essential descriptor of some and state of intelligence.",
      "start": 2877.28,
      "end": 2886.16,
      "confidence": 0.95441467
    },
    {
      "text": " but,",
      "start": 2886.16,
      "end": 2887.38,
      "confidence": 0.8953283
    },
    {
      "text": " Because it is a reaction.",
      "start": 2887.38,
      "end": 2892.49,
      "confidence": 0.99045146
    },
    {
      "text": " To a different term that existed in the term is narrow AI. If you go back to ancient history of gameplay, AI of checkers, AI chess AI computer games AI, everyone would say, look at this narrow intelligence. Sure the Chessie I can be Casper of what it can do. Anything else? It is so narrow. Artificial narrow intelligence.",
      "start": 2892.49,
      "end": 2914.47,
      "confidence": 0.9088615
    },
    {
      "text": " so in response, as a reaction to this,",
      "start": 2914.47,
      "end": 2917.59,
      "confidence": 0.8916347
    },
    {
      "text": " some people said,",
      "start": 2917.59,
      "end": 2919.25,
      "confidence": 0.9252724
    },
    {
      "text": " well, this is not good. It is so narrow. What we need is generally",
      "start": 2919.25,
      "end": 2924.87,
      "confidence": 0.94210947
    },
    {
      "text": " generally, I and AI that can just do all the things.",
      "start": 2924.87,
      "end": 2928.93,
      "confidence": 0.8411057
    },
    {
      "text": " The second.",
      "start": 2928.93,
      "end": 2932.02,
      "confidence": 0.7547197
    },
    {
      "text": " And that term just got a lot of traction. Yeah.",
      "start": 2932.02,
      "end": 2936.38,
      "confidence": 0.9095896
    },
    {
      "text": " the second thing that got a little often,",
      "start": 2936.38,
      "end": 2939.23,
      "confidence": 0.69368774
    },
    {
      "text": " Is pre-training.",
      "start": 2939.23,
      "end": 2940.81,
      "confidence": 0.616804
    },
    {
      "text": "Specifically the recipe of pre-training. I think the current the way people do RL now is Maybe",
      "start": 2940.81,
      "end": 2947.93,
      "confidence": 0.9195668
    },
    {
      "text": " and is undoing the conceptual imprint of pre-training. But pre-training had the property, you do more pre-training and the model gets better at everything more or less. Uniformly, generally I pre-training gives AGI.",
      "start": 2947.93,
      "end": 2965.19,
      "confidence": 0.8974244
    },
    {
      "text": " but,",
      "start": 2965.19,
      "end": 2968.37,
      "confidence": 0.88190466
    },
    {
      "text": " the thing that happened with AGI and pre-training, is that in some sense, the overshot the target",
      "start": 2968.37,
      "end": 2975.53,
      "confidence": 0.89223814
    },
    {
      "text": " Because by the kind, if you think about the term AGI, you will realize and the context of pre-training, it will realize that the human being is not an AGI.",
      "start": 2975.53,
      "end": 2986.85,
      "confidence": 0.93281937
    },
    {
      "text": " Because a human being yes, there is definitely a foundation of skills.",
      "start": 2986.85,
      "end": 2992.75,
      "confidence": 0.90885943
    },
    {
      "text": " A human being.",
      "start": 2992.75,
      "end": 2994.74,
      "confidence": 0.85555124
    },
    {
      "text": " A human being lacks, a huge amount of knowledge. Instead, we rely on continued learning.",
      "start": 2994.74,
      "end": 3002.65,
      "confidence": 0.8047866
    },
    {
      "text": " We learn continual learning and so then when you think about okay, so let's suppose that we achieve success and we produce a safe super in some kind of safe. Superintelligence, the question is but how do you define it? Where on the curve of continual learning is going to be, I produce like a super intelligent 15 year old. That's very eager to go and you say okay I'm going to they don't know very much at all. The great students. Very eager, you go and be a programmer. You go and be a doctor.",
      "start": 3002.65,
      "end": 3030.68,
      "confidence": 0.8690683
    },
    {
      "text": "Go and learn. So you could imagine that the deployment itself will involve some kind of a learning trial and error period. It's a process as opposed to you drop the Finnish thing. Okay. I see so you're suggesting",
      "start": 3030.68,
      "end": 3046.69,
      "confidence": 0.952229
    },
    {
      "text": " that the thing, you're pointing out with super intelligence,",
      "start": 3046.69,
      "end": 3050.57,
      "confidence": 0.94286466
    },
    {
      "text": " Is not some finished.",
      "start": 3050.57,
      "end": 3053.71,
      "confidence": 0.98738456
    },
    {
      "text": " Mind which knows how to do every single job in the economy because the way Say the original I think opening HR or whatever defines AGI is like it can do every single job that every single thing a human can do.",
      "start": 3053.71,
      "end": 3067.88,
      "confidence": 0.94205666
    },
    {
      "text": " Your proposing instead.",
      "start": 3067.88,
      "end": 3069.84,
      "confidence": 0.8022949
    },
    {
      "text": " A mind which can learn to do any single every single job. Yes, and that is super intelligence and then, but once you have the learning algorithm,",
      "start": 3069.84,
      "end": 3079.13,
      "confidence": 0.9551664
    },
    {
      "text": " It gets deployed into the world the same way a human labor or my join an organization.",
      "start": 3079.13,
      "end": 3084.89,
      "confidence": 0.9272379
    },
    {
      "text": " And it seems like one of these two things might happen, maybe neither of these happens. One.",
      "start": 3084.89,
      "end": 3090.63,
      "confidence": 0.9624775
    },
    {
      "text": " This.",
      "start": 3090.63,
      "end": 3092.03,
      "confidence": 0.8279219
    },
    {
      "text": " Super efficient learning algorithm.",
      "start": 3092.03,
      "end": 3094.75,
      "confidence": 0.8650147
    },
    {
      "text": " becomes superhuman becomes as good as you and potentially even better at the task of",
      "start": 3094.75,
      "end": 3102.16,
      "confidence": 0.94348973
    },
    {
      "text": "Ml research. And as a result, the algorithm itself becomes more and more superhuman. The other is even if that doesn't happen.",
      "start": 3102.16,
      "end": 3110.32,
      "confidence": 0.96797746
    },
    {
      "text": " If you have a single model, I mean, this is explicitly your vision. If you have a single model where instances of a model,",
      "start": 3110.32,
      "end": 3116.49,
      "confidence": 0.93087393
    },
    {
      "text": " Which are deployed through the economy. Doing different jobs learning. How to do those jobs, continually learning on the job.",
      "start": 3116.49,
      "end": 3123.23,
      "confidence": 0.98672837
    },
    {
      "text": " Picking up all the skills that any human could pick up. But actually picking up all up at the same time and then the amalgamating the learnings",
      "start": 3123.23,
      "end": 3129.4,
      "confidence": 0.946767
    },
    {
      "text": " you basically have a model which functionally becomes super intelligent.",
      "start": 3129.4,
      "end": 3133.74,
      "confidence": 0.98147106
    },
    {
      "text": " Even without any sort of recursive, self-improvement in software.",
      "start": 3133.74,
      "end": 3138.25,
      "confidence": 0.91963726
    },
    {
      "text": " Right, because you now have one model that can do every single job in the economy and humans can't merge our minds in the same way.",
      "start": 3138.25,
      "end": 3144.54,
      "confidence": 0.97499824
    },
    {
      "text": " And so, do you expect some sort of, like, intelligence explosion from broad deployment? I think,",
      "start": 3144.54,
      "end": 3149.78,
      "confidence": 0.9346835
    },
    {
      "text": " That it is likely that we will have rapid economic growth.",
      "start": 3149.78,
      "end": 3155.39,
      "confidence": 0.886491
    },
    {
      "text": " I think the broad deployment.",
      "start": 3155.39,
      "end": 3159.62,
      "confidence": 0.94628394
    },
    {
      "text": " like,",
      "start": 3159.62,
      "end": 3161.69,
      "confidence": 0.8052025
    },
    {
      "text": " there are two arguments, you could make",
      "start": 3161.69,
      "end": 3164.17,
      "confidence": 0.94034255
    },
    {
      "text": " We check on flick team.",
      "start": 3164.17,
      "end": 3165.96,
      "confidence": 0.5894473
    },
    {
      "text": " One. Is that look if indeed you get once indeed.",
      "start": 3165.96,
      "end": 3170.6,
      "confidence": 0.86734354
    },
    {
      "text": " You get to a point.",
      "start": 3170.6,
      "end": 3172.53,
      "confidence": 0.97164506
    },
    {
      "text": "Where you have an AI that can learn to do.",
      "start": 3172.53,
      "end": 3176.47,
      "confidence": 0.98356944
    },
    {
      "text": " Things quickly.",
      "start": 3176.47,
      "end": 3178.53,
      "confidence": 0.9805478
    },
    {
      "text": " And you have many of them.",
      "start": 3178.53,
      "end": 3180.89,
      "confidence": 0.97330856
    },
    {
      "text": " Then they will, then they will be a strong.",
      "start": 3180.89,
      "end": 3184.47,
      "confidence": 0.9699932
    },
    {
      "text": " Force to deploy them in the economy unless they will be some kind of a regulation that stops IT. Which by the way they might be,",
      "start": 3184.47,
      "end": 3192.65,
      "confidence": 0.97205323
    },
    {
      "text": " but,",
      "start": 3192.65,
      "end": 3194.12,
      "confidence": 0.86970073
    },
    {
      "text": " I think the idea of very rapid economic growth for some time, I think it's very possible from broad deployment. The question is how rapid it's going to be",
      "start": 3194.12,
      "end": 3204.14,
      "confidence": 0.9489798
    },
    {
      "text": " So I think this is hard to know because on the one hand you have this very efficient worker. On the other hand, there is the world is just really big.",
      "start": 3204.14,
      "end": 3213.39,
      "confidence": 0.9676753
    },
    {
      "text": " And is a lot of stuff.",
      "start": 3213.39,
      "end": 3215.27,
      "confidence": 0.8306176
    },
    {
      "text": " And that's that stuff moves, the different speed but then on the other hand now the AI could, you know? So I think very rapid economic growth is possible and we will see like all kinds of things like",
      "start": 3215.27,
      "end": 3227.23,
      "confidence": 0.9262245
    },
    {
      "text": " Different countries with different rules and the ones which have the friendly rules. They'll be faster.",
      "start": 3227.23,
      "end": 3232.87,
      "confidence": 0.80694056
    },
    {
      "text": "Hard to predict some people in our audience, like to read the transcripts instead of listening to the episode. And so we put a ton of effort into making the transcripts read like they are Standalone essays. The problem is that if you just transcribe a conversation, verbatim using speech to text model, it'll be full of all kinds of fits and starts and confusing phrasing. We mentioned this problem to label box and they asked if they could take a staff working with them on this is probably the reason that I'm most excited to recommend Label Box to people. It wasn't just oh hey tell us what kind of data you need and we'll go get it. They walked us through the entire process from helping us identify what kind of data we needed in the first place to assembling a team of expert aligners to generate it. Even after we got all the data back, Lee will box stayed involved. They helped us choose the right base model and set up Auto QA on the models output so they could tweak and refine it. And now we have a new transcriber tool that we can use for all our episodes moving forward. This is just one example of how believable box meet.",
      "start": 3232.87,
      "end": 3292.89,
      "confidence": 0.5288921
    },
    {
      "text": " Their customers at the ideas level and partners with them through their entire Journey. If you want to learn more or if you want to try out the transcriber tool yourself, go to labelbox.com slash.",
      "start": 3292.89,
      "end": 3305.62,
      "confidence": 0.9020755
    },
    {
      "text": "it seems to me that this is a very precarious situation to be in where",
      "start": 3305.62,
      "end": 3312.74,
      "confidence": 0.98885804
    },
    {
      "text": " Look in the limit, we know that this should be possible because if you have something that is as good as a human at learning but which can merge, its brains merge, there are different instances in a way that humans can't merge.",
      "start": 3312.74,
      "end": 3325.75,
      "confidence": 0.95904446
    },
    {
      "text": " Already. This seems like a thing that should physically be possible. Humans are possible. Digital computers are possible. You just need both of those combined to produce this thing and it also seems like this kind of thing is",
      "start": 3325.75,
      "end": 3335.48,
      "confidence": 0.9754779
    },
    {
      "text": " extremely powerful.",
      "start": 3335.48,
      "end": 3339.51,
      "confidence": 0.9875097
    },
    {
      "text": " and,",
      "start": 3339.51,
      "end": 3341.19,
      "confidence": 0.86485463
    },
    {
      "text": " Economic growth is one way to put it. I mean Dyson, Sphere is a lot of economic growth, but another way to put it is just like you will have",
      "start": 3341.19,
      "end": 3348.84,
      "confidence": 0.9455655
    },
    {
      "text": " Potentially a very short period of time because a human on the job. Can, you know, you your hire people to SSI and six months or like net productive? Probably write a human like learns really fast. And so, this thing is becoming smarter and smarter very fast.",
      "start": 3348.84,
      "end": 3360.26,
      "confidence": 0.872705
    },
    {
      "text": " What is how do you think about making that go? Well, and why is SSI position to do that? Well, where does SSI plan? There basically is, what internet ask. Yeah.",
      "start": 3360.26,
      "end": 3368.77,
      "confidence": 0.8800694
    },
    {
      "text": " so,",
      "start": 3368.77,
      "end": 3371.13,
      "confidence": 0.8738876
    },
    {
      "text": " one of the, one of the ways in which,",
      "start": 3371.13,
      "end": 3373.82,
      "confidence": 0.9600936
    },
    {
      "text": " My thinking has been changing.",
      "start": 3373.82,
      "end": 3376.26,
      "confidence": 0.9598387
    },
    {
      "text": "Is that?",
      "start": 3376.26,
      "end": 3378.03,
      "confidence": 0.9048987
    },
    {
      "text": " I now Place more importance on AI.",
      "start": 3378.03,
      "end": 3383.99,
      "confidence": 0.9867398
    },
    {
      "text": " Being.",
      "start": 3383.99,
      "end": 3385.59,
      "confidence": 0.86915004
    },
    {
      "text": " Deployed.",
      "start": 3385.59,
      "end": 3387.38,
      "confidence": 0.981239
    },
    {
      "text": " Incrementally, and in advance.",
      "start": 3387.38,
      "end": 3390.31,
      "confidence": 0.9764868
    },
    {
      "text": " One very difficult thing about AI.",
      "start": 3390.31,
      "end": 3393.66,
      "confidence": 0.99213785
    },
    {
      "text": " Is that we are talking about systems that don't yet exist.",
      "start": 3393.66,
      "end": 3399.86,
      "confidence": 0.97214466
    },
    {
      "text": " And it's hard to imagine them.",
      "start": 3399.86,
      "end": 3402.64,
      "confidence": 0.96319884
    },
    {
      "text": " I think that one of the things that's happening,",
      "start": 3402.64,
      "end": 3405.95,
      "confidence": 0.9836889
    },
    {
      "text": " Is that in practice?",
      "start": 3405.95,
      "end": 3408.19,
      "confidence": 0.9866191
    },
    {
      "text": " It's very hard to feel AGI.",
      "start": 3408.19,
      "end": 3411.19,
      "confidence": 0.96752286
    },
    {
      "text": " It's very hard to feel the AGI.",
      "start": 3411.19,
      "end": 3413.91,
      "confidence": 0.89321154
    },
    {
      "text": " We can talk about it.",
      "start": 3413.91,
      "end": 3416.21,
      "confidence": 0.985061
    },
    {
      "text": " But it's like it's like talking about like the Longview, like imagine like having a conversation about like how is it like to be old?",
      "start": 3416.21,
      "end": 3424.27,
      "confidence": 0.9220102
    },
    {
      "text": " when you like all the in frail and you can have a conversation, you can try to imagine it, but",
      "start": 3424.27,
      "end": 3430.64,
      "confidence": 0.8727044
    },
    {
      "text": " It's just hard and you come back to reality, well, that's not the case. And I think that a lot of the",
      "start": 3430.64,
      "end": 3438.36,
      "confidence": 0.9675364
    },
    {
      "text": " Issues around.",
      "start": 3438.36,
      "end": 3440.73,
      "confidence": 0.973091
    },
    {
      "text": " AGI and its future power.",
      "start": 3440.73,
      "end": 3444.21,
      "confidence": 0.8887981
    },
    {
      "text": " Stem from the fact that it's very difficult to imagine.",
      "start": 3444.21,
      "end": 3450.43,
      "confidence": 0.96348363
    },
    {
      "text": "Future AI.",
      "start": 3450.43,
      "end": 3452.26,
      "confidence": 0.9562447
    },
    {
      "text": " Is going to be different. It's going to be powerful, indeed. The whole problem. What is the problem of an AGI?",
      "start": 3452.26,
      "end": 3459.83,
      "confidence": 0.9450186
    },
    {
      "text": " The whole problem is the power.",
      "start": 3459.83,
      "end": 3462.34,
      "confidence": 0.99453014
    },
    {
      "text": " The whole problem is the power.",
      "start": 3462.34,
      "end": 3465.03,
      "confidence": 0.99259734
    },
    {
      "text": " When the power is really big, what's going to happen?",
      "start": 3465.03,
      "end": 3469.57,
      "confidence": 0.87145585
    },
    {
      "text": " And one of the, one of the ways in which I've changed my mind over the past year. And so,",
      "start": 3469.57,
      "end": 3475.51,
      "confidence": 0.98613775
    },
    {
      "text": " That.",
      "start": 3475.51,
      "end": 3476.9,
      "confidence": 0.90646803
    },
    {
      "text": " that change of mind, may be",
      "start": 3476.9,
      "end": 3478.83,
      "confidence": 0.8196072
    },
    {
      "text": " May I'll say, I'll hedge a little bit, maybe back propagate into into the plans of our company.",
      "start": 3478.83,
      "end": 3485.57,
      "confidence": 0.90483654
    },
    {
      "text": " Is that?",
      "start": 3485.57,
      "end": 3487.31,
      "confidence": 0.924432
    },
    {
      "text": " so, if it's hard to imagine,",
      "start": 3487.31,
      "end": 3490.43,
      "confidence": 0.9686165
    },
    {
      "text": " What do you do? You got to be showing the thing?",
      "start": 3490.43,
      "end": 3494.5,
      "confidence": 0.8870235
    },
    {
      "text": " You gotta be showing the same.",
      "start": 3494.5,
      "end": 3496.21,
      "confidence": 0.8611372
    },
    {
      "text": " And I maintain that, I think, I think most people who work on AI. Also can't imagine it",
      "start": 3496.21,
      "end": 3502.63,
      "confidence": 0.95011574
    },
    {
      "text": " Because it's too different from what people see on the day-to-day basis.",
      "start": 3502.63,
      "end": 3507.09,
      "confidence": 0.91856927
    },
    {
      "text": " I do maintain. Here's something which I predict will happen. That's a prediction.",
      "start": 3507.09,
      "end": 3513.86,
      "confidence": 0.90714574
    },
    {
      "text": " I maintain.",
      "start": 3513.86,
      "end": 3516.18,
      "confidence": 0.86906064
    },
    {
      "text": " but as AI becomes more powerful,",
      "start": 3516.18,
      "end": 3519.97,
      "confidence": 0.87392473
    },
    {
      "text": " Than people will change their behaviors.",
      "start": 3519.97,
      "end": 3524.29,
      "confidence": 0.82755184
    },
    {
      "text": "And we will see all kinds of unprecedented things which are not happening right now.",
      "start": 3524.29,
      "end": 3531.01,
      "confidence": 0.98015374
    },
    {
      "text": " And I'll give some examples.",
      "start": 3531.01,
      "end": 3533.21,
      "confidence": 0.9120682
    },
    {
      "text": " I do like I think I think For Better or Worse the different tier companies, we play very important role and what happens as with the government",
      "start": 3533.21,
      "end": 3542.66,
      "confidence": 0.9409415
    },
    {
      "text": " and the kind of things that I think will see which",
      "start": 3542.66,
      "end": 3545.78,
      "confidence": 0.966666
    },
    {
      "text": " You see the beginning of?",
      "start": 3545.78,
      "end": 3547.85,
      "confidence": 0.8017127
    },
    {
      "text": " Companies that are Fierce competitors starting over to collaborate.",
      "start": 3547.85,
      "end": 3553.73,
      "confidence": 0.913613
    },
    {
      "text": " On AI safety. You may have seen, open Ai and Tropic events. Doing a first small step but that did not exist. That's actually something which I predicted in one of my talks about three years ago, that's such a thing will happen. I also maintain it as AI continues to become more",
      "start": 3553.73,
      "end": 3572.8,
      "confidence": 0.8892247
    },
    {
      "text": " Powerful more visibly powerful. They will also be a desire from governments and the public to do something.",
      "start": 3572.8,
      "end": 3581.33,
      "confidence": 0.9576276
    },
    {
      "text": " And I think that this is a very important Force.",
      "start": 3581.33,
      "end": 3584.86,
      "confidence": 0.95183945
    },
    {
      "text": " Of showing the AI. That's number one.",
      "start": 3584.86,
      "end": 3589,
      "confidence": 0.89197314
    },
    {
      "text": " Number two. Okay, so then the AI has been billed. What needs to what needs to be done?",
      "start": 3589,
      "end": 3593.79,
      "confidence": 0.9014399
    },
    {
      "text": " So, one thing that I maintain that will happen.",
      "start": 3593.79,
      "end": 3598.25,
      "confidence": 0.8529351
    },
    {
      "text": "is that right now people who are working on AI,",
      "start": 3598.25,
      "end": 3601.26,
      "confidence": 0.92835224
    },
    {
      "text": " I maintain that the AI doesn't feel powerful because of its mistakes.",
      "start": 3601.26,
      "end": 3605.9,
      "confidence": 0.9587349
    },
    {
      "text": " I do think that at some point, the AI will start to feel powerful. Actually,",
      "start": 3605.9,
      "end": 3610.45,
      "confidence": 0.9560722
    },
    {
      "text": " And I think when that happens, we will see a big change in the way.",
      "start": 3610.45,
      "end": 3615.08,
      "confidence": 0.96805245
    },
    {
      "text": " All AI companies approach safety.",
      "start": 3615.08,
      "end": 3618.99,
      "confidence": 0.95448047
    },
    {
      "text": " They'll become much more paranoid.",
      "start": 3618.99,
      "end": 3621.52,
      "confidence": 0.9410103
    },
    {
      "text": " I think I say this is a predict as a prediction that we will see happen. We'll see if I'm right.",
      "start": 3621.52,
      "end": 3627.67,
      "confidence": 0.9579374
    },
    {
      "text": " But I think this is something that will happen because they will see the AI becoming more powerful.",
      "start": 3627.67,
      "end": 3632.33,
      "confidence": 0.93748474
    },
    {
      "text": " Everything that's happening right now. I maintain it's because people look at today's Ai and it's hard to imagine the future AI",
      "start": 3632.33,
      "end": 3640.8,
      "confidence": 0.95171
    },
    {
      "text": " And there is a third thing which needs to happen.",
      "start": 3640.8,
      "end": 3644.82,
      "confidence": 0.9670044
    },
    {
      "text": " And I think this is this this and I'm talking about it in broader terms, not just from the perspective of SSI.",
      "start": 3644.82,
      "end": 3652.03,
      "confidence": 0.92118245
    },
    {
      "text": " Because you asking me about our company. But the question is okay. So then what should, what should the companies aspire to build? Yeah. What should they start to build?",
      "start": 3652.03,
      "end": 3659.94,
      "confidence": 0.9292058
    },
    {
      "text": " And there has been one big idea that actually that everyone has been locked in locked into which is the the self improving AI.",
      "start": 3659.94,
      "end": 3668.48,
      "confidence": 0.8714147
    },
    {
      "text": "And why did it happen? Because there is fewer ideas and companies",
      "start": 3668.48,
      "end": 3674.47,
      "confidence": 0.9444013
    },
    {
      "text": " But I maintain that there is something that's better to build.",
      "start": 3674.47,
      "end": 3678.33,
      "confidence": 0.87905
    },
    {
      "text": " And I think that everyone will actually want that.",
      "start": 3678.33,
      "end": 3680.98,
      "confidence": 0.9729291
    },
    {
      "text": " It's like,",
      "start": 3680.98,
      "end": 3682.54,
      "confidence": 0.979634
    },
    {
      "text": " The AI. That's robustly. Aligned to care about sentient life specifically.",
      "start": 3682.54,
      "end": 3688.42,
      "confidence": 0.909185
    },
    {
      "text": " I think in particular, it will be the case to be made that it will be easier to build an AI that cares about sentient life.",
      "start": 3688.42,
      "end": 3697.72,
      "confidence": 0.9598302
    },
    {
      "text": " Than any other cares about human life alone.",
      "start": 3697.72,
      "end": 3700.43,
      "confidence": 0.83647174
    },
    {
      "text": " Because the AI itself will be sentient.",
      "start": 3700.43,
      "end": 3703.24,
      "confidence": 0.9541151
    },
    {
      "text": " and if you think about things like mirror neurons and human empathy for animals, which is, you know, you might argue, it's not big enough but it exists",
      "start": 3703.24,
      "end": 3712.97,
      "confidence": 0.9694572
    },
    {
      "text": " I think it's an emergent property from the fact that the model others, with the same circuit that we use to model ourselves because that's the most efficient thing to do.",
      "start": 3712.97,
      "end": 3723.18,
      "confidence": 0.9641033
    },
    {
      "text": " So,",
      "start": 3723.18,
      "end": 3724.57,
      "confidence": 0.62312895
    },
    {
      "text": " even if you got an AI to hear about sentient being,",
      "start": 3724.57,
      "end": 3727.61,
      "confidence": 0.89392775
    },
    {
      "text": " And it's not actually clear to me that that's what you should try to do if you solve the lineman.",
      "start": 3727.61,
      "end": 3731.56,
      "confidence": 0.8819641
    },
    {
      "text": " Would be the case. That most Indian beings will be AI, is there will be trillions eventually quadrillions of ai's? Humans will be a very small fraction of sentient beings.",
      "start": 3731.56,
      "end": 3742.49,
      "confidence": 0.8565367
    },
    {
      "text": "so,",
      "start": 3742.49,
      "end": 3744.28,
      "confidence": 0.9825047
    },
    {
      "text": " It's not clear to me if the goal is some kind of human control over.",
      "start": 3744.28,
      "end": 3748.61,
      "confidence": 0.9926342
    },
    {
      "text": " This future civilization.",
      "start": 3748.61,
      "end": 3752.16,
      "confidence": 0.7918064
    },
    {
      "text": " That this is the best Criterion. It's true. I I think that",
      "start": 3752.16,
      "end": 3758.1,
      "confidence": 0.96829826
    },
    {
      "text": " it's possible. It's not the best Criterion. I'll say, two things.",
      "start": 3758.1,
      "end": 3762.27,
      "confidence": 0.9630844
    },
    {
      "text": " I think that the number one,",
      "start": 3762.27,
      "end": 3766.12,
      "confidence": 0.8527065
    },
    {
      "text": " I think that if they're so,",
      "start": 3766.12,
      "end": 3769.99,
      "confidence": 0.92779046
    },
    {
      "text": " I think that care for sentient life. I think there is married to it. I think it should be considered.",
      "start": 3769.99,
      "end": 3775.75,
      "confidence": 0.94464624
    },
    {
      "text": " I think that it will be helpful if there was some kind of",
      "start": 3775.75,
      "end": 3780,
      "confidence": 0.98477453
    },
    {
      "text": " A shortlist over ideas.",
      "start": 3780,
      "end": 3783.57,
      "confidence": 0.6819272
    },
    {
      "text": " that then,",
      "start": 3783.57,
      "end": 3785.22,
      "confidence": 0.8757448
    },
    {
      "text": " The companies when they are in the situation, could you use? That's number two, number three. I think it would be really materially helpful. If",
      "start": 3785.22,
      "end": 3794.03,
      "confidence": 0.9462106
    },
    {
      "text": " The.",
      "start": 3794.03,
      "end": 3795.14,
      "confidence": 0.88279504
    },
    {
      "text": " Power of the most powerful super intelligence for somehow capped.",
      "start": 3795.14,
      "end": 3799.36,
      "confidence": 0.9433147
    },
    {
      "text": " Because it would address a lot of these concerns.",
      "start": 3799.36,
      "end": 3802.61,
      "confidence": 0.9614203
    },
    {
      "text": " The question of how to do it.",
      "start": 3802.61,
      "end": 3805.35,
      "confidence": 0.9529701
    },
    {
      "text": " I'm not sure but I think that would be materially helpful.",
      "start": 3805.35,
      "end": 3809.1,
      "confidence": 0.957926
    },
    {
      "text": " When you're talking about really, really powerful systems. Yeah, um, before we continue the 11 discussion, I want to double click on that. How much room is there at the top? How do you think about super intelligence? Do you think?",
      "start": 3809.1,
      "end": 3821.47,
      "confidence": 0.858012
    },
    {
      "text": "I mean using this learning efficiency idea, maybe it's just extremely fast at learning new skills or new knowledge and does it just have a bigger pool of strategies? Is there a single cohesive it?",
      "start": 3821.47,
      "end": 3833.46,
      "confidence": 0.9532402
    },
    {
      "text": " In the center, that's more powerful, or bigger. And if so,",
      "start": 3833.46,
      "end": 3838.79,
      "confidence": 0.9846647
    },
    {
      "text": " Do you do you imagine that this will be sort of Godlike in comparison to the rest of human civilization? Or does it just feel like another agent or another cluster of Agents?",
      "start": 3838.79,
      "end": 3848.09,
      "confidence": 0.9545067
    },
    {
      "text": " So this is an area of a different people of different intuitions. I think it will be very powerful for sure.",
      "start": 3848.09,
      "end": 3854.46,
      "confidence": 0.9588505
    },
    {
      "text": " I think that what I think is most likely to happen.",
      "start": 3854.46,
      "end": 3858.76,
      "confidence": 0.98809475
    },
    {
      "text": " Is that there will be multiple.",
      "start": 3858.76,
      "end": 3861.83,
      "confidence": 0.75080633
    },
    {
      "text": " Such ai's.",
      "start": 3861.83,
      "end": 3864.27,
      "confidence": 0.6275774
    },
    {
      "text": " Being created roughly the same time.",
      "start": 3864.27,
      "end": 3866.79,
      "confidence": 0.9445329
    },
    {
      "text": " I think that.",
      "start": 3866.79,
      "end": 3869.2,
      "confidence": 0.96052724
    },
    {
      "text": " if the cluster is big enough,",
      "start": 3869.2,
      "end": 3872.63,
      "confidence": 0.9770918
    },
    {
      "text": " Like the cluster is literally Continental size.",
      "start": 3872.63,
      "end": 3876.21,
      "confidence": 0.7360287
    },
    {
      "text": " That's in could be really powerful indeed, right? If you literally have a continent sized cluster like those, those guys can be very powerful and I",
      "start": 3876.21,
      "end": 3886.39,
      "confidence": 0.90612113
    },
    {
      "text": " Can tell you.",
      "start": 3886.39,
      "end": 3888.01,
      "confidence": 0.9791371
    },
    {
      "text": " Is that if you're talking about extremely powerful aisle like truly dramatically powerful, then yeah. It would be nice if they could be restrained in some ways.",
      "start": 3888.01,
      "end": 3898.75,
      "confidence": 0.9342441
    },
    {
      "text": "for if there was some kind of an agreement or something,",
      "start": 3898.75,
      "end": 3902.61,
      "confidence": 0.93312895
    },
    {
      "text": " because I think that if you are saying, hey, like, if you really",
      "start": 3902.61,
      "end": 3907.61,
      "confidence": 0.9462842
    },
    {
      "text": " Like what is the the concern of superintelligence? What is one way to explain the concern?",
      "start": 3907.61,
      "end": 3913.14,
      "confidence": 0.96425915
    },
    {
      "text": " If you imagine a system that is sufficiently powerful like really sufficiently powerful.",
      "start": 3913.14,
      "end": 3919.8,
      "confidence": 0.9784321
    },
    {
      "text": " And you could say okay you need to do something sensible like care for sentient life. Let's say in a very single minded way.",
      "start": 3919.8,
      "end": 3927.02,
      "confidence": 0.9622149
    },
    {
      "text": " We might not like the results that's really what it is. And so maybe by the way, the answer is that you do not build a single. You do not build an RL agent in the usual sense and actually I'll points several things out. I think human beings are a semi religion",
      "start": 3927.02,
      "end": 3942.94,
      "confidence": 0.92733073
    },
    {
      "text": " You know, we pursue reward and then the emotions or whatever, make a style out of the reward, you from reward.",
      "start": 3942.94,
      "end": 3949.99,
      "confidence": 0.86601585
    },
    {
      "text": " The market is like kind it's like a very short sighted kind of agent. Evolution is the same. Evolution is very intelligent in some ways but very dumb in other ways. The government has been designed to be a never-ending fight between three parts which has an effect. So I think things like this.",
      "start": 3949.99,
      "end": 3969.98,
      "confidence": 0.9272692
    },
    {
      "text": "Another thing that makes this discussion difficult is that we are talking about systems that don't exist. That we don't know how to build.",
      "start": 3969.98,
      "end": 3978.49,
      "confidence": 0.9745307
    },
    {
      "text": " Right? That's the other thing and that's actually my belief, I think what people are doing right now, will go some distance and then Peter out if you continue to improve, but it will also not be it.",
      "start": 3978.49,
      "end": 3989.73,
      "confidence": 0.9743272
    },
    {
      "text": " so, the it",
      "start": 3989.73,
      "end": 3991.08,
      "confidence": 0.91074103
    },
    {
      "text": " We don't know how to build.",
      "start": 3991.08,
      "end": 3992.81,
      "confidence": 0.992701
    },
    {
      "text": " and I think that a lot in a lot hinges on,",
      "start": 3992.81,
      "end": 3996.64,
      "confidence": 0.84306043
    },
    {
      "text": " Understanding reliable. Generalization.",
      "start": 3996.64,
      "end": 4000,
      "confidence": 0.9422578
    },
    {
      "text": " now, say another thing which is like",
      "start": 4000,
      "end": 4004.62,
      "confidence": 0.9120967
    },
    {
      "text": " You know, one of the things that you could say is what would that cause alignment to be difficult? Is that human? Well, I think it's, it's",
      "start": 4004.62,
      "end": 4012.4,
      "confidence": 0.8778668
    },
    {
      "text": " your ability to learn human values is fragile, then you'll ability to optimize them his fragile, you will, you actually don't optimize them and I can't, you say, are these, not all instances of unreliable generalization",
      "start": 4012.4,
      "end": 4024.58,
      "confidence": 0.84382236
    },
    {
      "text": " Why is it that human beings appear to generalize so much better? What do generalization was much better? What would happen in this case? What would be their effect? But those we can't, we can't. But those questions are right now, still an answerable. How does one think about",
      "start": 4024.58,
      "end": 4041.58,
      "confidence": 0.8927674
    },
    {
      "text": "What AI going? Well, looks like because I think you've scoped out",
      "start": 4041.58,
      "end": 4045.94,
      "confidence": 0.90168804
    },
    {
      "text": " how AI might have all of these sort of continual learning, Asians. I will be very powerful. Maybe there will be many different AIS. How do you think about lots of content compute size intelligence is going around?",
      "start": 4045.94,
      "end": 4059.14,
      "confidence": 0.93545943
    },
    {
      "text": " How dangerous is that?",
      "start": 4059.14,
      "end": 4061.76,
      "confidence": 0.98611563
    },
    {
      "text": " How do we make that less dangerous?",
      "start": 4061.76,
      "end": 4064.67,
      "confidence": 0.96643436
    },
    {
      "text": " and how do we do that in a way that",
      "start": 4064.67,
      "end": 4066.76,
      "confidence": 0.96836776
    },
    {
      "text": " Protects a equilibrium where there might be misaligned AIS out there and Bad actors out there.",
      "start": 4066.76,
      "end": 4075.69,
      "confidence": 0.92737544
    },
    {
      "text": " So one reason why I like the AI that cares for sentient life, you know, and we can debate on whether it's good or bad but",
      "start": 4075.69,
      "end": 4084.14,
      "confidence": 0.79645324
    },
    {
      "text": " if the first end of these dramatic systems, actually do care for, you know,",
      "start": 4084.14,
      "end": 4092.9,
      "confidence": 0.96251357
    },
    {
      "text": " love Humanity or something, you know, careful sentient life obviously this also needs to be achieved",
      "start": 4092.9,
      "end": 4099.84,
      "confidence": 0.9193442
    },
    {
      "text": " This needs to be achieved.",
      "start": 4099.84,
      "end": 4101.6,
      "confidence": 0.9595151
    },
    {
      "text": " So if this is achieved by the first end of those systems,",
      "start": 4101.6,
      "end": 4105.85,
      "confidence": 0.9720375
    },
    {
      "text": " then I can see it go well.",
      "start": 4105.85,
      "end": 4109.73,
      "confidence": 0.93499327
    },
    {
      "text": " At least four quite some time. And then there is the question of what happens in the long run. What happens in the long run, how do you achieve a long run equilibrium?",
      "start": 4109.73,
      "end": 4119.04,
      "confidence": 0.8787358
    },
    {
      "text": "and I think that,",
      "start": 4119.04,
      "end": 4121.32,
      "confidence": 0.91255236
    },
    {
      "text": " There, the reason answer is well.",
      "start": 4121.32,
      "end": 4124.49,
      "confidence": 0.88109523
    },
    {
      "text": " And I don't like this answer.",
      "start": 4124.49,
      "end": 4126.51,
      "confidence": 0.8815603
    },
    {
      "text": " But it needs to be considered.",
      "start": 4126.51,
      "end": 4129.81,
      "confidence": 0.98085046
    },
    {
      "text": " In the long run you might say. Okay, so if you have a world where powerful eyes exist,",
      "start": 4129.81,
      "end": 4136.63,
      "confidence": 0.94095314
    },
    {
      "text": " In the short term you could say okay you have Universal High income. If Universal High income",
      "start": 4136.63,
      "end": 4142.9,
      "confidence": 0.8705957
    },
    {
      "text": " and we all doing well, but we know that",
      "start": 4142.9,
      "end": 4146.15,
      "confidence": 0.9237887
    },
    {
      "text": " What do the Buddhists say changes? The only constant and so things change and there is some kind of government political structure thing and it changes because these things have a shelf life.",
      "start": 4146.15,
      "end": 4158.08,
      "confidence": 0.9099088
    },
    {
      "text": " you know, some new government think comes up and it functions and after sometime it stops functioning,",
      "start": 4158.08,
      "end": 4163.88,
      "confidence": 0.8975235
    },
    {
      "text": " That's something that you see happening all the time.",
      "start": 4163.88,
      "end": 4167.16,
      "confidence": 0.9309205
    },
    {
      "text": " And so, I think that for the long run equilibrium,",
      "start": 4167.16,
      "end": 4170.59,
      "confidence": 0.8415745
    },
    {
      "text": " One approach, you could say, okay, so maybe every person will have an AI that will do their bidding.",
      "start": 4170.59,
      "end": 4178.24,
      "confidence": 0.91887105
    },
    {
      "text": " And that's good.",
      "start": 4178.24,
      "end": 4179.96,
      "confidence": 0.91116434
    },
    {
      "text": " And if that could be maintained indefinitely, that's true. But the downside with that is, okay. So then the AI goes and like,",
      "start": 4179.96,
      "end": 4188.38,
      "confidence": 0.92736906
    },
    {
      "text": " earns earn, you know, earns money for the person and, you know, advocates for their needs and like the political sphere",
      "start": 4188.38,
      "end": 4195.28,
      "confidence": 0.8740009
    },
    {
      "text": "And maybe then writes, it will report saying, okay here's what I've done. Here's the situation and the process is great, keep it up.",
      "start": 4195.28,
      "end": 4201.55,
      "confidence": 0.90460867
    },
    {
      "text": " But the person is no longer a participant.",
      "start": 4201.55,
      "end": 4204.74,
      "confidence": 0.8985372
    },
    {
      "text": " and then you can say, that's a precarious place to be in, but so I'm going to preface by saying",
      "start": 4204.74,
      "end": 4212.26,
      "confidence": 0.91961324
    },
    {
      "text": " I don't like this solution but it is a solution.",
      "start": 4212.26,
      "end": 4217.47,
      "confidence": 0.9876645
    },
    {
      "text": " And the solution is if people become part AI with some kind of neuralink plus plus. Because what will happen is a result, is that now the AI understands something.",
      "start": 4217.47,
      "end": 4227.56,
      "confidence": 0.9172591
    },
    {
      "text": " and we understand it too like",
      "start": 4227.56,
      "end": 4230.45,
      "confidence": 0.85976774
    },
    {
      "text": " because now they understanding is transmitted whole sale. So now the AI is in some situation.",
      "start": 4230.45,
      "end": 4236.41,
      "confidence": 0.8800135
    },
    {
      "text": " Now, it's like you are in the situation yourself fully.",
      "start": 4236.41,
      "end": 4240.89,
      "confidence": 0.9266432
    },
    {
      "text": " And I think this is the answer to the equilibrium.",
      "start": 4240.89,
      "end": 4244.57,
      "confidence": 0.78366023
    },
    {
      "text": " I wonder if the fact that emotions which were",
      "start": 4244.57,
      "end": 4248.98,
      "confidence": 0.9712057
    },
    {
      "text": " Developed.",
      "start": 4248.98,
      "end": 4250.94,
      "confidence": 0.96555674
    },
    {
      "text": " Millions or many cases billions of years ago in a totally different. Environment are still guiding our actions so strongly",
      "start": 4250.94,
      "end": 4260.27,
      "confidence": 0.93559885
    },
    {
      "text": " Is an example of alignment success. Maybe spell out what I mean.",
      "start": 4260.27,
      "end": 4265.83,
      "confidence": 0.91502833
    },
    {
      "text": "The brainstem has these.",
      "start": 4265.83,
      "end": 4268.43,
      "confidence": 0.81309843
    },
    {
      "text": " I don't know if it's more accurate to call it a value function or word function, but the brain stem has a direct effort saying, mate, with somebody who's more successful. The cortex is the part that understands what a success mean in the modern context.",
      "start": 4268.43,
      "end": 4281.81,
      "confidence": 0.94214827
    },
    {
      "text": " But the brain stem is able to align the cortex and say, however you recognize success to be and I'm not smart enough to understand what that is. You're still going to pursue this directive. I think I think there is",
      "start": 4281.81,
      "end": 4293.76,
      "confidence": 0.9515514
    },
    {
      "text": " so I think there's a more General point. I think it's actually really mysterious how",
      "start": 4293.76,
      "end": 4300.05,
      "confidence": 0.9744297
    },
    {
      "text": " The brain and codes, high level desires. So how Evolution and Godzilla level desires, like it's pretty easy to understand how Evolution would would endow us with the desire for food. That smells good because smell is a chemical",
      "start": 4300.05,
      "end": 4315.84,
      "confidence": 0.85409606
    },
    {
      "text": " And so, just pursue that can make all it's very easy to imagine such a revolution, doing such a thing.",
      "start": 4315.84,
      "end": 4322.17,
      "confidence": 0.77433676
    },
    {
      "text": " but,",
      "start": 4322.17,
      "end": 4323.74,
      "confidence": 0.6478689
    },
    {
      "text": " Evolution also has endowed us with only these social desires, like we really care about being seen positively by Society. We care about being a good standing, the",
      "start": 4323.74,
      "end": 4335.4,
      "confidence": 0.8765661
    },
    {
      "text": " Like all these social intuitions that we have.",
      "start": 4335.4,
      "end": 4339.5,
      "confidence": 0.9598059
    },
    {
      "text": "Feel strongly that they are baked in.",
      "start": 4339.5,
      "end": 4341.97,
      "confidence": 0.84976333
    },
    {
      "text": " And I don't know how Evolution did it because it's a high level concept. It's represented in the brain",
      "start": 4341.97,
      "end": 4349.23,
      "confidence": 0.96648556
    },
    {
      "text": " like what people think like, let's say you are like you care about",
      "start": 4349.23,
      "end": 4354.36,
      "confidence": 0.9714997
    },
    {
      "text": " Some social thing.",
      "start": 4354.36,
      "end": 4356.35,
      "confidence": 0.95407075
    },
    {
      "text": " It's not like a low level signal like smell. It's not something that for which there is a sensor like the brain needs to do a lot of processing to piece together. Lots of bits of information to understand what's going on social and somehow Evolution said, that's what you should care about. Yes, how do you do it? And he did it quickly too. Yeah, because I think all these sophisticated social things that we care about, I think they've both pretty recently. So, Evolution had an easy time hardcore in these high level desire and",
      "start": 4356.35,
      "end": 4389.44,
      "confidence": 0.88443625
    },
    {
      "text": " I maintain or, you know, at least I'll say I'm unaware of good hypothesis for how it's done. I had some ideas. I was kicking around but none of them.",
      "start": 4389.44,
      "end": 4400.33,
      "confidence": 0.9508043
    },
    {
      "text": " None of them.",
      "start": 4400.33,
      "end": 4402.37,
      "confidence": 0.9510816
    },
    {
      "text": " Are satisfying. Yeah, and what's special impressive. It was a desire that you learned in your lifetime. It kind of makes sense because your brain is intelligent. It makes sense why we would learn intelligent desires, but your point is that the desire is maybe this is not your point, but one way to understand it is,",
      "start": 4402.37,
      "end": 4419.94,
      "confidence": 0.8962489
    },
    {
      "text": "The desire is built into the genome and the genome is not intelligent, right? But it's able to, you're somehow able to describe this feature that requires, like, it's not even clear how you define that feature and you can get it into the, you can build it into the jeans. Yeah, essentially, or maybe I'll put a differently if you think about the tools that are available to the genome,",
      "start": 4419.94,
      "end": 4439.72,
      "confidence": 0.97930443
    },
    {
      "text": " It says okay here's a recipe for building a brain and you could say here is the recipe for connecting the dopamine neurons to like the smell sense. Yeah.",
      "start": 4439.72,
      "end": 4448.78,
      "confidence": 0.9198257
    },
    {
      "text": " And if the smell is a certain kind of, you know, good smell. You want to eat that.",
      "start": 4448.78,
      "end": 4452.55,
      "confidence": 0.9084151
    },
    {
      "text": " I could imagine the genome doing that.",
      "start": 4452.55,
      "end": 4454.95,
      "confidence": 0.9108873
    },
    {
      "text": " I'm claiming that it is harder to imagine.",
      "start": 4454.95,
      "end": 4458.58,
      "confidence": 0.96576375
    },
    {
      "text": " It's harder to imagine the genome saying you should care about.",
      "start": 4458.58,
      "end": 4462.92,
      "confidence": 0.9635391
    },
    {
      "text": " Some complicated computation that your entire that like a big chunk of your brain does. That's all I'm claiming. I can tell you like a speculation. I was wondering how it could be done and let me offer speculation and I'll explain why this speculation is probably false.",
      "start": 4462.92,
      "end": 4477.36,
      "confidence": 0.90861326
    },
    {
      "text": " so, this speculation is",
      "start": 4477.36,
      "end": 4479.37,
      "confidence": 0.750406
    },
    {
      "text": " Okay.",
      "start": 4479.37,
      "end": 4480.71,
      "confidence": 0.6975294
    },
    {
      "text": " so, the brain it's like,",
      "start": 4480.71,
      "end": 4484.37,
      "confidence": 0.965799
    },
    {
      "text": " The brain has those regions, you know, the brain regions.",
      "start": 4484.37,
      "end": 4487.42,
      "confidence": 0.89040565
    },
    {
      "text": " We have our cortex, right? And as always brain regions,",
      "start": 4487.42,
      "end": 4491.48,
      "confidence": 0.79015255
    },
    {
      "text": "and the cortex is uniform, but the brain regions and, and the neurons in the cortex, they kind of speak to their neighbors, mostly",
      "start": 4491.48,
      "end": 4498.73,
      "confidence": 0.92706466
    },
    {
      "text": " and that's explains why you get brain regions because if you want to do some kind of speech processing,",
      "start": 4498.73,
      "end": 4503.64,
      "confidence": 0.9089001
    },
    {
      "text": " all the neurons that do speech in to talk to each other and they can because neurons can only speak to their nearby neighbors. For the most part, it has to be a region.",
      "start": 4503.64,
      "end": 4511.06,
      "confidence": 0.95333076
    },
    {
      "text": " All the regions are mostly located in the same place from person to person.",
      "start": 4511.06,
      "end": 4515.1,
      "confidence": 0.9688067
    },
    {
      "text": " So, maybe Evolution hardcoded literally a location on the brain.",
      "start": 4515.1,
      "end": 4519.03,
      "confidence": 0.9349001
    },
    {
      "text": " so, it's all like when when like, you know,",
      "start": 4519.03,
      "end": 4524.13,
      "confidence": 0.84593666
    },
    {
      "text": " The GPS of the brain GPS coordinates such and such when that fires, that's what should care about. Like maybe that's what evolution did because that would be within the toolkit of evolution.",
      "start": 4524.13,
      "end": 4532.9,
      "confidence": 0.85366935
    },
    {
      "text": " yeah, although there are examples where",
      "start": 4532.9,
      "end": 4535.87,
      "confidence": 0.9657352
    },
    {
      "text": " for example, people who are born, blind have that area of their cortex adopted by",
      "start": 4535.87,
      "end": 4541.52,
      "confidence": 0.93519324
    },
    {
      "text": " Another sense.",
      "start": 4541.52,
      "end": 4543.78,
      "confidence": 0.9427185
    },
    {
      "text": " And I have no idea, but I'd be surprised if the desires or the reward functions which require visual signal, no longer worked, you know, people who have their different areas of their cortex. Co-opted for example, if you no longer have",
      "start": 4543.78,
      "end": 4561.24,
      "confidence": 0.923459
    },
    {
      "text": " Vision.",
      "start": 4561.24,
      "end": 4562.72,
      "confidence": 0.83882856
    },
    {
      "text": "Can you still feel the sense that I want people around me to like me and so forth which usually there's also visual cues for so actually fully agree with that. I I think there's an even stronger Contra argument to this Theory which is like if you think about people. So there are people",
      "start": 4562.72,
      "end": 4578.19,
      "confidence": 0.96507823
    },
    {
      "text": " who get half of their brains removed in a childhood know.",
      "start": 4578.19,
      "end": 4583.19,
      "confidence": 0.90140474
    },
    {
      "text": " And they still have all the rich brain regions, but they all somehow move to just one hemisphere which suggests that the brain regions, their location is not fixed and so that theory is not true. It would have been cool if it was true but it's not. And so I think that's a mystery but it's an interesting mystery like the fact is somehow",
      "start": 4583.19,
      "end": 4600.53,
      "confidence": 0.9415342
    },
    {
      "text": "Evolution was able to endow us to care about social stuff, very, very reliably and even people who have like all kinds of strange mental conditions, and deficiencies and emotional problems tend to care about this. Also, the air tools like defects voice, clones and agents have dramatically increased the sophistication of Fraud and Abuse. So it's more important than ever to actually understand the identity and intent of whoever, or whatever is using your platform. That's exactly what sardine helps you do starting brings together, thousands of device behavior. And identity signals to help you assess risk everything from Howell user types or moves their mouths or holds their device to whether they're hiding their true location behind the VPN to, whether they're injecting a fake camera feed during kyc. Selfie checks, sardine combines these signals with insights from their network of almost 4 billion devices, things like a user history of fraud or they're assoc.",
      "start": 4600.53,
      "end": 4661.23,
      "confidence": 0.5288362
    },
    {
      "text": "Sessions with other high-risk accounts.",
      "start": 4661.23,
      "end": 4663.63,
      "confidence": 0.6680827
    },
    {
      "text": " You can spot that actors before they do damage, this would literally be impossible if you only use data from your own application. Sardine doesn't stop a detection. They offer Suite of agents to streamline onboarding tracks and automated investigations. So as fraudsters, use AI to scale their attacks. You can use AI to scale your defenses. Go to sardine.ai, cash to learn more and download their guide on AI fraud detection.",
      "start": 4663.63,
      "end": 4692.29,
      "confidence": 0.950307
    },
    {
      "text": " What is SSI playing on doing differently?",
      "start": 4692.29,
      "end": 4696.01,
      "confidence": 0.9187299
    },
    {
      "text": " So presumably, your plan is to be one of the frontier companies. When this time arrives,",
      "start": 4696.01,
      "end": 4700.87,
      "confidence": 0.9015237
    },
    {
      "text": " And then, what is?",
      "start": 4700.87,
      "end": 4703.47,
      "confidence": 0.8993737
    },
    {
      "text": " presumably, you started SSI because you're like, I think I have a way of approaching how to do the safely in a way that the other companies don't",
      "start": 4703.47,
      "end": 4711.8,
      "confidence": 0.94856584
    },
    {
      "text": " What is that difference?",
      "start": 4711.8,
      "end": 4713.81,
      "confidence": 0.88272953
    },
    {
      "text": " So, the way I would describe it as",
      "start": 4713.81,
      "end": 4716.47,
      "confidence": 0.8086708
    },
    {
      "text": " There are some ideas that I think a promising and I want to investigate them and see if they are indeed promising or not it's really that simple. It's an attempt.",
      "start": 4716.47,
      "end": 4727.26,
      "confidence": 0.9247624
    },
    {
      "text": " I think that if the ideas turn out to be correct, these ideas that we discussed around on understanding generalization.",
      "start": 4727.26,
      "end": 4735.67,
      "confidence": 0.84748095
    },
    {
      "text": "And then I think we will have something Worthy.",
      "start": 4735.67,
      "end": 4742.88,
      "confidence": 0.84916365
    },
    {
      "text": " Will they turn out to be correct? We are doing research. We are squarely age of research company. We are making progress. We've actually made quite a good progress of the past year, but we need to keep making more progress, more research.",
      "start": 4742.88,
      "end": 4755.83,
      "confidence": 0.9293408
    },
    {
      "text": " And that's how I see it. I see it as an attempt.",
      "start": 4755.83,
      "end": 4759.78,
      "confidence": 0.9811241
    },
    {
      "text": " To be.",
      "start": 4759.78,
      "end": 4761.38,
      "confidence": 0.9710014
    },
    {
      "text": " An attempt to be a voice and a participant.",
      "start": 4761.38,
      "end": 4765.98,
      "confidence": 0.88391566
    },
    {
      "text": " People have asked.",
      "start": 4765.98,
      "end": 4768.85,
      "confidence": 0.9930496
    },
    {
      "text": " Your co-founder and previously left to go to meta recently.",
      "start": 4768.85,
      "end": 4775.21,
      "confidence": 0.83617145
    },
    {
      "text": " And people have asked, well, if there was a lot of breakthroughs being made, that's like a thing that should have been unlikely. I wonder how you respond. Yes. So I for this I will simply remind a few facts",
      "start": 4775.21,
      "end": 4788.26,
      "confidence": 0.91085166
    },
    {
      "text": " That may have been forgotten. And I think this these facts which provide the contacts, I think they explained the situation.",
      "start": 4788.26,
      "end": 4795.32,
      "confidence": 0.92421407
    },
    {
      "text": " So, the context was that we were fundraising at the 32 billion.",
      "start": 4795.32,
      "end": 4801.15,
      "confidence": 0.8664631
    },
    {
      "text": " And then meta.",
      "start": 4801.15,
      "end": 4804.74,
      "confidence": 0.78301305
    },
    {
      "text": " Came in and offered to acquire us.",
      "start": 4804.74,
      "end": 4807.57,
      "confidence": 0.6701673
    },
    {
      "text": " And I said, no.",
      "start": 4807.57,
      "end": 4810.23,
      "confidence": 0.968464
    },
    {
      "text": "But my former co-founder.",
      "start": 4810.23,
      "end": 4812.88,
      "confidence": 0.925178
    },
    {
      "text": " Like in some sense, said yes.",
      "start": 4812.88,
      "end": 4816.07,
      "confidence": 0.9291641
    },
    {
      "text": " And as a result, he also was able to enjoy from a lot of near-term. Liquidity.",
      "start": 4816.07,
      "end": 4821.4,
      "confidence": 0.9492805
    },
    {
      "text": " And he was the only person from SSI to join meta.",
      "start": 4821.4,
      "end": 4825.36,
      "confidence": 0.96019226
    },
    {
      "text": " It sounds like exercise plan is to be a company that is at the frontier. When you get to this",
      "start": 4825.36,
      "end": 4830.41,
      "confidence": 0.9172279
    },
    {
      "text": " very important period in human history.",
      "start": 4830.41,
      "end": 4833.33,
      "confidence": 0.97636056
    },
    {
      "text": " Where you have superhuman intelligence and you have these ideas about how to make superhuman intelligence go. Well,",
      "start": 4833.33,
      "end": 4838.71,
      "confidence": 0.94982064
    },
    {
      "text": " But other companies will be trying their own ideas.",
      "start": 4838.71,
      "end": 4842.03,
      "confidence": 0.9758004
    },
    {
      "text": " What? Distinguishes, ssi's approach to making super intelligence go. Well, the main thing that distinguishes SSI is its technical approach.",
      "start": 4842.03,
      "end": 4854.12,
      "confidence": 0.91867936
    },
    {
      "text": " So we have a different technical approach that I think is Worthy.",
      "start": 4854.12,
      "end": 4858.2,
      "confidence": 0.9022164
    },
    {
      "text": " And we are pursuing it.",
      "start": 4858.2,
      "end": 4860.64,
      "confidence": 0.8235656
    },
    {
      "text": " I maintain that in the end, there will be a convergence of strategy. So I think there will be a convergence of strategies where",
      "start": 4860.64,
      "end": 4869.12,
      "confidence": 0.9644663
    },
    {
      "text": " at some point.",
      "start": 4869.12,
      "end": 4871.58,
      "confidence": 0.9740753
    },
    {
      "text": " As AI becomes more powerful.",
      "start": 4871.58,
      "end": 4874.27,
      "confidence": 0.7835501
    },
    {
      "text": " it's going to become more or less clear to everyone what the strategy should be and it should be something like yeah you need to find some way to talk to each other and you want your first",
      "start": 4874.27,
      "end": 4885.86,
      "confidence": 0.9624713
    },
    {
      "text": "actual like real, super intelligent AI to be aligned and somehow be",
      "start": 4885.86,
      "end": 4892.89,
      "confidence": 0.94121146
    },
    {
      "text": " You know.",
      "start": 4892.89,
      "end": 4896.42,
      "confidence": 0.9826027
    },
    {
      "text": " careful sentient life care for people Democratic one of those, some combination of thereof and I think",
      "start": 4896.42,
      "end": 4903.4,
      "confidence": 0.9665637
    },
    {
      "text": " This is the condition.",
      "start": 4903.4,
      "end": 4906.36,
      "confidence": 0.96997154
    },
    {
      "text": " That everyone should strive for.",
      "start": 4906.36,
      "end": 4909.73,
      "confidence": 0.97836685
    },
    {
      "text": " And that's what the size striving for.",
      "start": 4909.73,
      "end": 4912.33,
      "confidence": 0.86386913
    },
    {
      "text": " And I think that this time, if not already, all the other companies will realize, in the striving towards the same thing. And we'll see, I think that the world will truly change, the are become more powerful. Yeah, and I think a lot of these forecasts will like,",
      "start": 4912.33,
      "end": 4927.58,
      "confidence": 0.9292297
    },
    {
      "text": " I think things will be really different and people will be acting really differently. What is speaking of forecast? What are your forecasts to this system? You're describing which can learn, as well, as a human.",
      "start": 4927.58,
      "end": 4938.99,
      "confidence": 0.8805591
    },
    {
      "text": " And it was going to leave as a result becomes superhuman.",
      "start": 4938.99,
      "end": 4943.44,
      "confidence": 0.59514004
    },
    {
      "text": " I think like Fast 20 fire 2020 years, so I just want to unroll your",
      "start": 4943.44,
      "end": 4951.29,
      "confidence": 0.7818124
    },
    {
      "text": " How you might see the world coming. It's like, we have a couple more years where these other companies are continuing the current approach and it's still out and installs out here. Meaning, they are no more than",
      "start": 4951.29,
      "end": 4962.69,
      "confidence": 0.91156816
    },
    {
      "text": "Low hundreds of billions in revenue. Or how do you think about with styling on me means,",
      "start": 4962.69,
      "end": 4966.69,
      "confidence": 0.8310254
    },
    {
      "text": " I think that I think it could, I think it could still out and",
      "start": 4966.69,
      "end": 4972.29,
      "confidence": 0.96548045
    },
    {
      "text": " I think stolen out will look like",
      "start": 4972.29,
      "end": 4975.84,
      "confidence": 0.9081268
    },
    {
      "text": " it will all look very similar. Yeah. Among all the different companies. Something like this. I'm not sure because I think I think I think even I think even I think even if we stole anyhow, I think these companies could make us stupendous.",
      "start": 4975.84,
      "end": 4988.7,
      "confidence": 0.9447344
    },
    {
      "text": " Stupendous Revenue, maybe not profits. Because they will be, it will be, they will need to work hard to differentiate each other from the cells, but Revenue, definitely",
      "start": 4988.7,
      "end": 4998.07,
      "confidence": 0.90927804
    },
    {
      "text": " but there's something in your model implies that",
      "start": 4998.07,
      "end": 5001.79,
      "confidence": 0.9242276
    },
    {
      "text": " The, when the correct solution doesn't emerge, there will be convergence between all the companies.",
      "start": 5001.79,
      "end": 5007.62,
      "confidence": 0.9043518
    },
    {
      "text": " And I'm curious why you think that's the case? Well, I was talking more about convergence on their largest strategies. Hmm. I think eventually convergence on the technical approach is probably going to happen as well, but I was alluding to convergence to the largest strategies. What what, exactly is the thing that should be done?",
      "start": 5007.62,
      "end": 5023.28,
      "confidence": 0.8916058
    },
    {
      "text": " I just want to be better understand how you see the future in Rolling. So currently we have these different companies and you expect their approach to continue generating Revenue. Yes. But not get to this human life learner. Yes.",
      "start": 5023.28,
      "end": 5033.58,
      "confidence": 0.9041456
    },
    {
      "text": "So now we have these different Forks of companies we have, we have thinking machines, there's a bunch of other labs. Yes, and maybe one of them figures out the correct approach, but then the release of their product makes it clear to other people. How to do this thing. I think it won't be clear how to do its thing, but it will be clear that something different is possible, right? And that is information.",
      "start": 5033.58,
      "end": 5054.34,
      "confidence": 0.96175736
    },
    {
      "text": " And I think people will then be trying to figure out how that's how that works. I do think though that",
      "start": 5054.34,
      "end": 5062.35,
      "confidence": 0.9731922
    },
    {
      "text": " one of the things that,",
      "start": 5062.35,
      "end": 5064.24,
      "confidence": 0.89937603
    },
    {
      "text": " That I think, you know, not address here not discussed, is that?",
      "start": 5064.24,
      "end": 5068.75,
      "confidence": 0.9377888
    },
    {
      "text": " with each increase in the AI capabilities, I think they will be some kind of",
      "start": 5068.75,
      "end": 5075.57,
      "confidence": 0.9299372
    },
    {
      "text": " Changes. But I don't know exactly which ones.",
      "start": 5075.57,
      "end": 5078.65,
      "confidence": 0.87622356
    },
    {
      "text": " In how things have been done. So like,",
      "start": 5078.65,
      "end": 5082.45,
      "confidence": 0.9188632
    },
    {
      "text": " I think it's going to be important yet. I can't spell out what that is exactly and how are the",
      "start": 5082.45,
      "end": 5089.03,
      "confidence": 0.96256006
    },
    {
      "text": " by default you would expect the company that has the model company that has that model to be getting all these games because they have the model that is learning how to do all.",
      "start": 5089.03,
      "end": 5098.34,
      "confidence": 0.94896835
    },
    {
      "text": " Has the skills and knowledge that it's building up in the world.",
      "start": 5098.34,
      "end": 5102.02,
      "confidence": 0.9002064
    },
    {
      "text": " What is a reason to think that the benefits of that would be widely distributed and not just end up at",
      "start": 5102.02,
      "end": 5106.92,
      "confidence": 0.9113294
    },
    {
      "text": "Whatever model company gets this continuous. Learning Loop going first.",
      "start": 5106.92,
      "end": 5110.87,
      "confidence": 0.9341176
    },
    {
      "text": " Like I think that empirically what happened. So here here is what I think is going to happen. Now one I think empirically when",
      "start": 5110.87,
      "end": 5120.69,
      "confidence": 0.971075
    },
    {
      "text": " Let's let's look at let's look at how things have gone so far with them, the areas of the past. So one company produced an advanced",
      "start": 5120.69,
      "end": 5131.07,
      "confidence": 0.91647476
    },
    {
      "text": " And that the company scrambled.",
      "start": 5131.07,
      "end": 5133.66,
      "confidence": 0.7747153
    },
    {
      "text": " and,",
      "start": 5133.66,
      "end": 5134.97,
      "confidence": 0.93304116
    },
    {
      "text": " Produce some competitive, some some similar things after some out of time.",
      "start": 5134.97,
      "end": 5139.6,
      "confidence": 0.86554337
    },
    {
      "text": " And they started to compete in the market and push their push, the price is down. And so, I think, from the market perspective, I think something similar will happen there as well. Even if someone's okay, we are talking about the good World, by the way, where",
      "start": 5139.6,
      "end": 5155.64,
      "confidence": 0.90223694
    },
    {
      "text": " What's the good world? What's the good world?",
      "start": 5155.64,
      "end": 5159.61,
      "confidence": 0.9163617
    },
    {
      "text": " Where we have these.",
      "start": 5159.61,
      "end": 5162.62,
      "confidence": 0.92761505
    },
    {
      "text": " Powerful human like, learners.",
      "start": 5162.62,
      "end": 5165.58,
      "confidence": 0.8569055
    },
    {
      "text": " it also like and by the way, maybe there's another thing we haven't discussed on the",
      "start": 5165.58,
      "end": 5171.27,
      "confidence": 0.9270753
    },
    {
      "text": " On the spec of the super intelligent AI.",
      "start": 5171.27,
      "end": 5174.53,
      "confidence": 0.8299586
    },
    {
      "text": " That I think is worth considering.",
      "start": 5174.53,
      "end": 5176.93,
      "confidence": 0.93735147
    },
    {
      "text": "He said you make it narrow.",
      "start": 5176.93,
      "end": 5178.97,
      "confidence": 0.9246242
    },
    {
      "text": " Can be useful in narrow the same time so you can have lots of narrow super intelligent AIS, but suppose you have many of them.",
      "start": 5178.97,
      "end": 5186.55,
      "confidence": 0.93698
    },
    {
      "text": " and you have some and you have some company that's producing a lot of",
      "start": 5186.55,
      "end": 5192.45,
      "confidence": 0.97530895
    },
    {
      "text": " Profits from it. And then you have another company that comes in.",
      "start": 5192.45,
      "end": 5196.72,
      "confidence": 0.99316233
    },
    {
      "text": " And starts to compete, and the competition is going to work is through specialization.",
      "start": 5196.72,
      "end": 5201.7,
      "confidence": 0.9570469
    },
    {
      "text": " I think what's going to happen is that",
      "start": 5201.7,
      "end": 5204.46,
      "confidence": 0.8782003
    },
    {
      "text": " The way.",
      "start": 5204.46,
      "end": 5206.29,
      "confidence": 0.96432275
    },
    {
      "text": " Competition, like competition loves specialization and you see it in the market, you see it in evolution as well. So you want to have lots of different nations and you're going to have lots of different companies who are occupying different niches in.",
      "start": 5206.29,
      "end": 5220.37,
      "confidence": 0.9192433
    },
    {
      "text": " In this kind of world, we must say yeah like one company is really quite a bit better at some area of, really complicated economic activity and a different companies better than other area and the third companies really good litigation. That's what human like learning implies. Is that like a can learn it can but but you have a accumulated learning, you have a big investment.",
      "start": 5220.37,
      "end": 5244.9,
      "confidence": 0.8673954
    },
    {
      "text": " you spend a lot of compute to become really, really good, really phenomenal of this thing and someone else spent",
      "start": 5244.9,
      "end": 5252.3,
      "confidence": 0.93276006
    },
    {
      "text": "A huge amount of compute and a huge amount of experience to get really, really good at some other thing, right? You apply a lot of human learning to get there but now, like you are, you are at this",
      "start": 5252.3,
      "end": 5260.81,
      "confidence": 0.93284446
    },
    {
      "text": " High point where someone else would say, look like, I don't want to start learning what you've learned that was that would require many different companies to begin at the human, like continual learning agent at the same time.",
      "start": 5260.81,
      "end": 5272.59,
      "confidence": 0.9227011
    },
    {
      "text": " So that they can start their different research in different branches. But if one company,",
      "start": 5272.59,
      "end": 5279.63,
      "confidence": 0.9842341
    },
    {
      "text": " You know, gets that Asian first or gets that learner first?",
      "start": 5279.63,
      "end": 5284.19,
      "confidence": 0.94261533
    },
    {
      "text": " It does then seem like, well, you know, they like we just think about every single job in the economy.",
      "start": 5284.19,
      "end": 5291.25,
      "confidence": 0.92780113
    },
    {
      "text": " You just have instance, learning each one. Seems tractable for yeah. That's the that's that's a valid argument. My my strong intuition is that it's not going to go.",
      "start": 5291.25,
      "end": 5303.38,
      "confidence": 0.8055097
    },
    {
      "text": " My strong intuition, is that? Yeah. Like the argument says, it will go this way. Yeah, but my strong intuition is that it will not go this way.",
      "start": 5303.38,
      "end": 5310.93,
      "confidence": 0.86787695
    },
    {
      "text": " That this is the",
      "start": 5310.93,
      "end": 5313.22,
      "confidence": 0.9004461
    },
    {
      "text": " You know, in in theory, there is no difference between theory in practice, and perhaps the reasons. And that's gonna be one of those",
      "start": 5313.22,
      "end": 5319.17,
      "confidence": 0.7958322
    },
    {
      "text": " A lot of people's models of recursive self-improvement, literally explicitly State. We will have a million Ilias in a server, they're coming with different ideas and this will lead to a super intelligence emerging, very fast. You have some intuition about how parallelizable the thing you are doing. Is how how, what are the gains from making copies of Ilya? I don't know.",
      "start": 5319.17,
      "end": 5341.88,
      "confidence": 0.9151791
    },
    {
      "text": "I think.",
      "start": 5341.88,
      "end": 5343.37,
      "confidence": 0.95478344
    },
    {
      "text": " I think they'll definitely be there will be diminishing returns because you want you want people who think differently rather than the same.",
      "start": 5343.37,
      "end": 5351.74,
      "confidence": 0.9366748
    },
    {
      "text": " I think that if they were literal copies of me, I'm not sure how much more incremental value you'd get.",
      "start": 5351.74,
      "end": 5357.08,
      "confidence": 0.9734458
    },
    {
      "text": " I think that.",
      "start": 5357.08,
      "end": 5359.09,
      "confidence": 0.97539884
    },
    {
      "text": " But people who think differently, that's what you want. Why is it that it's been? If you look at different models even released by totally different companies trained on potentially non-overlapping data sets, it's actually crazy how similar alums are to each other maybe the data sets are not as none overlap and it's",
      "start": 5359.09,
      "end": 5378.75,
      "confidence": 0.89992946
    },
    {
      "text": " but,",
      "start": 5378.75,
      "end": 5380.34,
      "confidence": 0.7181679
    },
    {
      "text": " There's there's some sensors like even if an individual human might be less productive than the future AI. Maybe there's something to the fact that human teams that were diversity than teams of the eyes might have. But how do we listen meaningful, diversity among AI. So I think just raising that temperature just results in different gibberish. I think you want something more like",
      "start": 5380.34,
      "end": 5398.03,
      "confidence": 0.9227227
    },
    {
      "text": " different scientists have different different prejudices or different ideas. How do you get that kind of diversity among AI agents? So the reason there has been no diversity",
      "start": 5398.03,
      "end": 5407.17,
      "confidence": 0.9020804
    },
    {
      "text": " I believe it's because of pre-training.",
      "start": 5407.17,
      "end": 5409.7,
      "confidence": 0.7944932
    },
    {
      "text": " All the pre-trained models are the same.",
      "start": 5409.7,
      "end": 5412.74,
      "confidence": 0.90369385
    },
    {
      "text": "Pretty much.",
      "start": 5412.74,
      "end": 5414.15,
      "confidence": 0.9853865
    },
    {
      "text": " Because the tree trend on the same data. Now our real and post-training is where some differentiation starts to emerge because different people",
      "start": 5414.15,
      "end": 5422.42,
      "confidence": 0.9049731
    },
    {
      "text": " Come up with different RL training. Yeah.",
      "start": 5422.42,
      "end": 5425.48,
      "confidence": 0.9428995
    },
    {
      "text": " And then I've heard you here in the past about self-play as a way to either get data or match agents to other agents equivalent intelligence to kick off learning.",
      "start": 5425.48,
      "end": 5437.82,
      "confidence": 0.92881334
    },
    {
      "text": " How should we think about why there's no public?",
      "start": 5437.82,
      "end": 5443.14,
      "confidence": 0.98884475
    },
    {
      "text": " Proposals of this kind of thinking working with a lot of two things to say.",
      "start": 5443.14,
      "end": 5449.09,
      "confidence": 0.84052783
    },
    {
      "text": " I would say that the reason why I thought self-play closed, interesting.",
      "start": 5449.09,
      "end": 5453.13,
      "confidence": 0.8741741
    },
    {
      "text": " Is because it offered the way to create models using compute only without data.",
      "start": 5453.13,
      "end": 5459.84,
      "confidence": 0.93303764
    },
    {
      "text": " Right? And if you think the data is the ultimate bottle neck,",
      "start": 5459.84,
      "end": 5463.14,
      "confidence": 0.797865
    },
    {
      "text": " then using compute only is very interesting.",
      "start": 5463.14,
      "end": 5465.97,
      "confidence": 0.8512373
    },
    {
      "text": " So that's what makes it interesting. Now, the",
      "start": 5465.97,
      "end": 5469.73,
      "confidence": 0.94641954
    },
    {
      "text": " this thing is,",
      "start": 5469.73,
      "end": 5472.12,
      "confidence": 0.6659148
    },
    {
      "text": " The self-play, at least the way it was done in the past when you have agents which are somehow completely with each other. It's only good for the certain set of skills. It is too narrow.",
      "start": 5472.12,
      "end": 5484.93,
      "confidence": 0.8774399
    },
    {
      "text": "It's only good for like negotiation conflict.",
      "start": 5484.93,
      "end": 5490.16,
      "confidence": 0.9894803
    },
    {
      "text": " Certain social skills.",
      "start": 5490.16,
      "end": 5492.45,
      "confidence": 0.97181225
    },
    {
      "text": " Strategizing that kind of stuff. And so if you care about those skills, then self-play will be useful.",
      "start": 5492.45,
      "end": 5498.49,
      "confidence": 0.93091214
    },
    {
      "text": " Now actually, I think that self-play did",
      "start": 5498.49,
      "end": 5502.78,
      "confidence": 0.9181746
    },
    {
      "text": " Find a home but just in a different form.",
      "start": 5502.78,
      "end": 5506.57,
      "confidence": 0.9606085
    },
    {
      "text": " In a different form. So things like debate,",
      "start": 5506.57,
      "end": 5510.17,
      "confidence": 0.95221883
    },
    {
      "text": " Proved a verifier. You have some kind of an llm as a judge which is also incentivized to find mistakes in your work. You could say, this is not exactly self-play but this is, you know, a related adversarial setup that people are doing and believed and really self plays. An example of is a special case of more General like",
      "start": 5510.17,
      "end": 5529.15,
      "confidence": 0.8913761
    },
    {
      "text": " Um, competition between between ages.",
      "start": 5529.15,
      "end": 5532.49,
      "confidence": 0.7657419
    },
    {
      "text": " Write the response, the natural response to competition, is to try to be different and so if you were to put multiple agents and you tell them, you know, you all need to work on some problems and you are an agent.",
      "start": 5532.49,
      "end": 5543.6,
      "confidence": 0.93232524
    },
    {
      "text": " And you're inspecting. What else is working? You're going to say, well, if they're already taking this approach,",
      "start": 5543.6,
      "end": 5549.8,
      "confidence": 0.8037668
    },
    {
      "text": " it's not clear. I should pursue it. Should pursue something differentiate.",
      "start": 5549.8,
      "end": 5553.29,
      "confidence": 0.8846562
    },
    {
      "text": " And so I think that something like this could also create an incentive for a diversity of approaches. No, final question.",
      "start": 5553.29,
      "end": 5561.68,
      "confidence": 0.8507885
    },
    {
      "text": "What is research taste? You're obviously.",
      "start": 5561.68,
      "end": 5565.69,
      "confidence": 0.9242552
    },
    {
      "text": " the person in the world who is",
      "start": 5565.69,
      "end": 5569.81,
      "confidence": 0.9918727
    },
    {
      "text": " Considered to have the best taste in doing research. In AI, you were",
      "start": 5569.81,
      "end": 5575.66,
      "confidence": 0.9519345
    },
    {
      "text": " The co-author on many of the biggest, the biggest things that have happened in the history of deep learning from Alex. And at the gpt3 to sew on what is it that how do you characterize? How",
      "start": 5575.66,
      "end": 5588.5,
      "confidence": 0.92440826
    },
    {
      "text": " You come up with these ideas, I can so I can comment on this for myself. I think different people do it differently.",
      "start": 5588.5,
      "end": 5596.58,
      "confidence": 0.9649111
    },
    {
      "text": " but one thing that guides me personally,",
      "start": 5596.58,
      "end": 5602.33,
      "confidence": 0.9371424
    },
    {
      "text": " Is.",
      "start": 5602.33,
      "end": 5603.87,
      "confidence": 0.860614
    },
    {
      "text": " An aesthetic.",
      "start": 5603.87,
      "end": 5605.86,
      "confidence": 0.8437921
    },
    {
      "text": " Of how AI should be.",
      "start": 5605.86,
      "end": 5608.37,
      "confidence": 0.9708296
    },
    {
      "text": " By thinking about how people are but thinking correctly. Hmm. Like it's very easy to think about how people are incorrectly but what does it mean to think about people correctly? So I'll give you some examples.",
      "start": 5608.37,
      "end": 5621.15,
      "confidence": 0.94800514
    },
    {
      "text": " the idea of the artificial, neuron",
      "start": 5621.15,
      "end": 5624.99,
      "confidence": 0.9117281
    },
    {
      "text": " Is directly inspired by the brain and it's a great idea, why? Because you say sure, the brain has all these different organs as defaults, but the false probably don't matter. Hmm. Why do we think that the neurons matter because there is many of them",
      "start": 5624.99,
      "end": 5637.99,
      "confidence": 0.9256791
    },
    {
      "text": "It's kind of feels right. So you want the neuron? Yeah. You want some kind of local learning rule that will change the connections, you want some local learning rule that will change the connections between the neurons.",
      "start": 5637.99,
      "end": 5648.19,
      "confidence": 0.94128114
    },
    {
      "text": " Right? It feels plausible to the brain, does it? The idea of the distributed representation?",
      "start": 5648.19,
      "end": 5654.19,
      "confidence": 0.96213406
    },
    {
      "text": " The idea that the brain.",
      "start": 5654.19,
      "end": 5657.36,
      "confidence": 0.9892012
    },
    {
      "text": " You know the brain responds to experience a neural. It should learn from experience, not response the brain learns from experience.",
      "start": 5657.36,
      "end": 5663.43,
      "confidence": 0.8551903
    },
    {
      "text": " The neural of experience.",
      "start": 5663.43,
      "end": 5666.37,
      "confidence": 0.6602617
    },
    {
      "text": " And you kind of ask yourself is something fundamental or not fundamental house. Things should be. Yeah. And I think that's been guiding me a fair bit. Kind of thinking from multiple angles and looking for almost Beauty Simplicity ugliness. There's no room for ugliness. It's just Beauty Simplicity Elegance correct. Inspiration from the brain and all of those things need to be present at the same time and the more they are present, the more confident you can be in a top-down belief.",
      "start": 5666.37,
      "end": 5695.19,
      "confidence": 0.90244013
    },
    {
      "text": " And then the top down belief is the thing that sustains you when the experiments contradict you",
      "start": 5695.19,
      "end": 5701.56,
      "confidence": 0.860252
    },
    {
      "text": " because if you just trust the data all the time,",
      "start": 5701.56,
      "end": 5704.24,
      "confidence": 0.95910394
    },
    {
      "text": " well, sometimes you can be doing a correct thing, but there's a bug.",
      "start": 5704.24,
      "end": 5707.26,
      "confidence": 0.93094337
    },
    {
      "text": " But, you don't know the reasons about how can you tell that there is a bug?",
      "start": 5707.26,
      "end": 5710.28,
      "confidence": 0.8523397
    },
    {
      "text": "How do you know if you should keep the bugging or you conclude it's the wrong direction? Well, is the top down? Well, how should you can say, the things have to be this way? Something like this has to work, therefore, we gotta keep going, that's the top down and it's based on this, like, multifaceted Beauty and inspiration by the brain.",
      "start": 5710.28,
      "end": 5728.98,
      "confidence": 0.9504336
    },
    {
      "text": " All right.",
      "start": 5728.98,
      "end": 5730.46,
      "confidence": 0.54889464
    },
    {
      "text": " We'll leave it there. Thank you so much. Thank you so much. Oh,",
      "start": 5730.46,
      "end": 5734.07,
      "confidence": 0.8980338
    },
    {
      "text": " all right. Appreciate it was great. Yeah, I enjoyed it. Yes, me too.",
      "start": 5734.07,
      "end": 5738.88,
      "confidence": 0.6933442
    },
    {
      "text": " Everybody. I hope you enjoyed that episode. If you did at the most helpful thing you can do is just share it with other people who think might enjoy. It's also helpful if you leave a rating or a comment on whatever platform you're listening on,",
      "start": 5738.88,
      "end": 5752.46,
      "confidence": 0.94336694
    },
    {
      "text": " if you're interested in sponsoring the podcast, you're gonna reach out at War, cash.com slash advertise. Otherwise I'll see you in the next one.",
      "start": 5752.46,
      "end": 5762.06,
      "confidence": 0.82014036
    }
  ],
  "full_transcript": "You know, it's crazy that all of this is real. Yeah. Don't you think so like all this AI stuff and all this Bay Area? Yeah that it's happened. Like  Isn't it straight out of Science Fiction? Yeah, another thing that's crazy is like how normal this low tape off feels  The idea that would be investing one person of GDP in AI like if you like felt like a bigger deal, you know we're right now just feels like we get used to things pretty fast turns out. Yeah.  But it's kind of like it's abstract like, what does it mean?  What it means that you see in the news. Yeah, that Sergeant such company announced such and such dollar amount. Right?  That's, that's all you see, right?  It's not really felt in any other way so far. Yeah. Should we actually begin here? I think this is interesting discussion. Sure.  I think your point about well from the average person's point of view.  Nothing. Is that different will continue being true even into the singularity? No, I don't think so. Okay, interesting. So the thing, which I was referring to not feeling different.  Is okay. So such and such company announced some difficult to comprehend dollar amount of investment. Right? I don't think anyone knows what to do with that. Yeah, but I think that the impact over AI is going to be felt The eye is going to be diffused through the economy. The very strong economic forces for this.  And I think the impact is going to be felt very strongly.  When do you expect that impact? I think the models seem smarter than their economic impact would imply,  yeah, this is  one of the very confusing things about the models right now.  How to reconcile?  The fact.  That they are doing so well on evals. And you look at the evils and you go, those are pretty hard evals, right? They're doing so well.  but the economic impact seems to be dramatically behind and it's almost like  it's it's very difficult to make sense of how can the model on the one hand? Do these amazing things? And I know the other hand like repeated self twice, in some situation, in a kind of an example would be, let's say you use vybe coding to do something.  And you go to some place and then you get a bug.  And then you tell them the model. Can you please fix the bug? Yeah and the model says oh my God. You're so right. I have a bug. Let me go fix that and it uses the second bug. Yeah. And then you tell, if you have a, you have this new second bug, right? And it tells you oh my God, how could have done it. You're so right again.  And brings back the first bug. Yeah. And you can alternate between those. Yeah. And it's like, how is that possible? Yeah, it's like  I'm not sure, but it does suggest that the  something strange is going on.  I have to possible explanations. So here, this is the more kind of  Whimsical explanation is it? Maybe a real training makes the models a little bit too single-minded and narrowly focused a little bit too.  I don't know, I don't know where  even though it also makes them aware in some other ways.  And because of this, they can't do basic things. But there is another explanation which is  back when people were doing pre-training,  the question of what data to train on was answered.  Because they that answer was everything. Yeah.  When you do pre-training, you need all the data.  So, you don't have to think it's going to be this date or that data. Yeah. But when people do are real training, they do need to think they say, okay? We want to have this kind of our old training for this thing, and that kind of a real training for that thing. And from what I hear, All the companies have teams that just produce new URL environments and just started to the training. Mix.  And the question is, what are those? There are so many degrees of freedom? There is such a huge variety of irrelevance you could produce.  and one of the  one thing you could do and I think that's something that is done inadvertently.  Is that people take inspiration from The evals you say, hey, I would love our model to do really well when we release it on the evos look great.  What would be RL training that could help on this task, right?  I think that is something that happens and I think it could explain a lot of what's going on.  if you combine this with generalization of the models, actually being inadequate,  That has the potential to explain a lot of what we are seeing this disconnect between evil performance.  and actual real world performance, which is something that we don't  Today exactly even understand what what we mean by that. I like this idea that the real reward hacking is a human researchers who are too focused on the evolves. I think there's two ways to understand.  or to try to think about what you have, just  Pointed out. One is look if it's the case that simply by becoming superhuman at a coding competition. A model will not automatically become more tasteful.  And exercise better judgment about how to improve your code base. Well, then you should expand the suite of environments such that you're not just testing it on having the best performance in a coding competition. It should also be able to make the best kind of application for X thing. Or why thing or is anything in another, maybe this is what you're thinking at is to say, why should it be the case in the first place that  Becoming superhuman at coding competitions, doesn't make you a more tasteful, programmer, more generally. Maybe the thing to do is not to  Keep stacking up the amount of environments and the diversity of environments to figure out a poach, with let you learn from one and improve your performance on something else.  So I have an analog a human analogy, which might be helpful. So even the case, let's take the case of competitive programming. Since you mentioned that and suppose you have two students, One of them work decided they want to be the best competitive programmers. So they will practice.  10,000 hours for that domain.  They will solve all the problems, memorize all the proof techniques and be very very you know.  Be very skilled at quickly and correctly, implementing, all the algorithms and but notice by doing. So, they became the best one of the best.  Student number two thought. Oh, competitive programming school. Maybe the practice for a 100 hours much, much less. And they also did really well, which one do you think is going to do better in their career later on the second? Right and I think that's basically what's going on. The models are much more likely the first student but even more because then we say okay,  So, the model should be good with competitive programming, so let's get every single competitive. Programming problem ever.  And then, let's do some data augmentation. So we have even more competitive, programming problems. Yes. And we train on that. And so now I got this great competitive programmer and with this analogy, I think it's more intuitive.  I think it's more intuitive with this analogy that? Yeah. Okay. So if it's so well trained. Okay. It's like all the different algorithms and all the different proof techniques like, right? It's at its fingertips. And it's more intuitive that with this level of preparation. It not necessarily generalize to other things.  Then what is the analogy for what the second student is doing, before they do the 100 hours of fine-tuning. I think it's like,  They have it. I think it's the eat Factor. Yeah. Right. And like I know like, when I was an undergrad, I remember there was, there was a student like this that studied with me. So, I know it exists. I think it's interesting to distinguish it from whatever pre-training does. So when we would understand what you just said about, we don't have to choose the data and pre-training is to say,  Actually, it's not dissimilar to the 10,000 hours of practice. It just that you get that 10,000 hours of practice for free because it's already somewhere in the pre-training distribution. But it's like maybe you're suggesting actually, there's actually not that much generalization for free training, they're just so much data and free training but it's like it's not really generalizing better than RL like the main strength of pre-training is that there is a so much agree. Yeah.  And B.  You don't have to think hard about what data to put into free training. And it's a very kind of natural data and it does include in it, a lot of what people do. Yeah.  People's thoughts. And a lot of the features that you know, it's like the whole world as projected by people onto text. Yeah.  And pre-training tries to capture that using a huge amount of data.  It's  It's very the pre-training is very difficult to reason about because it's so hard to understand the manner in which the model, relies on pre-training data.  And whenever the model makes a mistake, could it be? Because something by chance is not as supported by the pre-training data, you know, and preach support by pre-training is maybe a loose term  I don't know if I can have anything more useful on this, but  I don't think there is a human animal to preaching. Hmm.  Here's analogies that people have proposed for what the human analogy to returning his and I'm curious to get your thoughts on why their potential wrong.  One is to think about the first 18 or 15 or 13 years of a person's life when they aren't necessarily economically productive. But they are doing something that is making them understand the world better and so forth. And the other is to think about Evolution as doing some kind of search for three billion years, which then results in A human lifetime instance.  And then I'm curious if you think either of these are actually analogous appreciating or how would you think about it? At least what lifetime human learning is like if not pretending?  I think there are some similarities between both of these two pre-training and portraying to play the role of both of these. But I think there are some big differences as well.  the amount of pre-training data is,  Very, very staggering, yes.  And somehow a human being after even 15 years with the tiny fraction of the free training data, they know much less, but whatever. They do know, they know much more deeply somehow and the mistakes like  Like already at that age, you would not make mistakes that Arias make. Yeah. There is another thing you might say, could it be something like Evolution and then answer is maybe. But in this case, I think Evolution might actually have an edge like there is this. I remember  Reading about this case, where some, you know, that one thing that neuroscientists do.  Or rather one way in which neuroscientists can learn about. The brain is by studying people with brain damage to different parts of the brain. And so, and some people have the most strange symptoms you could imagine. It's actually really, really interesting.  And there was one case that comes to mind, it's relevant.  I read about this person.  Who had some kind of brain damage that took out, I think a stroke or an accident that took out his emotional?  Processing. So he stopped feeling any emotion.  And as a result of that, you know, he still remained very articulate and he could solve little puzzles and on test, he seemed to be just fine.  But he thought no emotion, he didn't feel sad, he didn't feel angry, he didn't feel animated and he became somehow extremely bad. At making any decisions at all.  It would take him hours to decide on which socks to wear, and he would make very bad financial decisions.  and that's very  Does, what does it say about?  The role of our building emotions.  In making us like a viable agent, essentially, and I guess to connect to your question about pre-training. it's like,  Maybe preach, like maybe if you are good enough, that like getting everything out of pre-training, you couldn't get, you could get that as well. But that's the kind of thing which seems  Well, it may or may not be possible to get that from pre-train.  What is?  That clearly not just directly emotion. It seems like some  Almost value function like thing, which is giving telling you which decision to be like what the end reward for any decision should be.  And you think that doesn't sort of implicitly come from?  I think it's cool. I'm just saying, it's not, it's not 100% obvious. Yeah.  But what is that? Like, what how do you think about emotions in? What is the ml analogy for emotions?  It should be some kind of a value function thing. Yeah. But I don't think there is a great amount of energy because right now, value functions. Don't play very prominent control and the things people do. Maybe we're worth defining for the audience, what a valley function as if you want to do that. I mean certainly, I will be very happy to do that, right? So,  So, when people do reinforcement learning the very enforcement learning is done right now. How does it, how do people train those agents?  So, you have a neural net.  And give it a problem.  and then you tell the model, go solve it and the model takes maybe thousands hundreds of thousands of actions  Or thoughts or something and then it produces a solution. The solution is created. And then the score  Is used to provide a training signal for every single action.  In your trajectory.  so, that means that if you are doing  Something that goes for a long time. If you are training, a task that takes a long time to solve, you will do no learning at all until you solve the until you come up with the proposal of Solution. That's how reinforcement learning is done natively. That's how, oh, one are. One ostensibly are done.  The value function, says something like. Okay look.  maybe I could sometimes not always  Could tell you if you're doing well or badly, the notion of a value function, is more useful in some domains than others. So, for example, when you play chess,  And you lose a piece, you know. I messed up.  You don't need to play the whole game to know that what I just did was bad and therefore, whatever, whatever proceeded it was also bad. So, the value function. Lets you  Short-circuit.  The weight until the very end. Like let's suppose that you started to pursue some kind of okay, let's suppose that you are doing some kind of a math thing, or a programming thing.  And you're trying to explore a particular solution Direction.  and after let's say after 1000 steps of thinking, you concluded that this direction is unpromising,  As soon as you conclude this, you could already get a reward signal.  1000 times steps previously. When you decided to pursue down the path, you say, oh next time I shouldn't pursue this path in a seamless situation. Long before you actually came up with the proposed solution. This is in the Deep cigar one paper, is that  The.  Space of trajectories is so wide that maybe it's hard to learn a mapping from an intermediate trajectory.  And value and also give that, you know. Encoding for example you will have the wrong idea, then it will go back. Then you'll change something this sounds like such lack of face and deep learning like I mean sure it's might be difficult, but Nothing deep learning can do. Yeah. So  my expectation, is that  like value function should be useful and  then I fully I fully expect that they will be using the future if not already. What was I alluding to with the person whose emotional Center got and damaged is more that  maybe what it suggests is that the value function of humans is modulated by emotions in some important way that's hardcoded by evolution.  and maybe that is important for people to  be affected in the world. That's the thing I was actually gonna ask you. There's nothing really interesting about emotions of Valley function, which is that  it's impressive that they have this much utility while still being rather.  Simple to understand.  So after responses, I do agree that compared to  The kind of things that we learn in the things that we are talking about, the kind of, as we are talking about. Emotions are relatively simple. It might even be so simple that maybe you could map them out in a human understandable way. I think it would be cool to do.  in terms of utility though, I think there is a thing where  You know, there is this complexity robust and straight off.  Where complex things can be very useful.  But simple things.  Are very useful in very broad range of situations.  And so I think what one way to interpret, what we are seeing is that we've got these emotions that essentially evolved the mostly mostly from our mammal ancestors.  And then fine-tuned a little bit while we were hominins just a bit. We do have like a decent amount of social emotions though.  Which mammals May lack.  But they're not very sophisticated.  And because they're not sophisticated the service so. Well in the very different world compared to the one that we've been living in actually they are they also make mistakes for example our emotions well I don't know the hunger count is an emotion.  Bits the beta. But I think, for example, our intuitive feeling of hunger, Is not succeeding. In guiding us correctly in this world with an abundance of food. Yeah, people haven't talked about scaling data,  Scaling parameter or scaling compute?  Is there a more General way to think about scaling? What are the other scaling axes?  so,  The thing. So here is a perspective, here's a perspective. I think might be might be true.  so,  The way a male used to work is that people would just think of it with stuff and try to.  And try to get interesting results, that's what's been going on in the past.  Then.  The scale in Insight arrived, right? Scaling laws.  Gpt-3.  And suddenly everyone realized we should scale.  And it's just this, this is an example of how language affects thought.  Scaling is what, just one word, but it's such a powerful word because it informs people what to do this. Okay, let's let's try to scale things and so you say, okay, so what are we scaling and pre-training was a thing to scale, it was a particular scale in recipe. Yes. The big breaks through of pre-training. Is the realization that this recipe is good.  So you say, hey, if you  Mix some compute.  With some data into a neural net of a certain size.  You will get results and you will know that will be better if you just scale the recipe up. And this is also great companies love this because it gives you a very  Low risk way of investing your resources, right? It's much harder to invest your resources in research.  compare that, you know, if you research you need to have like go for researchers and research and come up with something versus  Get more data, get more compute, you know, it will get something from pre-training.  and indeed, you know, it looks like I based on various  Things. People say onto some people say on Twitter, maybe it appears the Gemini have found a way to get more out of retraining.  At some point, the pre-training will run out of data. The data is very clearly finite and so then, okay, what do you do next? Are you do some kind of souped up, retraining different recipe from the one that we've done before? Or are you doing our Rel, or maybe something else. But now that compute is Big, computer's not very big in some sense. We are back to the age of research. So maybe here's another way to put it up until 2020 from 25 from 2020 to 12 to 2020, it was the age of research.  Now, from 2020 to 2025, it was the age of scaling or maybe plus minus, let's add Airbus to these years. Because people say this is amazing. You got a scale more, keep scaling, the one word scaling.  But now the scale is so big like is it is the belief really that. Oh, it's so big but if you had 100x, more everything would be so different like it would be different for sure. But like is the belief that if you just hunted X the scale,  Everything would be transformed.  I don't think that's true.  So it's back to the age of research. Again just to be computers. That's very interesting to put a  But let me ask you the question you just posted and what are we scaling? And what is, what would it mean to have a recipe? Because  I guess I'm not aware of a very clean relationship that almost looks like a lot of physics which existed in pre-training, was a power law between data or computer parameters and loss. What is the kind of relationship? We should be seeking? And how should we think about what this new recipe might look like so we've already witnessed a transition from  one type of scale into a different type of scaling from pre-training to RL.  Now, people are scaling URL. Now, based on what people say, on Twitter, they spend more computer on our relative on pre-training at this point.  Because our Rel can actually consume quite a bit of compute, you know, you do very, very long rollouts. Yes. So it takes a lot of compute to produce the rollouts.  And then you get relatively small amount of learning polar out. So you really can spend, you really can spend a lot of compute.  And I could imagine.  Like I wouldn't at this at this it's more like I wouldn't even call it a scale scaling. I would say hey like what are you doing? And is the thing you are doing the the most productive thing you could be doing? Yeah. Can you find the most more productive way of using your computer? We've discussed the value function business earlier.  And maybe once people get good at value functions, they will be using their resources more productively.  And if you find a whole other way of training models, you could say, is this scaling? Or is it just using your resources? I think it becomes a little bit ambiguous. In a sense that when people were in the age of research back, then, it was like, people say, hey, let's try this in this and this. Let's try that. Oh, look something interesting is happening. And I think there will be a return to that. So if we're back in the era of research, stepping back, what is the part of the recipe that we need to think? Most about, when you say value function, people are already trying the current recipe but then having a limited judge and so forth. You could say that's a value function, but it sounds like you have something much more. Fundamental in mind. Do we need? Do we need to go back to? Should we even rethink?  Free training at all and not just add more steps to the end of that process. Yeah, so the the discussion about value function. I think it was interesting. I want to like emphasize that I think the value function is something like  It's going to make a real more efficient.  And I think that makes a difference.  But I think that anything you can do with the value function, you can do without just more slowly.  The thing, which I think is the most fundamental, is that these models somehow just generalize dramatically worse than people. Yes.  And it's super obvious.  That's, that's seems like a very fundamental thing, okay? So this is the Crocs generalization and there's two. Sub-questions.  There's one, which is about sample efficiency, which is why should it take so much more data for these models to learn than humans. There's a second about  Even separate from the amount of data. It takes there's a question of  why is it so hard to teach the thing? We want to a model then to a human which is to say,  for to a human that we don't necessarily need a verifiable reward to be able to  You're probably mentoring a bunch of researchers right now and you're, you know, talking with them, you're showing them your code and you're showing them how you think and from that they're picking up your way of thinking and how you should do research. You don't have to set like a verifiable reward for them that's like okay, this is the next part of the curriculum. And now this is the next part of your curriculum and oh it was this training was on stable and we got there's not this schleppy bespoke process.  so, perhaps these two issues are actually related in some way, but I'd be curious to  Explore this. This second thing which was more like continue to learning and this first thing which feels just like  Sample efficiency.  yeah, so, you know you could actually Wonder one one possible explanation for  The human sample efficiency that needs to be considered easily evolution. And evolution has given us a small amount of the more the most useful information possible.  And for things like vision.  Hearing and locomotion.  I think this is a pretty strong case that Evolution actually has given us a lot. So for example, human dexterity far exceeds, I mean,  Robots can become dexterous too. If you subject them to like a huge amount of training and simulation,  But to train a robot in the real world to quickly like pick up a new skill. Like a person does seems very out of reach and here you could say oh yeah like Locomotion all our ancestors needed. Great Locomotion squirrels like  So Locomotion, maybe like we've got like some unbelievable prior, you could make the same case for vision, you know, I believe in the point, oh, like children, learn to drive off the 16 hour after 10 hours of practice, which is true.  But our vision is so good.  At least for me when I remember myself being five years old, my I was I was very excited about cars back then. And I'm pretty sure my car recognition was more than adequate for self-driving already as a five-year-old.  You don't get to see that much data as a five-year-old. You spend most of your time in your parents house.  so you have very low data diversity, but you could say maybe that's Evolution to  but in language and math and coding.  Probably not.  It's just seems better than models. I mean obviously models are better than the average human at playing language in math and coding. But are they better at the average human at learning? Oh yeah, oh yeah, absolutely. What I meant to say is that language math and coding and coding suggest that whatever it is that makes people  good at Learning.  Is probably not so much, a complicated prior but something more, some fundamental thing.  Wait, I'm not sure. I understood what should that be? The case.  So consider a skill.  That people exhibit some kind of great reliability or you know. Yeah.  If the scale is one, there was very useful to our ancestors for many millions of years. Hundreds of millions of years. You could say, you could argue that maybe humans are good at it. Because of evolution because we have a prior and evolutionary prior that's encoded in some very non-obvious way. Yeah, that somehow makes us so good at it but if people exhibit, great ability reliability robustness ability to learn in a domain that really did not exist until recently.  Then this is more an indication.  That people might have.  just better machine learning period but then how should we think about what that is is it a matter of  Yeah, what is the ml analogy for why?  There's a couple interesting things about it takes for your samples, it's more unsupervised. You don't have to set a very like, a child learning to drive a car show? No, no, no. I teenager learning how to drive a car is like not exactly. Getting  Some pre-built verifiable reward. Their it comes from their interaction with the machine and the with the environment.  And yeah, it takes much of her samples, that seems more unsupervised, it seems more robust much more robust, the robustness of people. It's really staggering. Yeah, so like, okay and do you have a unified way of thinking about? Why are all these things happening at once? What is the ml knowledge that would, that could be, it could realize something like this. So, so this is where, you know, one of the things that you've been asking about is, how can, you know, the teenage driver kind of self-correct and learn from their experience without an external teacher. And the answer is, well, they have their value function, right? They have a general sense, which is also, by the way, extremely robust in people like  Whatever it is. The human value function, whatever the human value function, is with a few exceptions around addiction. It's actually very, very robust.  And so for something like a teenager, that's learning to drive.  They start to drive.  And they already have a sense of how they drive.  Immediately.  How badly their unconfident, and then they see. Okay. And they, and then, of course, the learning speed of any teenager is so fast, stuff. That 10 hours. You're good to go. Yeah, it seems like humans, have some solution. I'm curious about, like,  Well, how are they doing it? And like why is it so hard to like what how do we need to reconceptualize the way we're training models, to make something like this possible, you know? That is a great question to ask.  And it's a question.  I have a lot of opinions about  But unfortunately.  We live in a world where not not all machine learning ideas are discussed freely and this is this is one of them. So  this probably a way to do it.  I think it can be done. The fact that people  are like that. I think it's a proof that it can be done. There may be another blocker though, which is  There is a possibility.  That the human neurons actually do more compute than we think.  and if that is true, and if that plays an important role and things might be more difficult  but regardless, I do think it points to the existence of some  machine learning principle. The diabetic opinions on. But unfortunately, circumstances make it hard to to discuss in detail even though nobody listens to this podcast. Yeah. Yeah. So I have to say that prepping for Ilya was pretty tough because neither I nor anybody else, had any idea what he's working on and what SSI is trying to do. I had no basis to come up with my questions and the only thing I could go off on a was trying to think from first principles about, what are the bottle next to AGI, because clearly Ilya is working on them in some way part of this question involved thinking about RL scaling because everybody's asking how well our generalized and how we can make a generalized better as part of this. I was reading this paper that came out recently on RL scaling and it showed that actually the learning Corona. RL looks like a sigmoid. I found this very curious. Why should be a sigmoid where? It learns very little for a long time. And then it quickly learns a lot. And then asked him totes. This is very different from the power law. You see in pre-training where the model learns A bunch of the very beginning and then less and less over time. And it actually reminded me of a note that I had written down. After I had a conversation, the researcher friend, where he pointed out that the number of samples that you need to take in order to find a correct answer, scales exponentially with how different your current probability distribution is from the target properly distribution. And I was thinking about how these two ideas are related. I had this vague idea that they should be connected, but I really didn't know how. I don't have a math background, so I couldn't really formalize it. But I wondered if Gemini through could help me out here. And so, it took a picture of my notebook. And I took the paper and I put them both in the context of Gemini 3, and I asked it to find the connection and thought a bunch. And then it realized that the correct way to model the information you gain from a single. Yes, or no outcome in RL is as the entropy of a random binary variable. It made a graph, which showed how the bits you gain first sample in our all versus supervised learning skill. As a pass rating, increase is. And as soon as I saw the Graph, the Gemini 3 made immediately a ton of things started making sense to me. Then I wanted to see if there's any empirical basis to this Theory. So I asked Gemini to code an experiment to show whether the Improvement in Los scales in this way with pass rate. I just took the code that Gemini output. I copy pasted it into a Google collab notebook, and I was able to run this toy ml experiment and visualize its results without a single bug. It's interesting because the results look similar, but not identical to what we should have expected. And so, I downloaded this chart, and I put it into Gemini and asked it, what is going on here and came up with the hypothesis that I think is actually correct? Which is that we're capping, how much supervised learning can improve in the beginning, by having a fixed learning rate. And in fact, we should decrease the learning rate over time, it actually gives us an intuitive understanding, for why in practice, we have learning race schedule orders that decrease the learning rate over time. I did this entire flow from coming up with this vague initial question to building a theoretical understanding to running  Some toy ml experiments all with Gemini 3. This feels like the first model where it can actually come up with new connections that I wouldn't have anticipated. It's actually now become the default place. I go to, when I want to brainstorm new ways to think about a problem. If you want to read more about RL scaling, you can check out the blog posts that I wrote With a Little Help from Gemini 3 and if you want to check out Gemini 3 yourself, go to Gemini dot Google. I'm curious. If you say, we are back in the era of research.  You were there from 2012 to 2020?  And do you? Yeah what is now the vibe going to be if we go back to the area of research for example,  Even after Alex net, the amount of compute that was used to run experiments kept increasing and the size of Frontier systems kept increasing.  And do you think now that this era of research will still require tremendous amounts of compute? Do you think it will require going back into the archives and reading old papers? What is you, maybe what was the vibe of like you were Google and open AI in Sanford? And these places when there was like a more of a Vibe of research, what kind of things should we be expecting in the community?  So, one consequence of the age of scaling.  Is that there was this?  Scaling sucked out all the air in the room. Yeah.  and so,  because scaling sucked out all the air in the room. Everyone starts to do the same thing.  We got to the point where  we are in a world where there are more companies than ideas but quite a bit actually on that, you know, there is the Silicon Valley saying that says that ideas are cheap execution is everything.  And people say that a lot. Yeah, and there is truth to that  But then I saw, I saw someone say on Twitter.  something like,  If ideas are so cheap, how come no one's having any ideas.  And I think it's true too. I think.  like, if you think about a research progress in terms of bottlenecks,  There are several bottlenecks.  If you go back to the if and one of them is ideas and one of them is your ability to bring them to life. Yeah. Which might be compute but also engineering.  so if you go back to the 90s, let's say you had people who have had pretty good ideas,  and if they had much larger computers, maybe they could demonstrate that their ideas for viable, but they could not  So they could only have very very small demonstration and not convinced anyone? Yeah. So, the bottleneck was compute.  Then in the age of scaling.  Computers increased a lot.  and of course, there is a question of how much computers needed, but computers large, so,  Computers large enough.  Such that.  It's like not obvious that you need that much more compute to prove.  Some idea.  Like, I'll give you an analogy alexnet was on 2gp.  That was the total amount of computers for it.  The Transformer.  Was built on 8 to 64 gpus. No single Transformer. Paper experiment used more than 64 gpus of 2017 which would be like what two gpus of today?  So, the resnet.  Right.  many, like, even the the You could argue that the black hole one,  Reasoning.  was not the most compute heavy thing in the world, so there definitely  For, for research.  You need definitely some of computer.  But it's far from obvious that you need the absolute largest amount of compute ever for research. You might argue and I think it is true that if you want to build the absolutely best system.  if you want to build the absolutely best system,  then it helps so much more compute and especially if everyone is within the same paradigm,  then compute becomes one of the big differentiators.  Yeah, I guess.  What was possible to develop these ideas?  I'm asking you for the history because you're actually there. I'm not sure what actually happened, but it sounds like it was possible to develop these ideas using minimal amounts of compute. But it wasn't the Transformer didn't immediately become famous. It became the thing everybody started doing and then started experimenting on top of and building on top of  Because it was validated at higher and higher levels in compute. Correct. And if you at SSI have 50 different ideas, how will, you know, which one is the next Transformer? And which one is, you know, brittle without having the kinds of compute that other Frontier lapse have? So I can I can comment on that which is  the short comment is that, you know, you mentioned a society specifically for us The amount of compute that SSI has for research.  Is really not that small and I want to explain why like, a simple math can explain why the amount of compute that we have is actually a lot more comparable for research than one might think.  now, explain  so,  SSI has raised 3 billion dollars, which is like.  Not small by its like, a lot by any absolute sense, but you could say, but look at the other company is Raising much more.  But a lot of what their, a lot of their compute goes for inference.  Like these big numbers. This big loans, it's earmarked for inference, that's number one.  now, number two,  you need, if you want to have a product on which you do inference, you need to have a big staff of Engineers of salespeople. A lot of the research needs to be dedicated for producing all kinds of product related features.  So then, when you look at what's actually left for research, the difference becomes a lot smaller.  Now, the other thing is, is that if you are doing something different, Do you really need the absolute maximum scale?  To prove it. I don't think it's true at all. I think that in our case, we have  Sufficient to prove to convince ourselves and anyone else that what we're doing is correct.  there's been public SMS that, you know, companies like open AI, spend on the order of  Five six billion dollars a year, even just so far on experiments. This is separate from the amount of money they're sending on in France and so forth. So it seems like they're spending more a year or running experience like research experiments, the new guys have in total funding. I think it's a question of what you do with it.  It's a question of what you do with it. Like they have like the more I think in, in their case, in the case of others. I think there's a lot more Demand on the training compute. There's a lot more different work streams.  There is, there are different modalities. There is just more stuff.  And so it becomes fragmented.  How will SSI make money, you know?  my answer to this question is something like  We just focus right now. We just focus on the research and then settle. This question will reveal itself. I think there will be lots of possible. Answers is exercise plans to illustrate shots. Super intelligence.  Maybe.  I think that there is married to it. I think there's a lot of Merit because I think that it's very nice to not be affected by the day-to-day Market competition.  but,  I think there are two reasons that may cause us to change the plan.  One is pragmatic, if timelines turned out to be long,  Which they might.  and second, I think the reason a lot of value,  Inn.  The best.  And most powerful AI being out there impacting the world. Yeah.  I think this is a meaningful thing, but then so why is your default plan too? Straight out super intelligence because it sounds like you know,  Open Ai and the robotic, all these other companies they're explicit thinking is look, we have weaker and weaker intelligence. Is that the public can get used to and prepare for?  And why is it potentially better to build a super intelligence directly? So I'll make the case for and again. Yeah, the case for is that you are so one of the challenges, That people face when they're in the market is that they have to participate in the rat race.  And the rat race is quite difficult in that it exposes you to to difficult trade-offs, which you need to make.  And there is it is it is nice to say, will insulate ourselves from all this and just focus on the research and come out only when we are ready and not before.  But the Counterpoint is valid too.  And those are, those are opposing forces. The Counterpoint is. Hey, it is useful for the world.  To see powerful AI.  It is useful for the world to see powerful AI because that's the only way you can communicate it. Well, I guess not even just that. You can communicate the idea but communicate the AI  Not the idea.  Communicate the AI. What do you mean communicate there? So okay so let's suppose you write an essay about Ai and the essay says AI is going to be this and as going to be that and it's going to be this and you ready can you say? Okay this is an interesting essay right now, suppose you see and AI doing this and they are doing that.  It is in comparable.  like basically, I think, I think that there is a big benefit From AI, being in the public. And  That would be a reason for us to not be quite straight shot. Yeah. Well, I guess it's not even that which I, but I do think that is an important part of it. The other big thing is, I can't think of another discipline and human engineering and research where  The end artifact was made safer. Mostly through just thinking about how to make it safe as a post to why are airplane crashes per mile, so much lower today than they were decades ago. Why is it so much harder to find a bug in Linux than it would have been decades ago? And I think it's mostly because these systems were deployed to the world, you noticed failures, those failures were corrected, and assistance became more robust, and I'm not sure why.  AGI and superhuman intelligence would be any different especially given and I hope we can talk, we're going to get to this.  It seems like the harms of superintelligence are not just about like having some Loveland paperclip out there, but it just like this is a really powerful thing and we don't even know how to conceptualize how people interact with what people will do with that and having gradual access to. It seems like a Um, better way to maybe spread out the impact of it and to help people prepare for it. Well, I think, I think, on this point, even in the straight shot scenario, you would still do a gradual release of it.  Is how I would imagine it.  The gradualism would be and inherent inherent component of any plan. It just a question of what is the first thing that you get out of the door? That's number one, number two. I also think, you know, I believe you have advocated for continued learning more than other people. And I actually think that this is any important and correct thing and here is why  So one of the things so I'll give you another example.  Of how thinking how language affects thinking.  And in this case, it is to be two words, two words that have shaped everyone's thinking I maintain  First Ward, a GI  Second word, pre-training.  Let me explain.  So, the word, the term AGI.  Why does this term exists? It's a very particular term. Why does it exist?  There's a reason.  the reason that the term AGI exists,  Isn't in my opinion, not so much because it's like a very important essential descriptor of some and state of intelligence.  but,  Because it is a reaction.  To a different term that existed in the term is narrow AI. If you go back to ancient history of gameplay, AI of checkers, AI chess AI computer games AI, everyone would say, look at this narrow intelligence. Sure the Chessie I can be Casper of what it can do. Anything else? It is so narrow. Artificial narrow intelligence.  so in response, as a reaction to this,  some people said,  well, this is not good. It is so narrow. What we need is generally  generally, I and AI that can just do all the things.  The second.  And that term just got a lot of traction. Yeah.  the second thing that got a little often,  Is pre-training. Specifically the recipe of pre-training. I think the current the way people do RL now is Maybe  and is undoing the conceptual imprint of pre-training. But pre-training had the property, you do more pre-training and the model gets better at everything more or less. Uniformly, generally I pre-training gives AGI.  but,  the thing that happened with AGI and pre-training, is that in some sense, the overshot the target  Because by the kind, if you think about the term AGI, you will realize and the context of pre-training, it will realize that the human being is not an AGI.  Because a human being yes, there is definitely a foundation of skills.  A human being.  A human being lacks, a huge amount of knowledge. Instead, we rely on continued learning.  We learn continual learning and so then when you think about okay, so let's suppose that we achieve success and we produce a safe super in some kind of safe. Superintelligence, the question is but how do you define it? Where on the curve of continual learning is going to be, I produce like a super intelligent 15 year old. That's very eager to go and you say okay I'm going to they don't know very much at all. The great students. Very eager, you go and be a programmer. You go and be a doctor. Go and learn. So you could imagine that the deployment itself will involve some kind of a learning trial and error period. It's a process as opposed to you drop the Finnish thing. Okay. I see so you're suggesting  that the thing, you're pointing out with super intelligence,  Is not some finished.  Mind which knows how to do every single job in the economy because the way Say the original I think opening HR or whatever defines AGI is like it can do every single job that every single thing a human can do.  Your proposing instead.  A mind which can learn to do any single every single job. Yes, and that is super intelligence and then, but once you have the learning algorithm,  It gets deployed into the world the same way a human labor or my join an organization.  And it seems like one of these two things might happen, maybe neither of these happens. One.  This.  Super efficient learning algorithm.  becomes superhuman becomes as good as you and potentially even better at the task of Ml research. And as a result, the algorithm itself becomes more and more superhuman. The other is even if that doesn't happen.  If you have a single model, I mean, this is explicitly your vision. If you have a single model where instances of a model,  Which are deployed through the economy. Doing different jobs learning. How to do those jobs, continually learning on the job.  Picking up all the skills that any human could pick up. But actually picking up all up at the same time and then the amalgamating the learnings  you basically have a model which functionally becomes super intelligent.  Even without any sort of recursive, self-improvement in software.  Right, because you now have one model that can do every single job in the economy and humans can't merge our minds in the same way.  And so, do you expect some sort of, like, intelligence explosion from broad deployment? I think,  That it is likely that we will have rapid economic growth.  I think the broad deployment.  like,  there are two arguments, you could make  We check on flick team.  One. Is that look if indeed you get once indeed.  You get to a point. Where you have an AI that can learn to do.  Things quickly.  And you have many of them.  Then they will, then they will be a strong.  Force to deploy them in the economy unless they will be some kind of a regulation that stops IT. Which by the way they might be,  but,  I think the idea of very rapid economic growth for some time, I think it's very possible from broad deployment. The question is how rapid it's going to be  So I think this is hard to know because on the one hand you have this very efficient worker. On the other hand, there is the world is just really big.  And is a lot of stuff.  And that's that stuff moves, the different speed but then on the other hand now the AI could, you know? So I think very rapid economic growth is possible and we will see like all kinds of things like  Different countries with different rules and the ones which have the friendly rules. They'll be faster. Hard to predict some people in our audience, like to read the transcripts instead of listening to the episode. And so we put a ton of effort into making the transcripts read like they are Standalone essays. The problem is that if you just transcribe a conversation, verbatim using speech to text model, it'll be full of all kinds of fits and starts and confusing phrasing. We mentioned this problem to label box and they asked if they could take a staff working with them on this is probably the reason that I'm most excited to recommend Label Box to people. It wasn't just oh hey tell us what kind of data you need and we'll go get it. They walked us through the entire process from helping us identify what kind of data we needed in the first place to assembling a team of expert aligners to generate it. Even after we got all the data back, Lee will box stayed involved. They helped us choose the right base model and set up Auto QA on the models output so they could tweak and refine it. And now we have a new transcriber tool that we can use for all our episodes moving forward. This is just one example of how believable box meet.  Their customers at the ideas level and partners with them through their entire Journey. If you want to learn more or if you want to try out the transcriber tool yourself, go to labelbox.com slash. it seems to me that this is a very precarious situation to be in where  Look in the limit, we know that this should be possible because if you have something that is as good as a human at learning but which can merge, its brains merge, there are different instances in a way that humans can't merge.  Already. This seems like a thing that should physically be possible. Humans are possible. Digital computers are possible. You just need both of those combined to produce this thing and it also seems like this kind of thing is  extremely powerful.  and,  Economic growth is one way to put it. I mean Dyson, Sphere is a lot of economic growth, but another way to put it is just like you will have  Potentially a very short period of time because a human on the job. Can, you know, you your hire people to SSI and six months or like net productive? Probably write a human like learns really fast. And so, this thing is becoming smarter and smarter very fast.  What is how do you think about making that go? Well, and why is SSI position to do that? Well, where does SSI plan? There basically is, what internet ask. Yeah.  so,  one of the, one of the ways in which,  My thinking has been changing. Is that?  I now Place more importance on AI.  Being.  Deployed.  Incrementally, and in advance.  One very difficult thing about AI.  Is that we are talking about systems that don't yet exist.  And it's hard to imagine them.  I think that one of the things that's happening,  Is that in practice?  It's very hard to feel AGI.  It's very hard to feel the AGI.  We can talk about it.  But it's like it's like talking about like the Longview, like imagine like having a conversation about like how is it like to be old?  when you like all the in frail and you can have a conversation, you can try to imagine it, but  It's just hard and you come back to reality, well, that's not the case. And I think that a lot of the  Issues around.  AGI and its future power.  Stem from the fact that it's very difficult to imagine. Future AI.  Is going to be different. It's going to be powerful, indeed. The whole problem. What is the problem of an AGI?  The whole problem is the power.  The whole problem is the power.  When the power is really big, what's going to happen?  And one of the, one of the ways in which I've changed my mind over the past year. And so,  That.  that change of mind, may be  May I'll say, I'll hedge a little bit, maybe back propagate into into the plans of our company.  Is that?  so, if it's hard to imagine,  What do you do? You got to be showing the thing?  You gotta be showing the same.  And I maintain that, I think, I think most people who work on AI. Also can't imagine it  Because it's too different from what people see on the day-to-day basis.  I do maintain. Here's something which I predict will happen. That's a prediction.  I maintain.  but as AI becomes more powerful,  Than people will change their behaviors. And we will see all kinds of unprecedented things which are not happening right now.  And I'll give some examples.  I do like I think I think For Better or Worse the different tier companies, we play very important role and what happens as with the government  and the kind of things that I think will see which  You see the beginning of?  Companies that are Fierce competitors starting over to collaborate.  On AI safety. You may have seen, open Ai and Tropic events. Doing a first small step but that did not exist. That's actually something which I predicted in one of my talks about three years ago, that's such a thing will happen. I also maintain it as AI continues to become more  Powerful more visibly powerful. They will also be a desire from governments and the public to do something.  And I think that this is a very important Force.  Of showing the AI. That's number one.  Number two. Okay, so then the AI has been billed. What needs to what needs to be done?  So, one thing that I maintain that will happen. is that right now people who are working on AI,  I maintain that the AI doesn't feel powerful because of its mistakes.  I do think that at some point, the AI will start to feel powerful. Actually,  And I think when that happens, we will see a big change in the way.  All AI companies approach safety.  They'll become much more paranoid.  I think I say this is a predict as a prediction that we will see happen. We'll see if I'm right.  But I think this is something that will happen because they will see the AI becoming more powerful.  Everything that's happening right now. I maintain it's because people look at today's Ai and it's hard to imagine the future AI  And there is a third thing which needs to happen.  And I think this is this this and I'm talking about it in broader terms, not just from the perspective of SSI.  Because you asking me about our company. But the question is okay. So then what should, what should the companies aspire to build? Yeah. What should they start to build?  And there has been one big idea that actually that everyone has been locked in locked into which is the the self improving AI. And why did it happen? Because there is fewer ideas and companies  But I maintain that there is something that's better to build.  And I think that everyone will actually want that.  It's like,  The AI. That's robustly. Aligned to care about sentient life specifically.  I think in particular, it will be the case to be made that it will be easier to build an AI that cares about sentient life.  Than any other cares about human life alone.  Because the AI itself will be sentient.  and if you think about things like mirror neurons and human empathy for animals, which is, you know, you might argue, it's not big enough but it exists  I think it's an emergent property from the fact that the model others, with the same circuit that we use to model ourselves because that's the most efficient thing to do.  So,  even if you got an AI to hear about sentient being,  And it's not actually clear to me that that's what you should try to do if you solve the lineman.  Would be the case. That most Indian beings will be AI, is there will be trillions eventually quadrillions of ai's? Humans will be a very small fraction of sentient beings. so,  It's not clear to me if the goal is some kind of human control over.  This future civilization.  That this is the best Criterion. It's true. I I think that  it's possible. It's not the best Criterion. I'll say, two things.  I think that the number one,  I think that if they're so,  I think that care for sentient life. I think there is married to it. I think it should be considered.  I think that it will be helpful if there was some kind of  A shortlist over ideas.  that then,  The companies when they are in the situation, could you use? That's number two, number three. I think it would be really materially helpful. If  The.  Power of the most powerful super intelligence for somehow capped.  Because it would address a lot of these concerns.  The question of how to do it.  I'm not sure but I think that would be materially helpful.  When you're talking about really, really powerful systems. Yeah, um, before we continue the 11 discussion, I want to double click on that. How much room is there at the top? How do you think about super intelligence? Do you think? I mean using this learning efficiency idea, maybe it's just extremely fast at learning new skills or new knowledge and does it just have a bigger pool of strategies? Is there a single cohesive it?  In the center, that's more powerful, or bigger. And if so,  Do you do you imagine that this will be sort of Godlike in comparison to the rest of human civilization? Or does it just feel like another agent or another cluster of Agents?  So this is an area of a different people of different intuitions. I think it will be very powerful for sure.  I think that what I think is most likely to happen.  Is that there will be multiple.  Such ai's.  Being created roughly the same time.  I think that.  if the cluster is big enough,  Like the cluster is literally Continental size.  That's in could be really powerful indeed, right? If you literally have a continent sized cluster like those, those guys can be very powerful and I  Can tell you.  Is that if you're talking about extremely powerful aisle like truly dramatically powerful, then yeah. It would be nice if they could be restrained in some ways. for if there was some kind of an agreement or something,  because I think that if you are saying, hey, like, if you really  Like what is the the concern of superintelligence? What is one way to explain the concern?  If you imagine a system that is sufficiently powerful like really sufficiently powerful.  And you could say okay you need to do something sensible like care for sentient life. Let's say in a very single minded way.  We might not like the results that's really what it is. And so maybe by the way, the answer is that you do not build a single. You do not build an RL agent in the usual sense and actually I'll points several things out. I think human beings are a semi religion  You know, we pursue reward and then the emotions or whatever, make a style out of the reward, you from reward.  The market is like kind it's like a very short sighted kind of agent. Evolution is the same. Evolution is very intelligent in some ways but very dumb in other ways. The government has been designed to be a never-ending fight between three parts which has an effect. So I think things like this. Another thing that makes this discussion difficult is that we are talking about systems that don't exist. That we don't know how to build.  Right? That's the other thing and that's actually my belief, I think what people are doing right now, will go some distance and then Peter out if you continue to improve, but it will also not be it.  so, the it  We don't know how to build.  and I think that a lot in a lot hinges on,  Understanding reliable. Generalization.  now, say another thing which is like  You know, one of the things that you could say is what would that cause alignment to be difficult? Is that human? Well, I think it's, it's  your ability to learn human values is fragile, then you'll ability to optimize them his fragile, you will, you actually don't optimize them and I can't, you say, are these, not all instances of unreliable generalization  Why is it that human beings appear to generalize so much better? What do generalization was much better? What would happen in this case? What would be their effect? But those we can't, we can't. But those questions are right now, still an answerable. How does one think about What AI going? Well, looks like because I think you've scoped out  how AI might have all of these sort of continual learning, Asians. I will be very powerful. Maybe there will be many different AIS. How do you think about lots of content compute size intelligence is going around?  How dangerous is that?  How do we make that less dangerous?  and how do we do that in a way that  Protects a equilibrium where there might be misaligned AIS out there and Bad actors out there.  So one reason why I like the AI that cares for sentient life, you know, and we can debate on whether it's good or bad but  if the first end of these dramatic systems, actually do care for, you know,  love Humanity or something, you know, careful sentient life obviously this also needs to be achieved  This needs to be achieved.  So if this is achieved by the first end of those systems,  then I can see it go well.  At least four quite some time. And then there is the question of what happens in the long run. What happens in the long run, how do you achieve a long run equilibrium? and I think that,  There, the reason answer is well.  And I don't like this answer.  But it needs to be considered.  In the long run you might say. Okay, so if you have a world where powerful eyes exist,  In the short term you could say okay you have Universal High income. If Universal High income  and we all doing well, but we know that  What do the Buddhists say changes? The only constant and so things change and there is some kind of government political structure thing and it changes because these things have a shelf life.  you know, some new government think comes up and it functions and after sometime it stops functioning,  That's something that you see happening all the time.  And so, I think that for the long run equilibrium,  One approach, you could say, okay, so maybe every person will have an AI that will do their bidding.  And that's good.  And if that could be maintained indefinitely, that's true. But the downside with that is, okay. So then the AI goes and like,  earns earn, you know, earns money for the person and, you know, advocates for their needs and like the political sphere And maybe then writes, it will report saying, okay here's what I've done. Here's the situation and the process is great, keep it up.  But the person is no longer a participant.  and then you can say, that's a precarious place to be in, but so I'm going to preface by saying  I don't like this solution but it is a solution.  And the solution is if people become part AI with some kind of neuralink plus plus. Because what will happen is a result, is that now the AI understands something.  and we understand it too like  because now they understanding is transmitted whole sale. So now the AI is in some situation.  Now, it's like you are in the situation yourself fully.  And I think this is the answer to the equilibrium.  I wonder if the fact that emotions which were  Developed.  Millions or many cases billions of years ago in a totally different. Environment are still guiding our actions so strongly  Is an example of alignment success. Maybe spell out what I mean. The brainstem has these.  I don't know if it's more accurate to call it a value function or word function, but the brain stem has a direct effort saying, mate, with somebody who's more successful. The cortex is the part that understands what a success mean in the modern context.  But the brain stem is able to align the cortex and say, however you recognize success to be and I'm not smart enough to understand what that is. You're still going to pursue this directive. I think I think there is  so I think there's a more General point. I think it's actually really mysterious how  The brain and codes, high level desires. So how Evolution and Godzilla level desires, like it's pretty easy to understand how Evolution would would endow us with the desire for food. That smells good because smell is a chemical  And so, just pursue that can make all it's very easy to imagine such a revolution, doing such a thing.  but,  Evolution also has endowed us with only these social desires, like we really care about being seen positively by Society. We care about being a good standing, the  Like all these social intuitions that we have. Feel strongly that they are baked in.  And I don't know how Evolution did it because it's a high level concept. It's represented in the brain  like what people think like, let's say you are like you care about  Some social thing.  It's not like a low level signal like smell. It's not something that for which there is a sensor like the brain needs to do a lot of processing to piece together. Lots of bits of information to understand what's going on social and somehow Evolution said, that's what you should care about. Yes, how do you do it? And he did it quickly too. Yeah, because I think all these sophisticated social things that we care about, I think they've both pretty recently. So, Evolution had an easy time hardcore in these high level desire and  I maintain or, you know, at least I'll say I'm unaware of good hypothesis for how it's done. I had some ideas. I was kicking around but none of them.  None of them.  Are satisfying. Yeah, and what's special impressive. It was a desire that you learned in your lifetime. It kind of makes sense because your brain is intelligent. It makes sense why we would learn intelligent desires, but your point is that the desire is maybe this is not your point, but one way to understand it is, The desire is built into the genome and the genome is not intelligent, right? But it's able to, you're somehow able to describe this feature that requires, like, it's not even clear how you define that feature and you can get it into the, you can build it into the jeans. Yeah, essentially, or maybe I'll put a differently if you think about the tools that are available to the genome,  It says okay here's a recipe for building a brain and you could say here is the recipe for connecting the dopamine neurons to like the smell sense. Yeah.  And if the smell is a certain kind of, you know, good smell. You want to eat that.  I could imagine the genome doing that.  I'm claiming that it is harder to imagine.  It's harder to imagine the genome saying you should care about.  Some complicated computation that your entire that like a big chunk of your brain does. That's all I'm claiming. I can tell you like a speculation. I was wondering how it could be done and let me offer speculation and I'll explain why this speculation is probably false.  so, this speculation is  Okay.  so, the brain it's like,  The brain has those regions, you know, the brain regions.  We have our cortex, right? And as always brain regions, and the cortex is uniform, but the brain regions and, and the neurons in the cortex, they kind of speak to their neighbors, mostly  and that's explains why you get brain regions because if you want to do some kind of speech processing,  all the neurons that do speech in to talk to each other and they can because neurons can only speak to their nearby neighbors. For the most part, it has to be a region.  All the regions are mostly located in the same place from person to person.  So, maybe Evolution hardcoded literally a location on the brain.  so, it's all like when when like, you know,  The GPS of the brain GPS coordinates such and such when that fires, that's what should care about. Like maybe that's what evolution did because that would be within the toolkit of evolution.  yeah, although there are examples where  for example, people who are born, blind have that area of their cortex adopted by  Another sense.  And I have no idea, but I'd be surprised if the desires or the reward functions which require visual signal, no longer worked, you know, people who have their different areas of their cortex. Co-opted for example, if you no longer have  Vision. Can you still feel the sense that I want people around me to like me and so forth which usually there's also visual cues for so actually fully agree with that. I I think there's an even stronger Contra argument to this Theory which is like if you think about people. So there are people  who get half of their brains removed in a childhood know.  And they still have all the rich brain regions, but they all somehow move to just one hemisphere which suggests that the brain regions, their location is not fixed and so that theory is not true. It would have been cool if it was true but it's not. And so I think that's a mystery but it's an interesting mystery like the fact is somehow Evolution was able to endow us to care about social stuff, very, very reliably and even people who have like all kinds of strange mental conditions, and deficiencies and emotional problems tend to care about this. Also, the air tools like defects voice, clones and agents have dramatically increased the sophistication of Fraud and Abuse. So it's more important than ever to actually understand the identity and intent of whoever, or whatever is using your platform. That's exactly what sardine helps you do starting brings together, thousands of device behavior. And identity signals to help you assess risk everything from Howell user types or moves their mouths or holds their device to whether they're hiding their true location behind the VPN to, whether they're injecting a fake camera feed during kyc. Selfie checks, sardine combines these signals with insights from their network of almost 4 billion devices, things like a user history of fraud or they're assoc. Sessions with other high-risk accounts.  You can spot that actors before they do damage, this would literally be impossible if you only use data from your own application. Sardine doesn't stop a detection. They offer Suite of agents to streamline onboarding tracks and automated investigations. So as fraudsters, use AI to scale their attacks. You can use AI to scale your defenses. Go to sardine.ai, cash to learn more and download their guide on AI fraud detection.  What is SSI playing on doing differently?  So presumably, your plan is to be one of the frontier companies. When this time arrives,  And then, what is?  presumably, you started SSI because you're like, I think I have a way of approaching how to do the safely in a way that the other companies don't  What is that difference?  So, the way I would describe it as  There are some ideas that I think a promising and I want to investigate them and see if they are indeed promising or not it's really that simple. It's an attempt.  I think that if the ideas turn out to be correct, these ideas that we discussed around on understanding generalization. And then I think we will have something Worthy.  Will they turn out to be correct? We are doing research. We are squarely age of research company. We are making progress. We've actually made quite a good progress of the past year, but we need to keep making more progress, more research.  And that's how I see it. I see it as an attempt.  To be.  An attempt to be a voice and a participant.  People have asked.  Your co-founder and previously left to go to meta recently.  And people have asked, well, if there was a lot of breakthroughs being made, that's like a thing that should have been unlikely. I wonder how you respond. Yes. So I for this I will simply remind a few facts  That may have been forgotten. And I think this these facts which provide the contacts, I think they explained the situation.  So, the context was that we were fundraising at the 32 billion.  And then meta.  Came in and offered to acquire us.  And I said, no. But my former co-founder.  Like in some sense, said yes.  And as a result, he also was able to enjoy from a lot of near-term. Liquidity.  And he was the only person from SSI to join meta.  It sounds like exercise plan is to be a company that is at the frontier. When you get to this  very important period in human history.  Where you have superhuman intelligence and you have these ideas about how to make superhuman intelligence go. Well,  But other companies will be trying their own ideas.  What? Distinguishes, ssi's approach to making super intelligence go. Well, the main thing that distinguishes SSI is its technical approach.  So we have a different technical approach that I think is Worthy.  And we are pursuing it.  I maintain that in the end, there will be a convergence of strategy. So I think there will be a convergence of strategies where  at some point.  As AI becomes more powerful.  it's going to become more or less clear to everyone what the strategy should be and it should be something like yeah you need to find some way to talk to each other and you want your first actual like real, super intelligent AI to be aligned and somehow be  You know.  careful sentient life care for people Democratic one of those, some combination of thereof and I think  This is the condition.  That everyone should strive for.  And that's what the size striving for.  And I think that this time, if not already, all the other companies will realize, in the striving towards the same thing. And we'll see, I think that the world will truly change, the are become more powerful. Yeah, and I think a lot of these forecasts will like,  I think things will be really different and people will be acting really differently. What is speaking of forecast? What are your forecasts to this system? You're describing which can learn, as well, as a human.  And it was going to leave as a result becomes superhuman.  I think like Fast 20 fire 2020 years, so I just want to unroll your  How you might see the world coming. It's like, we have a couple more years where these other companies are continuing the current approach and it's still out and installs out here. Meaning, they are no more than Low hundreds of billions in revenue. Or how do you think about with styling on me means,  I think that I think it could, I think it could still out and  I think stolen out will look like  it will all look very similar. Yeah. Among all the different companies. Something like this. I'm not sure because I think I think I think even I think even I think even if we stole anyhow, I think these companies could make us stupendous.  Stupendous Revenue, maybe not profits. Because they will be, it will be, they will need to work hard to differentiate each other from the cells, but Revenue, definitely  but there's something in your model implies that  The, when the correct solution doesn't emerge, there will be convergence between all the companies.  And I'm curious why you think that's the case? Well, I was talking more about convergence on their largest strategies. Hmm. I think eventually convergence on the technical approach is probably going to happen as well, but I was alluding to convergence to the largest strategies. What what, exactly is the thing that should be done?  I just want to be better understand how you see the future in Rolling. So currently we have these different companies and you expect their approach to continue generating Revenue. Yes. But not get to this human life learner. Yes. So now we have these different Forks of companies we have, we have thinking machines, there's a bunch of other labs. Yes, and maybe one of them figures out the correct approach, but then the release of their product makes it clear to other people. How to do this thing. I think it won't be clear how to do its thing, but it will be clear that something different is possible, right? And that is information.  And I think people will then be trying to figure out how that's how that works. I do think though that  one of the things that,  That I think, you know, not address here not discussed, is that?  with each increase in the AI capabilities, I think they will be some kind of  Changes. But I don't know exactly which ones.  In how things have been done. So like,  I think it's going to be important yet. I can't spell out what that is exactly and how are the  by default you would expect the company that has the model company that has that model to be getting all these games because they have the model that is learning how to do all.  Has the skills and knowledge that it's building up in the world.  What is a reason to think that the benefits of that would be widely distributed and not just end up at Whatever model company gets this continuous. Learning Loop going first.  Like I think that empirically what happened. So here here is what I think is going to happen. Now one I think empirically when  Let's let's look at let's look at how things have gone so far with them, the areas of the past. So one company produced an advanced  And that the company scrambled.  and,  Produce some competitive, some some similar things after some out of time.  And they started to compete in the market and push their push, the price is down. And so, I think, from the market perspective, I think something similar will happen there as well. Even if someone's okay, we are talking about the good World, by the way, where  What's the good world? What's the good world?  Where we have these.  Powerful human like, learners.  it also like and by the way, maybe there's another thing we haven't discussed on the  On the spec of the super intelligent AI.  That I think is worth considering. He said you make it narrow.  Can be useful in narrow the same time so you can have lots of narrow super intelligent AIS, but suppose you have many of them.  and you have some and you have some company that's producing a lot of  Profits from it. And then you have another company that comes in.  And starts to compete, and the competition is going to work is through specialization.  I think what's going to happen is that  The way.  Competition, like competition loves specialization and you see it in the market, you see it in evolution as well. So you want to have lots of different nations and you're going to have lots of different companies who are occupying different niches in.  In this kind of world, we must say yeah like one company is really quite a bit better at some area of, really complicated economic activity and a different companies better than other area and the third companies really good litigation. That's what human like learning implies. Is that like a can learn it can but but you have a accumulated learning, you have a big investment.  you spend a lot of compute to become really, really good, really phenomenal of this thing and someone else spent A huge amount of compute and a huge amount of experience to get really, really good at some other thing, right? You apply a lot of human learning to get there but now, like you are, you are at this  High point where someone else would say, look like, I don't want to start learning what you've learned that was that would require many different companies to begin at the human, like continual learning agent at the same time.  So that they can start their different research in different branches. But if one company,  You know, gets that Asian first or gets that learner first?  It does then seem like, well, you know, they like we just think about every single job in the economy.  You just have instance, learning each one. Seems tractable for yeah. That's the that's that's a valid argument. My my strong intuition is that it's not going to go.  My strong intuition, is that? Yeah. Like the argument says, it will go this way. Yeah, but my strong intuition is that it will not go this way.  That this is the  You know, in in theory, there is no difference between theory in practice, and perhaps the reasons. And that's gonna be one of those  A lot of people's models of recursive self-improvement, literally explicitly State. We will have a million Ilias in a server, they're coming with different ideas and this will lead to a super intelligence emerging, very fast. You have some intuition about how parallelizable the thing you are doing. Is how how, what are the gains from making copies of Ilya? I don't know. I think.  I think they'll definitely be there will be diminishing returns because you want you want people who think differently rather than the same.  I think that if they were literal copies of me, I'm not sure how much more incremental value you'd get.  I think that.  But people who think differently, that's what you want. Why is it that it's been? If you look at different models even released by totally different companies trained on potentially non-overlapping data sets, it's actually crazy how similar alums are to each other maybe the data sets are not as none overlap and it's  but,  There's there's some sensors like even if an individual human might be less productive than the future AI. Maybe there's something to the fact that human teams that were diversity than teams of the eyes might have. But how do we listen meaningful, diversity among AI. So I think just raising that temperature just results in different gibberish. I think you want something more like  different scientists have different different prejudices or different ideas. How do you get that kind of diversity among AI agents? So the reason there has been no diversity  I believe it's because of pre-training.  All the pre-trained models are the same. Pretty much.  Because the tree trend on the same data. Now our real and post-training is where some differentiation starts to emerge because different people  Come up with different RL training. Yeah.  And then I've heard you here in the past about self-play as a way to either get data or match agents to other agents equivalent intelligence to kick off learning.  How should we think about why there's no public?  Proposals of this kind of thinking working with a lot of two things to say.  I would say that the reason why I thought self-play closed, interesting.  Is because it offered the way to create models using compute only without data.  Right? And if you think the data is the ultimate bottle neck,  then using compute only is very interesting.  So that's what makes it interesting. Now, the  this thing is,  The self-play, at least the way it was done in the past when you have agents which are somehow completely with each other. It's only good for the certain set of skills. It is too narrow. It's only good for like negotiation conflict.  Certain social skills.  Strategizing that kind of stuff. And so if you care about those skills, then self-play will be useful.  Now actually, I think that self-play did  Find a home but just in a different form.  In a different form. So things like debate,  Proved a verifier. You have some kind of an llm as a judge which is also incentivized to find mistakes in your work. You could say, this is not exactly self-play but this is, you know, a related adversarial setup that people are doing and believed and really self plays. An example of is a special case of more General like  Um, competition between between ages.  Write the response, the natural response to competition, is to try to be different and so if you were to put multiple agents and you tell them, you know, you all need to work on some problems and you are an agent.  And you're inspecting. What else is working? You're going to say, well, if they're already taking this approach,  it's not clear. I should pursue it. Should pursue something differentiate.  And so I think that something like this could also create an incentive for a diversity of approaches. No, final question. What is research taste? You're obviously.  the person in the world who is  Considered to have the best taste in doing research. In AI, you were  The co-author on many of the biggest, the biggest things that have happened in the history of deep learning from Alex. And at the gpt3 to sew on what is it that how do you characterize? How  You come up with these ideas, I can so I can comment on this for myself. I think different people do it differently.  but one thing that guides me personally,  Is.  An aesthetic.  Of how AI should be.  By thinking about how people are but thinking correctly. Hmm. Like it's very easy to think about how people are incorrectly but what does it mean to think about people correctly? So I'll give you some examples.  the idea of the artificial, neuron  Is directly inspired by the brain and it's a great idea, why? Because you say sure, the brain has all these different organs as defaults, but the false probably don't matter. Hmm. Why do we think that the neurons matter because there is many of them It's kind of feels right. So you want the neuron? Yeah. You want some kind of local learning rule that will change the connections, you want some local learning rule that will change the connections between the neurons.  Right? It feels plausible to the brain, does it? The idea of the distributed representation?  The idea that the brain.  You know the brain responds to experience a neural. It should learn from experience, not response the brain learns from experience.  The neural of experience.  And you kind of ask yourself is something fundamental or not fundamental house. Things should be. Yeah. And I think that's been guiding me a fair bit. Kind of thinking from multiple angles and looking for almost Beauty Simplicity ugliness. There's no room for ugliness. It's just Beauty Simplicity Elegance correct. Inspiration from the brain and all of those things need to be present at the same time and the more they are present, the more confident you can be in a top-down belief.  And then the top down belief is the thing that sustains you when the experiments contradict you  because if you just trust the data all the time,  well, sometimes you can be doing a correct thing, but there's a bug.  But, you don't know the reasons about how can you tell that there is a bug? How do you know if you should keep the bugging or you conclude it's the wrong direction? Well, is the top down? Well, how should you can say, the things have to be this way? Something like this has to work, therefore, we gotta keep going, that's the top down and it's based on this, like, multifaceted Beauty and inspiration by the brain.  All right.  We'll leave it there. Thank you so much. Thank you so much. Oh,  all right. Appreciate it was great. Yeah, I enjoyed it. Yes, me too.  Everybody. I hope you enjoyed that episode. If you did at the most helpful thing you can do is just share it with other people who think might enjoy. It's also helpful if you leave a rating or a comment on whatever platform you're listening on,  if you're interested in sponsoring the podcast, you're gonna reach out at War, cash.com slash advertise. Otherwise I'll see you in the next one.",
  "transcribed_at": "2025-12-05T20:45:02.502Z"
}